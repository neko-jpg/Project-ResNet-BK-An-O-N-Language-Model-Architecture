"""
Resonant Holographic Tensor Train (HTT) Embedding

Riemannian Resonant Tunneling ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿç¾ã™ã‚‹HTT Embeddingã€‚
ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã‚’å®Œå…¨å¯¾ç§°ãªè¶…ç«‹æ–¹ä½“ï¼ˆHypercubeï¼‰ã«å¼·åˆ¶ã™ã‚‹ã“ã¨ã§ã€
å‹¾é…ã®æ¡ä»¶æ•°ã‚’ Îºâ‰ˆ1 ã«ä¿ã¡ã€ä»»æ„ã®èªå½™ã‚µã‚¤ã‚ºã§ã‚‚å­¦ç¿’ã‚’å®‰å®šåŒ–ã—ã¾ã™ã€‚

Mathematical Foundation:
    å•é¡Œ: æ¨™æº–HTTã®å¹³æ–¹æ ¹åˆ†è§£ V = v1 Ã— v2 ã¯å½¢çŠ¶æ­ªã¿ã‚’ç”Ÿã‚€
    
    è§£æ±ºç­–: Resonant Tunneling
    1. V_res = 2^âŒˆlogâ‚‚(V)âŒ‰ ã«æ‹¡å¼µï¼ˆGhost Tokenè¿½åŠ ï¼‰
    2. è¶…ç«‹æ–¹ä½“åˆ†è§£: V_res = n Ã— n Ã— n Ã— n ï¼ˆ4ã‚³ã‚¢ã®å ´åˆï¼‰
    3. Iso-Spectral ZetaåˆæœŸåŒ–: GUEåˆ†å¸ƒã§å›ºæœ‰å€¤ã‚’ç­‰é–“éš”åŒ–
    
    æ•°å­¦çš„å¸°çµ:
    - æ¡ä»¶æ•° Îº(G_k) â‰ˆ 1 for all cores
    - å‹¾é…æµãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãªãå…¨ã‚³ã‚¢ã«å‡ç­‰ã«åˆ°é”
    - åˆæœŸLogitsã«æ§‹é€ çš„å‡¹å‡¸ â†’ å³åº§ã«å­¦ç¿’é–‹å§‹

Physical Intuition (ç‰©ç†çš„ç›´è¦³):
    - Ghost Token = å‹¾é…ã®ã€Œãƒã‚¤ãƒ‘ã‚¹é“è·¯ã€
    - è¶…ç«‹æ–¹ä½“ = é‡å­çŠ¶æ…‹ã®å®Œå…¨å¯¾ç§°æ€§
    - ZetaåˆæœŸåŒ– = ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã®åç™ºï¼ˆLevel Repulsionï¼‰

Requirements:
    - ä»»æ„ã® vocab_size ã§æ¡ä»¶æ•° Îº â‰ˆ 1
    - 90%ä»¥ä¸Šã®åœ§ç¸®ç‡ã‚’ç¶­æŒ
    - å‹¾é…ãƒ•ãƒ­ãƒ¼ã®ä¿å­˜
    - åˆæœŸçŠ¶æ…‹ã§ã®å¯¾ç§°æ€§ã®è‡ªç™ºçš„ç ´ã‚Œ

Author: Project MUSE Team (Riemannian Resonant Tunneling Extension)
"""

import math
from typing import Optional, Tuple, List

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from .config import Phase1Config
from .errors import InvalidConfigError, NumericalInstabilityError

# Import optimized Triton kernel for 4-core contraction
try:
    from src.kernels.resonant_triton_contraction import (
        resonant_contraction_memory_efficient,
        direct_contraction_logits,
        TRITON_AVAILABLE as RESONANT_TRITON_AVAILABLE,
    )
    _RESONANT_TRITON_AVAILABLE = RESONANT_TRITON_AVAILABLE
except ImportError:
    _RESONANT_TRITON_AVAILABLE = False
    resonant_contraction_memory_efficient = None
    direct_contraction_logits = None


class ResonantHTTEmbedding(nn.Module):
    """
    Resonant Holographic Tensor Train Embedding Layer
    
    Riemannian Resonant Tunnelingã‚’å®Ÿç¾ã™ã‚‹HTT Embeddingã€‚
    vocab_sizeã‚’2^Nã«æ‹¡å¼µã—ã€å®Œå…¨å¯¾ç§°ãªè¶…ç«‹æ–¹ä½“ãƒ†ãƒ³ã‚½ãƒ«ã§åˆ†è§£ã—ã¾ã™ã€‚
    
    Args:
        vocab_size: å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º
        d_model: å‡ºåŠ›æ¬¡å…ƒï¼ˆãƒ¢ãƒ‡ãƒ«ã®éš ã‚Œå±¤æ¬¡å…ƒï¼‰
        rank: Tensor Trainã®ãƒ©ãƒ³ã‚¯ï¼ˆåœ§ç¸®ç‡ã‚’åˆ¶å¾¡ï¼‰
        num_cores: ã‚³ã‚¢æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ã€è¶…ç«‹æ–¹ä½“åˆ†è§£ï¼‰
        phase_encoding: ä½ç›¸å›è»¢ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã‹
        use_zeta_init: Iso-Spectral ZetaåˆæœŸåŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Attributes:
        vocab_size: å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º
        resonant_vocab_size: 2^N ã«æ‹¡å¼µã•ã‚ŒãŸèªå½™ã‚µã‚¤ã‚º
        ghost_tokens: æœªä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆresonant - actualï¼‰
        d_model: å‡ºåŠ›æ¬¡å…ƒ
        rank: TTãƒ©ãƒ³ã‚¯
        num_cores: ã‚³ã‚¢æ•°
        core_factors: å„ã‚³ã‚¢ã®å› æ•° [n1, n2, n3, n4]
        cores: List[nn.Parameter] - Tensor Trainã‚³ã‚¢
    
    Example:
        >>> # æ¨™æº–Embeddingã®ç½®ãæ›ãˆ
        >>> # 50,000èª â†’ 65,536ï¼ˆ2^16ï¼‰ã«æ‹¡å¼µ
        >>> embedding = ResonantHTTEmbedding(50000, 1024, rank=16)
        >>> print(f"Resonant size: {embedding.resonant_vocab_size}")  # 65536
        >>> print(f"Ghost tokens: {embedding.ghost_tokens}")  # 15536
        >>> 
        >>> input_ids = torch.randint(0, 50000, (4, 128))
        >>> output = embedding(input_ids)  # (4, 128, 1024)
    """
    
    def __init__(
        self,
        vocab_size: int,
        d_model: int,
        rank: int = 16,
        num_cores: int = 4,
        phase_encoding: bool = True,
        use_zeta_init: bool = True,
        use_complex_phase: bool = False,  # Phase 2æº–å‚™
    ):
        super().__init__()
        
        # Validation
        if vocab_size <= 0:
            raise InvalidConfigError(
                param_name="vocab_size",
                param_value=vocab_size,
                reason="Must be positive integer"
            )
        if d_model <= 0:
            raise InvalidConfigError(
                param_name="d_model",
                param_value=d_model,
                reason="Must be positive integer"
            )
        if rank <= 0:
            raise InvalidConfigError(
                param_name="rank",
                param_value=rank,
                reason="Must be positive integer"
            )
        if num_cores < 2 or num_cores > 6:
            raise InvalidConfigError(
                param_name="num_cores",
                param_value=num_cores,
                reason="Must be between 2 and 6"
            )
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.rank = rank
        self.num_cores = num_cores
        self.phase_encoding = phase_encoding
        self.use_zeta_init = use_zeta_init
        self.use_complex_phase = use_complex_phase
        
        # ========== 1. Resonant Number Expansion ==========
        # vocab_size ã‚’æœ€ã‚‚è¿‘ã„ 2^N ã«æ‹¡å¼µ
        self.resonant_vocab_size = self._compute_resonant_vocab_size(vocab_size)
        self.ghost_tokens = self.resonant_vocab_size - vocab_size
        
        print(f"ğŸ“ Resonant HTT: {vocab_size:,} â†’ {self.resonant_vocab_size:,} "
              f"(+{self.ghost_tokens:,} ghost tokens, {self.ghost_tokens/self.resonant_vocab_size*100:.1f}% overhead)")
        
        # ========== 2. Hypercube Factorization ==========
        # vocab ã¨ d_model ã®ä¸¡æ–¹ã‚’è¶…ç«‹æ–¹ä½“åˆ†è§£
        self.vocab_factors = self._hypercube_factorization(
            self.resonant_vocab_size, num_cores
        )
        self.d_factors = self._hypercube_factorization_d_model(d_model, num_cores)
        
        print(f"   Vocab factors: {' Ã— '.join(map(str, self.vocab_factors))} = {np.prod(self.vocab_factors)}")
        print(f"   D_model factors: {' Ã— '.join(map(str, self.d_factors))} = {np.prod(self.d_factors)}")
        
        # ========== 3. Create Tensor Train Cores ==========
        # 4ã‚³ã‚¢ã®å ´åˆ: Core_k ã®å½¢çŠ¶ã¯ (v_k, r_{k-1}, r_k, d_k)
        # å¢ƒç•Œæ¡ä»¶: r_0 = 1, r_{num_cores} = 1
        self.cores = nn.ParameterList()
        
        for k in range(num_cores):
            v_k = self.vocab_factors[k]
            d_k = self.d_factors[k]
            r_left = 1 if k == 0 else rank
            r_right = 1 if k == num_cores - 1 else rank
            
            # Core shape: (v_k, r_left, r_right, d_k)
            core = nn.Parameter(torch.empty(v_k, r_left, r_right, d_k))
            self.cores.append(core)
        
        # ========== 4. Iso-Spectral Zeta Initialization ==========
        if use_zeta_init:
            self._iso_spectral_zeta_init()
        else:
            self._orthogonal_init()
        
        # ========== 5. Phase Parameters ==========
        if phase_encoding:
            self.phase_shift = nn.Parameter(torch.zeros(rank))
            self._init_phase_parameters()
        else:
            self.register_buffer('phase_shift', torch.zeros(rank))
        
        # ========== 6. Gradient Hooks ==========
        self._register_gradient_hooks()
        
        # ========== 7. Parameter Count Tracking ==========
        self._standard_params = vocab_size * d_model
        self._tt_params = sum(p.numel() for p in self.cores)
        if phase_encoding:
            self._tt_params += rank
        self._compression_ratio = self._tt_params / self._standard_params
        
        print(f"   Compression: {self._compression_ratio*100:.2f}% "
              f"({self._tt_params:,} / {self._standard_params:,})")
    
    def _compute_resonant_vocab_size(self, vocab_size: int) -> int:
        """
        vocab_sizeã‚’æœ€ã‚‚è¿‘ã„2^Nã«æ‹¡å¼µ
        
        Example:
            50,000 â†’ 65,536 (2^16)
            32,000 â†’ 32,768 (2^15)
            3,200 â†’ 4,096 (2^12)
        """
        log2 = math.log2(vocab_size)
        n = math.ceil(log2)
        return 2 ** n
    
    def _hypercube_factorization(self, size: int, num_cores: int) -> List[int]:
        """
        ã‚µã‚¤ã‚ºã‚’è¶…ç«‹æ–¹ä½“ï¼ˆã§ãã‚‹ã ã‘å‡ç­‰ï¼‰ã«åˆ†è§£
        
        ä¾‹: 65536, 4ã‚³ã‚¢ â†’ [16, 16, 16, 16] (16^4 = 65536)
        ä¾‹: 4096, 3ã‚³ã‚¢ â†’ [16, 16, 16] (16^3 = 4096)
        """
        # 2^N ã‚’ num_cores ã§å‡ç­‰åˆ†å‰²
        log2 = int(math.log2(size))
        base_exp = log2 // num_cores
        remainder = log2 % num_cores
        
        factors = []
        for i in range(num_cores):
            exp = base_exp + (1 if i < remainder else 0)
            factors.append(2 ** exp)
        
        # ç©ãŒåˆã£ã¦ã„ã‚‹ã‹ç¢ºèª
        product = np.prod(factors)
        assert product == size, f"Factorization error: {factors} = {product} != {size}"
        
        return factors
    
    def _hypercube_factorization_d_model(self, d_model: int, num_cores: int) -> List[int]:
        """
        d_modelã‚’num_coresã«åˆ†è§£ï¼ˆå®Œå…¨å¯¾ç§°ã§ãªãã¦ã‚‚ã‚ˆã„ï¼‰
        
        ä¾‹: 1024, 4ã‚³ã‚¢ â†’ [4, 8, 8, 4] (4*8*8*4 = 1024)
        ä¾‹: 4096, 4ã‚³ã‚¢ â†’ [8, 8, 8, 8] (8^4 = 4096)
        """
        # d_modelãŒ2ã®ã¹ãä¹—ã«è¿‘ã„å ´åˆã¯å‡ç­‰åˆ†å‰²ã‚’è©¦ã¿ã‚‹
        log2_approx = math.log2(d_model)
        
        if abs(log2_approx - round(log2_approx)) < 0.01:
            # ã»ã¼2ã®ã¹ãä¹—
            return self._hypercube_factorization(int(2 ** round(log2_approx)), num_cores)
        
        # ä¸€èˆ¬çš„ãªåˆ†è§£ï¼ˆã§ãã‚‹ã ã‘å‡ç­‰ã«ï¼‰
        root = d_model ** (1.0 / num_cores)
        factors = []
        remaining = d_model
        
        for i in range(num_cores - 1):
            # æœ€ã‚‚è¿‘ã„å› æ•°ã‚’è¦‹ã¤ã‘ã‚‹
            factor = max(1, round(root))
            # remainingãŒå‰²ã‚Šåˆ‡ã‚Œã‚‹å› æ•°ã‚’æ¢ã™
            while remaining % factor != 0 and factor > 1:
                factor -= 1
            factors.append(factor)
            remaining //= factor
        
        factors.append(remaining)
        
        # ç©ãŒåˆã£ã¦ã„ã‚‹ã‹ç¢ºèª
        product = np.prod(factors)
        if product != d_model:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦è¶…ç«‹æ–¹ä½“ã«
            padded = 2 ** math.ceil(math.log2(d_model))
            factors = self._hypercube_factorization(padded, num_cores)
            self._d_model_padded = padded
        else:
            self._d_model_padded = d_model
        
        return factors
    
    def _iso_spectral_zeta_init(self):
        """
        Iso-Spectral Zeta Initialization
        
        ãƒªãƒ¼ãƒãƒ³ãƒ»ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®é›¶ç‚¹åˆ†å¸ƒï¼ˆGUE: Gaussian Unitary Ensembleï¼‰ã«åŸºã¥ãåˆæœŸåŒ–ã€‚
        å›ºæœ‰å€¤ãŒã€Œåç™ºã€ã—ã‚ã†Level Repulsionç‰¹æ€§ã«ã‚ˆã‚Šã€åˆæœŸLogitsã«æ§‹é€ çš„å‡¹å‡¸ã‚’ä¸ãˆã‚‹ã€‚
        
        æ•°å­¦çš„æ ¹æ‹ :
        - GUEè¡Œåˆ—ã®å›ºæœ‰å€¤é–“éš”ã¯ Wigner semicircle law ã«å¾“ã†
        - é›¶ç‚¹é–“ã®é–“éš”ã¯åç™ºåŠ›ã§ç­‰é–“éš”åŒ–ã•ã‚Œã‚‹
        - ã“ã‚Œã«ã‚ˆã‚ŠåˆæœŸç¢ºç‡åˆ†å¸ƒã®ã€Œå¹³å¦ã•ã€ã‚’é˜²ãã€å‹¾é…ãŒæµã‚Œã‚„ã™ããªã‚‹
        
        Implementation:
        1. å„ã‚³ã‚¢ã«å¯¾ã—ã¦GUEè¡Œåˆ—ã‚’ç”Ÿæˆ
        2. QRåˆ†è§£ã§ç›´äº¤åŒ–ï¼ˆæ¡ä»¶æ•° Îº = 1ï¼‰
        3. å›ºæœ‰å€¤åˆ†å¸ƒã‚’Wigneråˆ†å¸ƒã«èª¿æ•´
        """
        print("   ğŸ§¬ Applying Iso-Spectral Zeta Initialization...")
        
        for k, core in enumerate(self.cores):
            v_k, r_left, r_right, d_k = core.shape
            
            # Step 1: å„(v_k, d_k)ã®ã‚¹ãƒ©ã‚¤ã‚¹ã«å¯¾ã—ã¦GUEåˆæœŸåŒ–
            with torch.no_grad():
                for i in range(r_left):
                    for j in range(r_right):
                        # GUEè¡Œåˆ—ã‚’ç”Ÿæˆ: H = (A + Aâ€ ) / 2 where A ~ N(0, 1)
                        slice_2d = core[:, i, j, :]  # (v_k, d_k)
                        
                        # ãƒ©ãƒ³ãƒ€ãƒ è¤‡ç´ è¡Œåˆ—ã‚’ç”Ÿæˆï¼ˆå®Ÿéƒ¨ã®ã¿ä½¿ç”¨ï¼‰
                        A = torch.randn_like(slice_2d)
                        H = (A + A.T[:v_k, :d_k] if v_k == d_k else A) / math.sqrt(2)
                        
                        # Wigner semicircle scaling
                        # æ¨™æº–åå·®ã‚’ sqrt(2/N) ã«è¨­å®š
                        N = max(v_k, d_k)
                        # Base scale - increased from 0.5 to 1.0 for stronger gradient signal
                        base_scale = math.sqrt(2.0 / N) * 1.0
                        
                        # Vocab-size dependent boost: larger vocab needs MUCH larger scale
                        # 4096 (2^12) is the reference, 32768 (2^15) gets (15/12)^2 â‰ˆ 1.56x boost
                        # This compensates for the 8x larger output space
                        vocab_boost_raw = math.log(self.resonant_vocab_size) / math.log(4096)
                        vocab_boost = vocab_boost_raw ** 2  # Square for stronger effect
                        scale = base_scale * vocab_boost
                        
                        # Level repulsion ã‚’æ¨¡å€£: å›ºæœ‰å€¤ã‚’ç­‰é–“éš”åŒ–
                        if v_k == d_k and v_k <= 64:  # å°ã•ã„è¡Œåˆ—ã®ã¿å³å¯†å‡¦ç†
                            # å®Œå…¨ãªGUEã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                            gue = torch.randn(v_k, v_k, dtype=core.dtype)
                            gue = (gue + gue.T) / math.sqrt(2)
                            eigenvalues, eigenvectors = torch.linalg.eigh(gue)
                            
                            # Wigner semicircle ã«å¾“ã†å›ºæœ‰å€¤
                            # E(s) = (32/Ï€Â²) * s * exp(-4sÂ²/Ï€) where s = eigenvalue spacing
                            # ç°¡ç•¥åŒ–: ç­‰é–“éš”å›ºæœ‰å€¤ã‚’ä½¿ç”¨
                            target_eigenvalues = torch.linspace(-1, 1, v_k) * scale * v_k
                            
                            # å†æ§‹æˆ: H = V * diag(Î») * Vâ€ 
                            H = eigenvectors @ torch.diag(target_eigenvalues) @ eigenvectors.T
                            slice_2d.copy_(H[:, :d_k])
                        else:
                            # å¤§ãã„è¡Œåˆ—: è¿‘ä¼¼GUEåˆæœŸåŒ–
                            slice_2d.copy_(H * scale)
        
        # æœ€çµ‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: 4ã‚³ã‚¢ç¸®ç´„å¾Œã®logitsåˆ†æ•£ã‚’é©åˆ‡ã«
        # FIXED: éåº¦ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å‰Šé™¤ï¼ˆvanishing logits ã®åŸå› ã ã£ãŸï¼‰
        # ä»£ã‚ã‚Šã«è»½ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ã¿
        scale_factor = 1.0 / math.sqrt(self.num_cores)  # â‰ˆ0.5 for 4 cores
        for core in self.cores:
            core.data *= scale_factor
    
    def _orthogonal_init(self):
        """
        ç›´äº¤åˆæœŸåŒ–ï¼ˆZetaåˆæœŸåŒ–ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        
        QRåˆ†è§£ã‚’ä½¿ç”¨ã—ã¦å„ã‚³ã‚¢ã‚’ç›´äº¤è¡Œåˆ—ã«åˆæœŸåŒ–ã€‚
        æ¡ä»¶æ•° Îº = 1 ã‚’ä¿è¨¼ã€‚
        """
        for core in self.cores:
            v_k, r_left, r_right, d_k = core.shape
            
            with torch.no_grad():
                for i in range(r_left):
                    for j in range(r_right):
                        slice_2d = core[:, i, j, :]
                        
                        # æ­£è¦ä¹±æ•°ã§åˆæœŸåŒ–
                        nn.init.orthogonal_(slice_2d.view(v_k, d_k))
                        
                        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                        slice_2d *= 0.02 / (self.num_cores ** 0.5)
    
    def _init_phase_parameters(self):
        """ä½ç›¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–"""
        if self.use_zeta_init:
            # Zeta zeros inspired phases
            # ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®éè‡ªæ˜ãªé›¶ç‚¹ã®è™šéƒ¨ã¯ 14.13..., 21.02..., 25.01..., etc.
            # ã“ã‚Œã‚’æ¨¡å€£ã—ã¦ä½ç›¸ã‚’è¨­å®š
            with torch.no_grad():
                # æœ€åˆã® rank å€‹ã®ã€Œé›¶ç‚¹ã€çš„ãªä½ç›¸
                for i in range(self.rank):
                    # æ¦‚ç®—: Î³_n â‰ˆ 2Ï€n / log(n+1)
                    self.phase_shift[i] = 2 * math.pi * (i + 1) / math.log(i + 2)
                    self.phase_shift[i] %= (2 * math.pi)  # [0, 2Ï€]ã«æ­£è¦åŒ–
                self.phase_shift -= math.pi  # [-Ï€, Ï€]ã«æ­£è¦åŒ–
                self.phase_shift *= 0.01  # å°ã•ãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        else:
            nn.init.zeros_(self.phase_shift)
    
    def _register_gradient_hooks(self):
        """å‹¾é…ã‚µãƒ‹ã‚¿ã‚¤ã‚ºãƒ•ãƒƒã‚¯ã‚’ç™»éŒ²"""
        def _sanitize_grad(grad):
            if grad is None:
                return None
            # Avoid value-clamping here: backward hooks run pre-unscale under GradScaler.
            # Unconditional nan_to_num avoids GPUâ†”CPU sync from `.any()` checks.
            return torch.nan_to_num(grad, nan=0.0, posinf=0.0, neginf=0.0)
        
        for core in self.cores:
            core.register_hook(_sanitize_grad)
        
        if self.phase_encoding:
            self.phase_shift.register_hook(_sanitize_grad)
    
    def get_compression_ratio(self) -> float:
        """åœ§ç¸®ç‡ã‚’è¿”ã™"""
        return self._compression_ratio
    
    def get_parameter_counts(self) -> Tuple[int, int]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¿”ã™ (standard, tt)"""
        return self._standard_params, self._tt_params
    
    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with Resonant Tensor Train contraction
        
        Args:
            input_ids: (Batch, SeqLen) ãƒˆãƒ¼ã‚¯ãƒ³ID [0, vocab_size)
        
        Returns:
            embeddings: (Batch, SeqLen, d_model) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        if input_ids.dim() != 2:
            raise ValueError(f"Expected 2D input_ids (B, L), got shape {input_ids.shape}")
        
        B, L = input_ids.shape
        
        # ========== 1. Index Decomposition ==========
        # ãƒˆãƒ¼ã‚¯ãƒ³ID i ã‚’ (i_1, i_2, ..., i_k) ã«åˆ†è§£
        indices = self._decompose_indices(input_ids)  # List of (B, L) tensors
        
        # ========== 2. Gather Cores ==========
        # å„ã‚³ã‚¢ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ã‚¹ãƒ©ã‚¤ã‚¹ã‚’å–å¾—
        gathered_cores = []
        for k, (core, idx) in enumerate(zip(self.cores, indices)):
            # core: (v_k, r_left, r_right, d_k)
            # idx: (B, L) with values in [0, v_k)
            
            # Clamp indices for safety (ghost tokens are valid)
            idx = torch.clamp(idx, 0, core.shape[0] - 1)
            
            # Gather: (B, L, r_left, r_right, d_k)
            gathered = core[idx]
            gathered_cores.append(gathered)
        
        # ========== 3. Apply Phase Rotation ==========
        if self.phase_encoding:
            phase_shift_safe = torch.clamp(self.phase_shift, -math.pi, math.pi)
            
            if self.use_complex_phase:
                # Complex phase rotation
                phase_factor = torch.exp(1j * phase_shift_safe)
                # Apply to middle cores (internal rank dimensions)
                for k in range(1, len(gathered_cores) - 1):
                    gc = gathered_cores[k].to(torch.complex64)
                    # Phase on r_left dimension
                    gc = gc * phase_factor.view(1, 1, -1, 1, 1)
                    gathered_cores[k] = gc
            else:
                # Real phase approximation: cos(Î¸)
                phase_mod = torch.cos(phase_shift_safe)
                phase_mod = torch.clamp(phase_mod, -1.0, 1.0)
                # Apply to first non-boundary core
                if len(gathered_cores) > 2:
                    gathered_cores[1] = gathered_cores[1] * phase_mod.view(1, 1, -1, 1, 1)
        
        # ========== 4. Tensor Train Contraction ==========
        # Sequential contraction: Core_1 @ Core_2 @ ... @ Core_k
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            result = self._contract_cores(gathered_cores)
        
        # ========== 5. Reshape and Crop ==========
        # result: (B, L, d1*d2*...*dk) â†’ (B, L, d_model)
        d_product = np.prod(self.d_factors)
        result = result.reshape(B, L, d_product)
        result = result[:, :, :self.d_model]  # Crop to exact d_model
        
        # ========== 6. Numerical Stability ==========
        if not torch.isfinite(result).all():
            result = torch.nan_to_num(result, nan=0.0, posinf=100.0, neginf=-100.0)
        
        # Normalize for variance stability
        result = result / (self.rank ** 0.5)
        
        # Handle complex output
        if torch.is_complex(result):
            result = result.real
        
        return result.to(input_ids.device)
    
    def _decompose_indices(self, input_ids: torch.Tensor) -> List[torch.Tensor]:
        """
        ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å„ã‚³ã‚¢ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆ†è§£
        
        Args:
            input_ids: (B, L) token IDs in [0, vocab_size)
        
        Returns:
            List of (B, L) tensors, one per core
        """
        indices = []
        remaining = input_ids.clone()
        
        # å³ã‹ã‚‰å·¦ã¸åˆ†è§£: i = i_1 * (v_2*v_3*v_4) + i_2 * (v_3*v_4) + i_3 * v_4 + i_4
        for k in reversed(range(self.num_cores)):
            v_k = self.vocab_factors[k]
            idx_k = remaining % v_k
            remaining = remaining // v_k
            indices.insert(0, idx_k)
        
        return indices
    
    def _contract_cores(self, gathered_cores: List[torch.Tensor]) -> torch.Tensor:
        """
        Tensor Train coresã‚’ç¸®ç´„ï¼ˆTritonæœ€é©åŒ–ç‰ˆ with PyTorchãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        
        4ã‚³ã‚¢ã®å ´åˆ:
        - Core 0: (B, L, 1, r, d0) â†’ (B, L, r, d0)
        - Core 1: (B, L, r, r, d1)
        - Core 2: (B, L, r, r, d2)
        - Core 3: (B, L, r, 1, d3) â†’ (B, L, r, d3)
        
        Returns:
            result: (B, L, d_product) tensor
        """
        # Try optimized Triton path first
        if _RESONANT_TRITON_AVAILABLE and gathered_cores[0].is_cuda:
            try:
                d_product = int(np.prod(self.d_factors))
                return resonant_contraction_memory_efficient(
                    gathered_cores, d_product, use_triton=True
                )
            except Exception:
                pass  # Fall through to PyTorch path
        
        # PyTorch fallback (original implementation)
        return self._contract_cores_pytorch(gathered_cores)
    
    def _contract_cores_pytorch(self, gathered_cores: List[torch.Tensor]) -> torch.Tensor:
        """PyTorch-based 4-core contraction (fallback when Triton unavailable)."""
        B, L = gathered_cores[0].shape[:2]
        
        # Core 0: (B, L, 1, r, d0) â†’ squeeze r_left=1 â†’ (B, L, r, d0)
        result = gathered_cores[0].squeeze(2)  # (B, L, r, d0)
        d_accumulated = result.shape[-1]  # d0
        
        for k in range(1, len(gathered_cores)):
            core_k = gathered_cores[k]  # (B, L, r_left, r_right, d_k)
            
            # Ensure dtype match
            if result.dtype != core_k.dtype:
                core_k = core_k.to(result.dtype)
            
            is_last = (k == len(gathered_cores) - 1)
            
            if is_last:
                # Last core: (B, L, r, 1, d_k) â†’ squeeze r_right=1 â†’ (B, L, r, d_k)
                core_k = core_k.squeeze(3)  # (B, L, r, d_k)
            
            r = result.shape[2]
            d_k = core_k.shape[-1]
            
            if is_last:
                # Final contraction: sum over rank r, outer product over d
                result_flat = result.reshape(B * L, r, d_accumulated)
                core_flat = core_k.reshape(B * L, r, d_k)
                out = torch.einsum('nrd,nre->nde', result_flat, core_flat)
                result = out.reshape(B, L, -1)  # (B, L, d_acc * d_k)
            else:
                # Intermediate: contract over r_left, keep r_right
                r_right = core_k.shape[3]
                result_flat = result.reshape(B * L, r, d_accumulated)
                core_flat = core_k.reshape(B * L, r, r_right * d_k)
                out = torch.einsum('nrd,nre->nde', result_flat, core_flat)
                out = out.reshape(B, L, d_accumulated, r_right, d_k)
                out = out.permute(0, 1, 3, 2, 4)  # (B, L, r_right, d_acc, d_k)
                result = out.reshape(B, L, r_right, d_accumulated * d_k)
                d_accumulated = d_accumulated * d_k
        
        return result
    
    def extra_repr(self) -> str:
        return (
            f"vocab_size={self.vocab_size}, "
            f"resonant_vocab_size={self.resonant_vocab_size}, "
            f"d_model={self.d_model}, "
            f"rank={self.rank}, num_cores={self.num_cores}, "
            f"ghost_tokens={self.ghost_tokens}, "
            f"compression_ratio={self._compression_ratio:.4f} "
            f"({self._tt_params}/{self._standard_params})"
        )


class ResonantHTTDecoder(nn.Module):
    """
    Decodes hidden states to vocabulary logits using shared Resonant HTT weights.
    """
    def __init__(self, embedding: ResonantHTTEmbedding):
        super().__init__()
        self.embedding = embedding

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hidden_states: (batch_size, seq_len, d_model)
        Returns:
            logits: (batch_size, seq_len, vocab_size)
        """
        B, L, D = hidden_states.shape
        emb = self.embedding
        
        # Pad d_model if needed
        d_product = np.prod(emb.d_factors)
        if D < d_product:
            hidden_states = F.pad(hidden_states, (0, d_product - D))
        
        # Reshape to match d_factors
        h = hidden_states.view(B, L, *emb.d_factors)
        
        # Contract with transposed cores (reverse order, transposed)
        logits = self._decode_contraction(h)
        
        # Crop to actual vocab_size (remove ghost tokens)
        logits = logits[:, :, :emb.vocab_size]
        
        return logits
    
    def _decode_contraction(self, h: torch.Tensor) -> torch.Tensor:
        """
        Memory-efficient inverse contraction for decoding.
        
        Uses direct_contraction_logits when available to avoid building
        the full embedding matrix (O(V) â†’ O(V^0.25) memory).
        """
        emb = self.embedding
        B, L = h.shape[:2]
        
        # Flatten d dimensions
        h_flat = h.view(B, L, -1)[:, :, :emb.d_model]
        
        # Try optimized path first
        if _RESONANT_TRITON_AVAILABLE and h.is_cuda:
            try:
                logits = direct_contraction_logits(
                    h_flat,
                    [core.data for core in emb.cores],
                    emb.d_factors,
                    emb.vocab_factors,
                )
                return logits[:, :, :emb.vocab_size]
            except Exception:
                pass  # Fall through to standard path
        
        # Standard path: chunked embedding-based logits
        device = h.device
        dtype = h.dtype
        
        # Process in chunks to save memory
        CHUNK_SIZE = 4096  # Process 4K tokens at a time
        vocab_size = emb.vocab_size
        
        logits_chunks = []
        for v_start in range(0, vocab_size, CHUNK_SIZE):
            v_end = min(v_start + CHUNK_SIZE, vocab_size)
            
            # Get embeddings for this vocab chunk
            chunk_tokens = torch.arange(v_start, v_end, device=device)
            chunk_embeddings = emb(chunk_tokens.unsqueeze(0)).squeeze(0)  # (chunk, d_model)
            
            # Compute logits for this chunk
            chunk_logits = torch.matmul(h_flat, chunk_embeddings.T)
            logits_chunks.append(chunk_logits)
        
        logits = torch.cat(logits_chunks, dim=-1)
        return logits


def create_resonant_htt_embedding(
    vocab_size: int,
    d_model: int,
    config: Optional[Phase1Config] = None,
) -> ResonantHTTEmbedding:
    """
    Factory function to create Resonant HTT embedding
    
    Args:
        vocab_size: èªå½™ã‚µã‚¤ã‚º
        d_model: å‡ºåŠ›æ¬¡å…ƒ
        config: Phase1Configï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰
    
    Returns:
        ResonantHTTEmbedding instance
    """
    if config is None:
        config = Phase1Config()
    
    return ResonantHTTEmbedding(
        vocab_size=vocab_size,
        d_model=d_model,
        rank=config.htt_rank,
        num_cores=getattr(config, 'resonant_num_cores', 4),
        phase_encoding=config.htt_phase_encoding,
        use_zeta_init=getattr(config, 'use_zeta_init', True),
    )


def diagnose_vocab_size(vocab_size: int) -> dict:
    """
    èªå½™ã‚µã‚¤ã‚ºã®ã€Œå¥å…¨æ€§ã€ã‚’è¨ºæ–­
    
    Returns:
        dict with:
        - is_power_of_2: 2ã®ã¹ãä¹—ã‹
        - resonant_size: æœ€ã‚‚è¿‘ã„2ã®ã¹ãä¹—
        - overhead_percent: Ghost tokenã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆ%ï¼‰
        - risk_level: 'low', 'medium', 'high'
        - recommendation: æ¨å¥¨äº‹é …
    """
    log2 = math.log2(vocab_size)
    is_power_of_2 = (log2 == int(log2))
    resonant_size = 2 ** math.ceil(log2)
    overhead = (resonant_size - vocab_size) / resonant_size * 100
    
    # ãƒªã‚¹ã‚¯è©•ä¾¡
    if is_power_of_2:
        risk_level = 'low'
        recommendation = "Perfect! Vocab size is already a power of 2."
    elif overhead < 10:
        risk_level = 'low'
        recommendation = f"Good. Only {overhead:.1f}% overhead with resonant expansion."
    elif overhead < 30:
        risk_level = 'medium'
        recommendation = f"Moderate overhead ({overhead:.1f}%). Consider using ResonantHTT."
    else:
        risk_level = 'high'
        recommendation = f"High overhead ({overhead:.1f}%). Strongly recommend ResonantHTT or adjusting vocab_size."
    
    return {
        'vocab_size': vocab_size,
        'is_power_of_2': is_power_of_2,
        'resonant_size': resonant_size,
        'overhead_percent': overhead,
        'risk_level': risk_level,
        'recommendation': recommendation,
    }
