"""
Phase 8 Integrated Model - ResNetBK Based Implementation

Phase 8ã¯Phase 7ï¼ˆResNetBK + HTT Embedding + Hybrid Hyperbolic Attentionï¼‰ã®æ‹¡å¼µã§ã™ã€‚
Phase 7ã®å…¨æ©Ÿèƒ½ã‚’ç¶™æ‰¿ã—ã€ä»¥ä¸‹ã®æ‹¡å¼µã‚’è¿½åŠ ã—ã¾ã™ï¼š

1. BK-Core Hyperbolic Integration: BK-Coreã®G_iiã‚’ä½¿ç”¨ã—ãŸç‰©ç†ãƒ™ãƒ¼ã‚¹ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
2. AR-SSM Hyperbolic Fusion: AR-SSMã¨åŒæ›²ç©ºé–“ã®èåˆ
3. Entailment Cones: è«–ç†çš„å«æ„é–¢ä¿‚ã®å¹¾ä½•å­¦çš„æ¤œè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
4. Persistent Homology: ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«è§£æã¨å¾ªç’°è«–ç†æ¤œå‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
5. Sheaf Attention: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰é–“ã®æ§‹é€ çš„æ•´åˆæ€§ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
6. Quantized HTT: é‡å­åŒ–ã•ã‚ŒãŸHolographic Tensor TrainåŸ‹ã‚è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
7. ãã®ä»–ã®æœ€é©åŒ–æŠ€è¡“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

é‡è¦ãªè¨­è¨ˆåŸå‰‡:
- Phase 7IntegratedModelã‚’ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨ï¼ˆResNetBKãƒ™ãƒ¼ã‚¹ï¼‰
- BK-Coreã®G_iiã‚’å–å¾—ã—ã¦ç‰©ç†æƒ…å ±ã¨ã—ã¦æ´»ç”¨
- O(N)è¤‡é›‘åº¦ã‚’ç¶­æŒ
- 8GB VRAMåˆ¶ç´„ã‚’æº€ãŸã™
"""
import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Tuple
from dataclasses import asdict
import time

from src.models.phase7.integrated_model import Phase7IntegratedModel
from src.models.phase1.htt_embedding import HTTDecoder
from .config import Phase8Config, Phase8Diagnostics
from .bk_core_hyperbolic import BKCoreHyperbolicIntegration, BKCoreHyperbolicConfig
from .ar_ssm_fusion import ARSSMHyperbolicFusion, ARSSMFusionConfig

# Import optimization kernels for training
try:
    from src.kernels.resonance_adaptive_curvature import ResonanceAdaptiveCurvature, StabilityMonitor
    _RESONANCE_OPT_AVAILABLE = True
except ImportError:
    _RESONANCE_OPT_AVAILABLE = False
    ResonanceAdaptiveCurvature = None
    StabilityMonitor = None
from .entailment_cones import EntailmentCones, EntailmentConeConfig
from .persistent_homology import HyperbolicPersistentHomology, PersistentHomologyConfig
from .sheaf_attention import SheafAttentionModule, SheafAttentionConfig
from .quantized_htt import (
    QuantizedHolographicTTEmbedding,
    QuantizationConfig,
    QuantizedHTTDecoder,
    AdaptiveRankQuantizedHTTEmbedding,
    AdaptiveQuantizedHTTDecoder,
)

# Import Hyperbolic Normalization (Phase 1)
try:
    from src.models.hyperbolic_normalization import HyperbolicRMSNorm
    _HYPERBOLIC_NORM_AVAILABLE = True
except ImportError:
    _HYPERBOLIC_NORM_AVAILABLE = False
    HyperbolicRMSNorm = None

# Import Resonant HTT Embedding (Riemannian Resonant Tunneling)
try:
    from src.models.phase1.resonant_htt_embedding import (
        ResonantHTTEmbedding,
        ResonantHTTDecoder,
        diagnose_vocab_size,
    )
    _RESONANT_HTT_AVAILABLE = True
except ImportError:
    _RESONANT_HTT_AVAILABLE = False
    ResonantHTTEmbedding = None
    ResonantHTTDecoder = None
    diagnose_vocab_size = None


class Phase8IntegratedModel(nn.Module):
    """
    Phase 8 Integrated Model
    
    Phase 7IntegratedModelã‚’ç¶™æ‰¿ã—ã€Phase 8ã®æ‹¡å¼µæ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
    1. Phase7IntegratedModelï¼ˆResNetBK + HTT + Hybrid Hyperbolic Attentionï¼‰
    2. BK-Core Hyperbolic Integrationï¼ˆG_iiã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰
    3. AR-SSM Hyperbolic Fusionï¼ˆAR-SSM + åŒæ›²ç©ºé–“ï¼‰
    4. ã‚ªãƒ—ã‚·ãƒ§ãƒ³: Entailment Cones, Persistent Homology, Sheaf Attention
    
    Requirements: Phase8 design.md Section 2
    """
    
    def __init__(self, config: Phase8Config):
        super().__init__()
        self.config = config
        
        # ========== Phase 7 Core Model ==========
        # Phase 7IntegratedModelã‚’ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨
        # ã“ã‚Œã«ã¯ResNetBK, HTT Embedding, Hybrid Hyperbolic AttentionãŒå«ã¾ã‚Œã‚‹
        # Phase8å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–ã—ã¦Phase7Configã‚’ä½œæˆ
        # Use dataclasses.fields() to properly get all inherited fields
        from dataclasses import fields as dc_fields
        
        phase7_config_dict = {}
        for f in dc_fields(config):
            phase7_config_dict[f.name] = getattr(config, f.name)
        
        # Phase8å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–
        phase8_specific_params = [
            # Component flags
            'use_bk_hyperbolic', 'use_ar_ssm_fusion',
            'enable_entailment_cones', 'enable_persistent_homology', 'enable_sheaf_attention',
            'enable_adaptive_computation', 'enable_koopman_bridge', 'enable_sparse_attention',
            'enable_kv_compression', 'enable_numerical_guards', 'enable_curvature_adaptation',
            # BK-Core settings
            'bk_hyperbolic_gate_scale', 'bk_hyperbolic_resonance_threshold',
            # AR-SSM settings
            'ar_ssm_max_rank', 'ar_ssm_min_rank',
            'ar_ssm_hyperbolic_rank_threshold', 'ar_ssm_curvature_adaptation_rate',
            # Entailment settings
            'entailment_aperture', 'entailment_margin',
            # Topology settings
            'topology_persistence_threshold', 'topology_max_dimension', 'topology_betti_threshold',
            'topology_loss_weight', 'topology_cycle_weight', 'topology_fragment_weight',
            'topology_subset_size',
            # Sheaf settings
            'sheaf_num_sections', 'sheaf_agreement_threshold',
            # Adaptive computation settings
            'adaptive_exit_threshold', 'adaptive_min_layers',
            # Sparse attention settings
            'sparse_top_k', 'sparse_block_size',
            # KV compression settings
            'kv_cache_dim', 'kv_eviction_threshold',
            'kv_use_reconstruction_loss', 'kv_reconstruction_weight',
            # Curvature settings
            'curvature_initial', 'curvature_min', 'curvature_max',
            # Numerical safety settings
            'max_norm', 'grad_clip',
            # Optimization & System settings (New in Phase 8)
            'use_torch_compile', 'compile_mode', 'compile_fullgraph',
            'use_flash_attention_2',
            'dataloader_num_workers', 'dataloader_pin_memory',
            'dataloader_prefetch_factor', 'dataloader_persistent_workers',
            'gradient_accumulation_steps',
            # Quantized HTT
            'quantized_htt',
            'low_rank_embedding', 'low_rank_ffn',
            'adaptive_rank_quantization', 'adaptive_rank_hot_bits', 'adaptive_rank_cold_bits',
            'adaptive_rank_hot', 'adaptive_rank_cold', 'adaptive_rank_frequency_threshold',
            'adaptive_rank_update_alpha', 'adaptive_rank_use_loss',
            # Phase 8 Optimization Kernels (NEW)
            'use_fused_mobius', 'use_green_function_cache', 'green_function_cache_size',
            'use_parallel_ssm_scan', 'use_fused_scattering_gate',
            'use_batched_hyperbolic_distance', 'use_resonance_adaptive_curvature',
            'resonance_threshold', 'curvature_adjustment_rate',
            'use_ternary_mobius_matmul', 'use_quantized_htt_fusion',
            # Resonant HTT (NEW)
            'use_resonant_htt', 'resonant_num_cores', 'use_zeta_init',
        ]

        # Phase 7Configã«å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã‚’å‰Šé™¤
        for param in phase8_specific_params:
            phase7_config_dict.pop(param, None)
        
        # Phase7Configã‚’ä½œæˆ
        from src.models.phase7.integrated_model import Phase7Config
        phase7_config = Phase7Config(**phase7_config_dict)
        self.phase7_model = Phase7IntegratedModel(phase7_config)

        # ========== Quantized HTT Replacement ==========
        # config.quantized_httãŒTrueã®å ´åˆã€é€šå¸¸ã®HTTã‚’é‡å­åŒ–ç‰ˆã«ç½®ãæ›ãˆã‚‹
        self.quantized_embedding = None
        if getattr(config, 'adaptive_rank_quantization', False):
            print("Phase 8: Activating AdaptiveRankQuantizedHTTEmbedding...")
            self.quantized_embedding = AdaptiveRankQuantizedHTTEmbedding(
                vocab_size=config.vocab_size,
                d_model=config.d_model,
                hot_rank=config.adaptive_rank_hot,
                cold_rank=config.adaptive_rank_cold,
                hot_bits=config.adaptive_rank_hot_bits,
                cold_bits=config.adaptive_rank_cold_bits,
                frequency_threshold=config.adaptive_rank_frequency_threshold,
                ema_alpha=config.adaptive_rank_update_alpha,
                phase_encoding=True,
            )
            self.phase7_model.htt_embedding = self.quantized_embedding
            self.phase7_model.model.token_embedding = self.quantized_embedding
            self.phase7_model.model.lm_head = AdaptiveQuantizedHTTDecoder(self.quantized_embedding)
        elif getattr(config, 'quantized_htt', False):
            print("Phase 8: Activating QuantizedHolographicTTEmbedding...")
            self.quantized_embedding = QuantizedHolographicTTEmbedding(
                vocab_size=config.vocab_size,
                d_model=config.d_model,
                rank=config.htt_rank,
                bits=8, # Default to 8-bit, could be configurable
                phase_encoding=True,
            )
            # Replace in Phase 7 Model
            self.phase7_model.htt_embedding = self.quantized_embedding
            self.phase7_model.model.token_embedding = self.quantized_embedding
            # Re-initialize decoder with new embedding
            self.phase7_model.model.lm_head = QuantizedHTTDecoder(self.quantized_embedding)
        
        # ========== Resonant HTT Replacement (Riemannian Resonant Tunneling) ==========
        # use_resonant_httãŒTrueã®å ´åˆã€é€šå¸¸ã®HTTã‚’ResonantHTTã«ç½®ãæ›ãˆã‚‹
        self.resonant_embedding = None
        if getattr(config, 'use_resonant_htt', False):
            if _RESONANT_HTT_AVAILABLE:
                # èªå½™ã‚µã‚¤ã‚ºã®è¨ºæ–­ã‚’å‡ºåŠ›
                if diagnose_vocab_size is not None:
                    diag = diagnose_vocab_size(config.vocab_size)
                    print(f"Phase 8: ğŸ”® Vocab Diagnosis: {diag['recommendation']}")
                
                print(f"Phase 8: Activating ResonantHTTEmbedding (Riemannian Resonant Tunneling)...")
                self.resonant_embedding = ResonantHTTEmbedding(
                    vocab_size=config.vocab_size,
                    d_model=config.d_model,
                    rank=config.htt_rank,
                    num_cores=getattr(config, 'resonant_num_cores', 4),
                    phase_encoding=True,
                    use_zeta_init=getattr(config, 'use_zeta_init', True),
                )
                # Replace in Phase 7 Model
                self.phase7_model.htt_embedding = self.resonant_embedding
                self.phase7_model.model.token_embedding = self.resonant_embedding
                # Re-initialize decoder with new embedding
                self.phase7_model.model.lm_head = ResonantHTTDecoder(self.resonant_embedding)
                print(f"Phase 8: âœ” ResonantHTT active - Condition number Îºâ‰ˆ1 guaranteed")
            else:
                print("Phase 8: âš  ResonantHTT requested but not available, using standard HTT")
        
        # ========== Phase 8 Core Extensions ==========
        # BK-Core Hyperbolic Integrationï¼ˆå¿…é ˆï¼‰
        if config.use_bk_hyperbolic:
            bk_config = BKCoreHyperbolicConfig(
                d_model=config.d_model,
                curvature=getattr(config, 'curvature_initial', 1.0),
                gate_scale=config.bk_hyperbolic_gate_scale,
                resonance_threshold=config.bk_hyperbolic_resonance_threshold,
                use_scattering_gate=True,
                use_resonance_detection=True,
                # Phase 8 kernel optimizations
                use_fused_mobius=getattr(config, 'use_fused_mobius', True),
                use_green_function_cache=getattr(config, 'use_green_function_cache', True),
                use_fused_scattering_gate=getattr(config, 'use_fused_scattering_gate', True),
                green_function_cache_size=getattr(config, 'green_function_cache_size', 512),
            )
            self.bk_hyperbolic = BKCoreHyperbolicIntegration(bk_config)
        else:
            self.bk_hyperbolic = None
        
        # AR-SSM Hyperbolic Fusionï¼ˆå¿…é ˆï¼‰
        if config.use_ar_ssm_fusion:
            ar_ssm_config = ARSSMFusionConfig(
                d_model=config.d_model,
                d_state=config.d_model // 4,  # çŠ¶æ…‹æ¬¡å…ƒã¯d_modelã®1/4
                max_rank=config.ar_ssm_max_rank,
                min_rank=config.ar_ssm_min_rank,
                curvature=getattr(config, 'curvature_initial', 1.0),
                distance_threshold=config.ar_ssm_hyperbolic_rank_threshold,
                curvature_adjustment_rate=config.ar_ssm_curvature_adaptation_rate,
                use_physics_gating=True,
                use_adaptive_rank=True,
                # Phase 8 kernel optimizations
                use_parallel_ssm_scan=getattr(config, 'use_parallel_ssm_scan', True),
                use_batched_hyperbolic_distance=getattr(config, 'use_batched_hyperbolic_distance', True),
            )
            self.ar_ssm_fusion = ARSSMHyperbolicFusion(ar_ssm_config)
        else:
            self.ar_ssm_fusion = None
        
        # ========== Phase 8 Optional Extensions ==========
        # Entailment Conesï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if config.enable_entailment_cones:
            entailment_config = EntailmentConeConfig(
                d_model=config.d_model,
                curvature=1.0,
                initial_aperture=config.entailment_aperture,
            )
            self.entailment_cones = EntailmentCones(entailment_config)
        else:
            self.entailment_cones = None
        
        # Persistent Homologyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if config.enable_persistent_homology:
            homology_config = PersistentHomologyConfig(
                d_model=config.d_model,
                max_dimension=config.topology_max_dimension,
                threshold_beta1=int(config.topology_betti_threshold * 10),  # é–¾å€¤ã‚’æ•´æ•°ã«å¤‰æ›
            )
            self.persistent_homology = HyperbolicPersistentHomology(homology_config)
        else:
            self.persistent_homology = None
        
        # Sheaf Attentionï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if config.enable_sheaf_attention:
            sheaf_config = SheafAttentionConfig(
                d_model=config.d_model,
                num_heads=config.num_heads,
                agreement_threshold=config.sheaf_agreement_threshold,
            )
            self.sheaf_attention = SheafAttentionModule(sheaf_config)
        else:
            self.sheaf_attention = None
        
        # è¨ºæ–­æƒ…å ±ã®åˆæœŸåŒ–
        self.diagnostics = Phase8Diagnostics()
        self.last_topology_loss: Optional[torch.Tensor] = None
        
        # ========== Hyperbolic Normalization (Phase 1) ==========
        # Add post-embedding normalization to stabilize gradients
        if _HYPERBOLIC_NORM_AVAILABLE:
            self.embedding_norm = HyperbolicRMSNorm(config.d_model, eps=1e-6)
            print("Phase 8: âœ” HyperbolicRMSNorm enabled for embedding stabilization")
        else:
            self.embedding_norm = nn.LayerNorm(config.d_model)
            print("Phase 8: Using standard LayerNorm (HyperbolicRMSNorm not available)")
    
    def forward(
        self,
        input_ids: torch.Tensor,
        return_diagnostics: bool = False,
    ) -> Tuple[torch.Tensor, Optional[Dict[str, Any]]]:
        """
        Forward pass
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. Phase 7 forwardï¼ˆResNetBK + HTT + Hybrid Hyperbolic Attentionï¼‰
        2. BK-Coreã‹ã‚‰G_iiã‚’å–å¾—
        3. BK-Core Hyperbolic Integrationï¼ˆG_iiã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰
        4. AR-SSM Hyperbolic Fusion
        5. ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã®é©ç”¨
        
        Args:
            input_ids: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID [batch, seq_len]
            return_diagnostics: è¨ºæ–­æƒ…å ±ã‚’è¿”ã™ã‹
        
        Returns:
            logits: å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆ [batch, seq_len, vocab_size]
            diagnostics: è¨ºæ–­æƒ…å ±ï¼ˆreturn_diagnostics=Trueã®å ´åˆï¼‰
        """
        start_time = time.time()
        
        # ========== Phase 7 Forward ==========
        # Phase 7ã®å®Œå…¨ãªå‰å‘ãè¨ˆç®—ã‚’å®Ÿè¡Œ
        # ã“ã‚Œã«ã¯ResNetBK, HTT Embedding, Hybrid Hyperbolic AttentionãŒå«ã¾ã‚Œã‚‹
        if return_diagnostics:
            logits, phase7_diagnostics = self.phase7_model(input_ids, return_diagnostics=True)
        else:
            logits = self.phase7_model(input_ids, return_diagnostics=False)
            phase7_diagnostics = {}
        
        # ========== BK-Coreã‹ã‚‰G_iiã‚’å–å¾— ==========
        # Phase 7ã®ResNetBKã‹ã‚‰BK-Coreã®G_iiã‚’å–å¾—
        G_ii = self._extract_green_function(input_ids)
        
        # ========== Phase 8 Extensions ==========
        # ä¸­é–“è¡¨ç¾ã‚’å–å¾—ï¼ˆPhase 7ã®åŸ‹ã‚è¾¼ã¿å±¤ã‹ã‚‰ï¼‰
        # HTT Embedding (Quantized or Standard)
        x = self.phase7_model.htt_embedding(input_ids)  # [batch, seq_len, d_model]
        
        # ========== Apply Hyperbolic Normalization ==========
        # Stabilize embeddings to prevent NaN in gradients
        x = self.embedding_norm(x)
        
        batch_size, seq_len, d_model = x.shape
        topology_loss_value = None
        
        # Task 38.2: BK-Core Hyperbolic Integration
        if self.bk_hyperbolic is not None and G_ii is not None:
            # G_iiã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            # G_iiã®å½¢çŠ¶ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å‡¦ç†
            if G_ii.dim() == 2:  # [batch, seq_len]
                # ãƒ€ãƒŸãƒ¼ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã«ã¯Phase7ã‹ã‚‰å–å¾—ã™ã¹ãï¼‰
                # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€å˜ä½è¡Œåˆ—ã‚’ä½¿ç”¨
                dummy_attn = torch.eye(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1, -1)
                bk_features, bk_diag = self.bk_hyperbolic(x, attention_weights=dummy_attn)
            else:
                # G_iiãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                bk_features, bk_diag = self.bk_hyperbolic(x, attention_weights=None)
            
            # è¨ºæ–­æƒ…å ±ã‚’åé›†
            if return_diagnostics:
                self.diagnostics.bk_hyperbolic_gate_mean = bk_diag.get('gate_mean', torch.tensor(0.0)).item()
                self.diagnostics.bk_hyperbolic_gate_std = bk_diag.get('gate_std', torch.tensor(0.0)).item()
                self.diagnostics.bk_resonance_detected = bk_diag.get('is_resonant', torch.tensor(False)).item()
                self.diagnostics.bk_resonance_strength = bk_diag.get('resonance_strength', torch.tensor(0.0)).item()
            
            # BKç‰¹å¾´é‡ã‚’å…ƒã®ç‰¹å¾´é‡ã«åŠ ç®—ï¼ˆæ®‹å·®æ¥ç¶šï¼‰
            x = x + bk_features
        
        # Task 38.3: AR-SSM Hyperbolic Fusion
        if self.ar_ssm_fusion is not None:
            # AR-SSMã¨åŒæ›²ç©ºé–“ã‚’èåˆ
            # G_iiã‚’ç‰©ç†æƒ…å ±ã¨ã—ã¦æ¸¡ã™
            ar_ssm_output, ar_ssm_diag = self.ar_ssm_fusion(x, G_ii)
            
            # è¨ºæ–­æƒ…å ±ã‚’åé›† (handle both tensor and float values)
            if return_diagnostics:
                rank_val = ar_ssm_diag.get('effective_rank_mean', 0.0)
                dist_val = ar_ssm_diag.get('distance_mean', 0.0)
                self.diagnostics.ar_ssm_rank_mean = rank_val.item() if hasattr(rank_val, 'item') else float(rank_val)
                self.diagnostics.ar_ssm_hyperbolic_distance_mean = dist_val.item() if hasattr(dist_val, 'item') else float(dist_val)
            
                # æ›²ç‡èª¿æ•´ã®ææ¡ˆãŒã‚ã‚Œã°è¨˜éŒ²
                if 'suggested_curvature' in ar_ssm_diag:
                    self.diagnostics.ar_ssm_curvature_adjusted = True
            
            # AR-SSMå‡ºåŠ›ã‚’å…ƒã®ç‰¹å¾´é‡ã«åŠ ç®—ï¼ˆæ®‹å·®æ¥ç¶šï¼‰
            x = x + ar_ssm_output
        
        # Task 38.4: Entailment Conesï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.entailment_cones is not None:
            # è«–ç†çš„å«æ„é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
            # Optimized: Avoid loop if possible, or keep simple
            # (Vectorization will be handled in separate optimization step)
            if return_diagnostics:
                 entailment_diag = self._check_entailment(x)
                 self.diagnostics.entailment_violation_rate = entailment_diag.get('violation_rate', 0.0)
                 self.diagnostics.avg_aperture = entailment_diag.get('avg_aperture', 0.0)
        
        # Task 38.5: Persistent Homologyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.persistent_homology is not None:
            # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«è§£æ
            # Only analyze first item in batch to save time
            if return_diagnostics:
                homology_diag = self._analyze_topology(x)
                self.diagnostics.betti_numbers = homology_diag.get('betti_numbers', [])
                self.diagnostics.persistent_entropy = homology_diag.get('persistent_entropy', 0.0)
                self.diagnostics.circular_reasoning_detected = homology_diag.get('circular_reasoning', False)

                # å¾ªç’°è«–ç†ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€æ›²ç‡ã‚’å¢—åŠ ã•ã›ã‚‹ææ¡ˆ
                if self.diagnostics.circular_reasoning_detected:
                    self.diagnostics.topology_curvature_adjustment_suggested = True

            if getattr(self.config, 'topology_loss_weight', 0.0) > 0:
                topo_loss_raw = self.persistent_homology.regularization_loss(
                    x,
                    curvature=torch.tensor(self.config.curvature_initial, device=x.device, dtype=x.dtype),
                    cycle_weight=self.config.topology_cycle_weight,
                    fragmentation_weight=self.config.topology_fragment_weight,
                    max_tokens=self.config.topology_subset_size,
                )
                topology_loss_value = self.config.topology_loss_weight * topo_loss_raw
                self.last_topology_loss = topology_loss_value
                if return_diagnostics:
                    self.diagnostics.topology_loss = float(topology_loss_value.detach().item())
                    self.diagnostics.topology_subset_tokens = min(self.config.topology_subset_size, x.shape[1])
        
        # Task 38.6: Sheaf Attentionï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.sheaf_attention is not None:
            # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰é–“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            if return_diagnostics:
                sheaf_diag = self._check_sheaf_consistency(x)
                self.diagnostics.sheaf_agreement_mean = sheaf_diag.get('agreement_mean', 0.0)
                self.diagnostics.sheaf_consensus_rate = sheaf_diag.get('consensus_rate', 0.0)
        
        # ========== Performance Metrics ==========
        if return_diagnostics:
            forward_time = (time.time() - start_time) * 1000  # ms
            self.diagnostics.forward_time_ms = forward_time
            
            batch_size, seq_len = input_ids.shape
            tokens = batch_size * seq_len
            self.diagnostics.tokens_per_second = tokens / (forward_time / 1000) if forward_time > 0 else 0.0
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            if torch.cuda.is_available():
                self.diagnostics.peak_memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
            
            # Phase 7ã®è¨ºæ–­æƒ…å ±ã¨ãƒãƒ¼ã‚¸
            all_diagnostics = {
                'phase7': phase7_diagnostics,
                'phase8': asdict(self.diagnostics),
            }

            # Attach optional auxiliary losses to diagnostics
            if self.persistent_homology is not None and topology_loss_value is not None:
                all_diagnostics['phase8']['topology_aux_loss'] = float(topology_loss_value.detach().item())
            
            return logits, all_diagnostics
        
        return logits, None
    
    def _extract_green_function(self, input_ids: torch.Tensor) -> Optional[torch.Tensor]:
        """
        Phase 7ã®ResNetBKã‹ã‚‰BK-Coreã®G_iiã‚’æŠ½å‡º
        
        Args:
            input_ids: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID
        
        Returns:
            G_ii: ã‚°ãƒªãƒ¼ãƒ³é–¢æ•°å¯¾è§’æˆåˆ†ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        """
        try:
            # Phase 7ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰BK-Coreã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
            # ResNetBKã®ãƒ–ãƒ­ãƒƒã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹
            if hasattr(self.phase7_model.model, 'blocks'):
                # æœ€åˆã®ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰G_iiã‚’å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
                first_block = self.phase7_model.model.blocks[0]
                if hasattr(first_block, 'bk_layer'):
                    bk_layer = first_block.bk_layer
                    if hasattr(bk_layer, 'last_G_ii'):
                        return bk_layer.last_G_ii
            
            # G_iiãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã™
            return None
        except Exception:
            return None
    
    def _check_entailment(self, x: torch.Tensor) -> Dict[str, float]:
        """
        è«–ç†çš„å«æ„é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆTask 38.4ï¼‰
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [batch, seq_len, d_model]
        
        Returns:
            diagnostics: è¨ºæ–­æƒ…å ±
        """
        if self.entailment_cones is None:
            return {'violation_rate': 0.0, 'avg_aperture': 0.0}
        
        try:
            batch_size, seq_len, d_model = x.shape
            
            # åŒæ›²ç©ºé–“ã«æŠ•å½±ï¼ˆãƒã‚¢ãƒ³ã‚«ãƒ¬ãƒœãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰
            x_norm = torch.norm(x, dim=-1, keepdim=True)
            x_hyperbolic = x / (x_norm + 1e-8) * torch.tanh(x_norm)
            
            # éš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®å«æ„é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
            violations = 0
            total_checks = 0
            apertures = []
            
            # OPTIMIZATION: Use slicing instead of loop for speed
            # Premise: x[:, :-1], Hypothesis: x[:, 1:]
            premise = x_hyperbolic[:, :-1, :] # [B, L-1, D]
            hypothesis = x_hyperbolic[:, 1:, :] # [B, L-1, D]

            # Flatten to [B*(L-1), D] for batch processing
            premise_flat = premise.reshape(-1, d_model)
            hypothesis_flat = hypothesis.reshape(-1, d_model)

            # å«æ„ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            entailment_score = self.entailment_cones.compute_entailment_score(
                premise_flat, hypothesis_flat
            )

            # é–‹å£è§’ã‚’è¨ˆç®— (Sample a subset if too large?)
            aperture = self.entailment_cones.compute_aperture(premise_flat)
            apertures.append(aperture.mean().item())

            # é•åã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå«æ„ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ä»¥ä¸‹ï¼‰
            violations = (entailment_score < self.config.entailment_margin).sum().item()
            total_checks = premise_flat.shape[0]
            
            violation_rate = violations / total_checks if total_checks > 0 else 0.0
            avg_aperture = sum(apertures) / len(apertures) if apertures else 0.0
            
            return {
                'violation_rate': violation_rate,
                'avg_aperture': avg_aperture,
            }
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return {'violation_rate': 0.0, 'avg_aperture': 0.0}
    
    def _analyze_topology(self, x: torch.Tensor) -> Dict[str, Any]:
        """
        ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«è§£æã‚’å®Ÿè¡Œï¼ˆTask 38.5ï¼‰
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [batch, seq_len, d_model]
        
        Returns:
            diagnostics: è¨ºæ–­æƒ…å ±
        """
        if self.persistent_homology is None:
            return {
                'betti_numbers': [0, 0],
                'persistent_entropy': 0.0,
                'circular_reasoning': False,
            }
        
        try:
            batch_size, seq_len, d_model = x.shape
            
            # åŒæ›²ç©ºé–“ã«æŠ•å½±
            x_norm = torch.norm(x, dim=-1, keepdim=True)
            x_hyperbolic = x / (x_norm + 1e-8) * torch.tanh(x_norm)
            
            # ãƒãƒƒãƒã®æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã§è§£æï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
            sample = x_hyperbolic[0]  # [seq_len, d_model]
            
            # Bettiæ•°ã‚’è¨ˆç®—
            betti_numbers = self.persistent_homology.compute_betti_numbers(sample)
            
            # æ°¸ç¶šã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—
            persistent_entropy = self.persistent_homology.compute_persistent_entropy(sample)
            
            # å¾ªç’°è«–ç†ã‚’æ¤œå‡ºï¼ˆÎ²â‚ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆï¼‰
            circular_reasoning = False
            if len(betti_numbers) > 1:
                beta_1 = betti_numbers[1]
                circular_reasoning = beta_1 > self.config.topology_betti_threshold
            
            return {
                'betti_numbers': betti_numbers,
                'persistent_entropy': persistent_entropy.item() if isinstance(persistent_entropy, torch.Tensor) else persistent_entropy,
                'circular_reasoning': circular_reasoning,
            }
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return {
                'betti_numbers': [0, 0],
                'persistent_entropy': 0.0,
                'circular_reasoning': False,
            }
    
    def _check_sheaf_consistency(self, x: torch.Tensor) -> Dict[str, float]:
        """
        Sheaf Attentionã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆTask 38.6ï¼‰
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [batch, seq_len, d_model]
        
        Returns:
            diagnostics: è¨ºæ–­æƒ…å ±
        """
        if self.sheaf_attention is None:
            return {'agreement_mean': 0.0, 'consensus_rate': 0.0}
        
        try:
            batch_size, seq_len, d_model = x.shape
            
            # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã€num_headsã§å‡ç­‰åˆ†å‰²ï¼‰
            num_heads = self.config.num_heads
            head_dim = d_model // num_heads
            
            # [batch, seq_len, num_heads, head_dim]ã«å¤‰å½¢
            x_heads = x.view(batch_size, seq_len, num_heads, head_dim)
            
            # Optimize: Vectorized computation of adjacent head agreement
            # Heads: 0..N-2 vs 1..N-1
            head_i = x_heads[:, :, :-1, :] # [B, L, H-1, D]
            head_j = x_heads[:, :, 1:, :]  # [B, L, H-1, D]
            
            head_i_norm = torch.nn.functional.normalize(head_i, dim=-1)
            head_j_norm = torch.nn.functional.normalize(head_j, dim=-1)
            
            # Dot product
            agreement = (head_i_norm * head_j_norm).sum(dim=-1) # [B, L, H-1]
            
            agreement_mean = agreement.mean().item()
            consensus_rate = (agreement > self.config.sheaf_agreement_threshold).float().mean().item()
            
            return {
                'agreement_mean': agreement_mean,
                'consensus_rate': consensus_rate,
            }
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return {'agreement_mean': 0.0, 'consensus_rate': 0.0}
    
    def get_total_parameter_count(self) -> int:
        """
        ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—
        
        Returns:
            total_params: ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        """
        return sum(p.numel() for p in self.parameters())
    
    def get_phase7_parameter_count(self) -> int:
        """
        Phase 7ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—
        
        Returns:
            phase7_params: Phase 7ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        """
        return sum(p.numel() for p in self.phase7_model.parameters())
    
    def get_phase8_extension_parameter_count(self) -> int:
        """
        Phase 8æ‹¡å¼µã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—
        
        Returns:
            extension_params: Phase 8æ‹¡å¼µã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        """
        total = self.get_total_parameter_count()
        phase7 = self.get_phase7_parameter_count()
        return total - phase7
    
    def get_diagnostics(self) -> Phase8Diagnostics:
        """
        ç¾åœ¨ã®è¨ºæ–­æƒ…å ±ã‚’å–å¾—
        
        Returns:
            diagnostics: è¨ºæ–­æƒ…å ±
        """
        return self.diagnostics
    
    def reset_diagnostics(self):
        """è¨ºæ–­æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.diagnostics = Phase8Diagnostics()


def create_phase8_model(
    vocab_size: int = 50257,
    d_model: int = 256,
    n_layers: int = 8,
    htt_rank: int = 16,
    use_bk_hyperbolic: bool = True,
    use_ar_ssm_fusion: bool = True,
    enable_entailment_cones: bool = False,
    enable_persistent_homology: bool = False,
    enable_sheaf_attention: bool = False,
    **kwargs,
) -> Phase8IntegratedModel:
    """
    Phase 8 Integrated Modelã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
    
    Args:
        vocab_size: èªå½™ã‚µã‚¤ã‚º
        d_model: ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ
        n_layers: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        htt_rank: HTTåŸ‹ã‚è¾¼ã¿ã®ãƒ©ãƒ³ã‚¯
        use_bk_hyperbolic: BK-Core Hyperbolic Integrationã‚’ä½¿ç”¨ã™ã‚‹ã‹
        use_ar_ssm_fusion: AR-SSM Hyperbolic Fusionã‚’ä½¿ç”¨ã™ã‚‹ã‹
        enable_entailment_cones: Entailment Conesã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        enable_persistent_homology: Persistent Homologyã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        enable_sheaf_attention: Sheaf Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        **kwargs: ãã®ä»–ã®è¨­å®š
    
    Returns:
        Phase8IntegratedModel instance
    """
    config = Phase8Config(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        htt_rank=htt_rank,
        use_bk_hyperbolic=use_bk_hyperbolic,
        use_ar_ssm_fusion=use_ar_ssm_fusion,
        enable_entailment_cones=enable_entailment_cones,
        enable_persistent_homology=enable_persistent_homology,
        enable_sheaf_attention=enable_sheaf_attention,
        **kwargs,
    )
    return Phase8IntegratedModel(config)
