{
  "timestamp": "2025-11-29 10:45:17",
  "gpu": "NVIDIA GeForce RTX 3080 Laptop GPU",
  "total_memory_gb": 8.0,
  "phase8_available": false,
  "results": [
    {
      "config": {
        "d_model": 512,
        "n_layers": 8,
        "num_heads": 8,
        "seq_len": 512,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 83.46,
      "parameters_billions": 0.083,
      "peak_memory_gb": 0.209,
      "tokens_per_sec": 37605,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 1024,
        "n_layers": 16,
        "num_heads": 16,
        "seq_len": 512,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 356.75,
      "parameters_billions": 0.357,
      "peak_memory_gb": 0.644,
      "tokens_per_sec": 20854,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 2048,
        "n_layers": 24,
        "num_heads": 16,
        "seq_len": 512,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 1722.29,
      "parameters_billions": 1.722,
      "peak_memory_gb": 2.749,
      "tokens_per_sec": 10862,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 1024,
        "n_layers": 16,
        "num_heads": 16,
        "seq_len": 1024,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 356.75,
      "parameters_billions": 0.357,
      "peak_memory_gb": 0.714,
      "tokens_per_sec": 25848,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 1024,
        "n_layers": 16,
        "num_heads": 16,
        "seq_len": 2048,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 356.75,
      "parameters_billions": 0.357,
      "peak_memory_gb": 0.851,
      "tokens_per_sec": 41635,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 1024,
        "n_layers": 16,
        "num_heads": 16,
        "seq_len": 512,
        "batch_size": 2,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 356.75,
      "parameters_billions": 0.357,
      "peak_memory_gb": 0.714,
      "tokens_per_sec": 40627,
      "status": "SUCCESS"
    },
    {
      "config": {
        "d_model": 1536,
        "n_layers": 32,
        "num_heads": 12,
        "seq_len": 512,
        "batch_size": 1,
        "vocab_size": 50257,
        "use_hyperbolic_ssm": true,
        "use_linear_attention": true,
        "use_block_distance": true,
        "use_entailment_cones": false,
        "use_sheaf_attention": false,
        "use_quantization": false,
        "gradient_checkpointing": true,
        "mixed_precision": true
      },
      "parameters_millions": 1293.34,
      "parameters_billions": 1.293,
      "peak_memory_gb": 2.095,
      "tokens_per_sec": 10842,
      "status": "SUCCESS"
    }
  ]
}