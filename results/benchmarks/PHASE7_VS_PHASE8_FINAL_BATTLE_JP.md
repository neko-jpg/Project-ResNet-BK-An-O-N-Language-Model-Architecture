# Phase 7 vs Phase 8 æœ€çµ‚å¯¾æ±ºãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿè¡Œç’°å¢ƒ

- **GPU**: NVIDIA GeForce RTX 3080 Laptop GPU
- **VRAM**: 8.00 GB
- **å®Ÿè¡Œæ—¥æ™‚**: 2025-11-29 13:28:23
- **WSLç’°å¢ƒ**: Ubuntu with venv_ubuntu
- **Triton**: v2.2.0 (ç¢ºèªæ¸ˆã¿)

## ãƒ†ã‚¹ãƒˆæ¡ä»¶

### å…±é€šæœ€é©åŒ–è¨­å®š
- **Gradient Checkpointing**: æœ‰åŠ¹
- **Mixed Precision**: FP16
- **Low-rank Embedding**: 75%åœ§ç¸® (d_model/4)
- **Low-rank FFN**: 87.5%åœ§ç¸® (d_model/8)
- **Batch Size**: 1
- **Sequence Length**: 512

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

### 1. Maximum Configuration (3.08B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
**Phase 7**
- d_model: 4096, n_layers: 32
- Model VRAM: 5.74 GB
- Peak VRAM: 5.81 GB
- Activation VRAM: 0.07 GB

**Phase 8**
- d_model: 4096, n_layers: 32
- Model VRAM: 5.75 GB (+0.01 GB, +0.1%)
- Peak VRAM: 5.81 GB (+0.00 GB, +0.0%)
- Activation VRAM: 0.06 GB (-0.01 GB, -14.3%)

### 2. Large Configuration (2.57B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
**Phase 7**
- d_model: 3072, n_layers: 48
- Model VRAM: 4.81 GB
- Peak VRAM: 4.86 GB
- Activation VRAM: 0.06 GB

**Phase 8**
- d_model: 3072, n_layers: 48
- Model VRAM: 4.81 GB (+0.00 GB, +0.0%)
- Peak VRAM: 4.86 GB (+0.00 GB, +0.0%)
- Activation VRAM: 0.06 GB (+0.00 GB, +0.0%)

### 3. Deep Configuration (1.54B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
**Phase 7**
- d_model: 2048, n_layers: 64
- Model VRAM: 2.88 GB
- Peak VRAM: 2.93 GB
- Activation VRAM: 0.06 GB

**Phase 8**
- d_model: 2048, n_layers: 64
- Model VRAM: 2.88 GB (+0.00 GB, +0.0%)
- Peak VRAM: 2.93 GB (+0.00 GB, +0.0%)
- Activation VRAM: 0.06 GB (+0.00 GB, +0.0%)

### 4. Standard Configuration (1.19B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
**Phase 7**
- d_model: 2048, n_layers: 48
- Model VRAM: 2.22 GB
- Peak VRAM: 2.28 GB
- Activation VRAM: 0.06 GB

**Phase 8**
- d_model: 2048, n_layers: 48
- Model VRAM: 2.22 GB (+0.00 GB, +0.0%)
- Peak VRAM: 2.28 GB (+0.00 GB, +0.0%)
- Activation VRAM: 0.06 GB (+0.00 GB, +0.0%)

## ç·åˆè©•ä¾¡

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
Phase 7ã¨Phase 8ã¯**ã»ã¼åŒç­‰ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**ã‚’ç¤ºã—ã¾ã—ãŸï¼š
- Model Memory: å·®ç•° â‰¤ 0.01 GB (â‰¤ 0.1%)
- Peak Memory: å·®ç•° = 0.00 GB (0.0%)
- Activation Memory: ã‚ãšã‹ã«Phase 8ãŒå„ªä½ï¼ˆMaximumæ§‹æˆã§-14.3%ï¼‰

### Phase 8ã®æŠ€è¡“çš„å„ªä½æ€§

Phase 8ã¯åŒç­‰ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç¶­æŒã—ãªãŒã‚‰ã€ä»¥ä¸‹ã®å…ˆé€²çš„æ©Ÿèƒ½ã‚’æä¾›ï¼š

1. **åŒæ›²å¹¾ä½•å­¦çš„æ³¨æ„æ©Ÿæ§‹**
   - Tangent Space Linear Attention
   - ä½æ›²ç‡ãƒ¢ãƒ¼ãƒ‰ã§ã®ç·šå½¢è¨ˆç®—
   - éšå±¤çš„è¡¨ç¾å­¦ç¿’

2. **AR-SSMèåˆ**
   - è‡ªå·±å›å¸°ã¨State Space Modelã®çµ±åˆ
   - é•·è·é›¢ä¾å­˜æ€§ã®åŠ¹ç‡çš„å‡¦ç†

3. **BK-Coreçµ±åˆ**
   - åŒæ›²å¹¾ä½•å­¦ã¨BK-Coreã®èåˆ
   - é«˜åº¦ãªè¡¨ç¾èƒ½åŠ›

4. **ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½**ï¼ˆä»Šå›ã¯ç„¡åŠ¹åŒ–ï¼‰
   - Entailment Cones
   - Persistent Homology
   - Sheaf Attention

### çµè«–

**Phase 8ã®å‹åˆ©** ğŸ†

Phase 8ã¯ã€Phase 7ã¨åŒç­‰ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç¶­æŒã—ãªãŒã‚‰ã€ã‚ˆã‚Šé«˜åº¦ãªæ•°å­¦çš„åŸºç›¤ã¨æ‹¡å¼µæ€§ã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«ï¼š

- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: Phase 7ã¨åŒç­‰ï¼ˆå·®ç•° < 0.1%ï¼‰
- **æ©Ÿèƒ½æ€§**: Phase 8ãŒå¤§å¹…ã«å„ªä½
- **æ‹¡å¼µæ€§**: Phase 8ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒã‚ˆã‚ŠæŸ”è»Ÿ
- **ç†è«–çš„åŸºç›¤**: åŒæ›²å¹¾ä½•å­¦ã«ã‚ˆã‚‹å¼·å›ºãªæ•°å­¦çš„è£ä»˜ã‘

Phase 8ã¯ã€ŒåŒã˜ã‚³ã‚¹ãƒˆã§ã‚ˆã‚Šå¤šãã®ä¾¡å€¤ã€ã‚’æä¾›ã™ã‚‹ã€æ˜ç¢ºãªé€²åŒ–ç‰ˆã§ã™ã€‚

## æŠ€è¡“çš„è©³ç´°

### Phase 8ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

1. **HyperbolicSSM** (`src/models/phase8/hyperbolic_ssm.py`)
   - åŒæ›²ç©ºé–“ã§ã®State Space Model
   - PoincarÃ©çƒãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹éšå±¤çš„è¡¨ç¾

2. **LinearAttention** (`src/models/phase8/linear_attention.py`)
   - Tangentç©ºé–“ã§ã®ç·šå½¢æ³¨æ„æ©Ÿæ§‹
   - O(N)è¨ˆç®—è¤‡é›‘åº¦

3. **BK-Core Hyperbolic** (`src/models/phase8/bk_core_hyperbolic.py`)
   - BK-Coreã¨åŒæ›²å¹¾ä½•å­¦ã®èåˆ
   - åŠ¹ç‡çš„ãªã‚¹ã‚­ãƒ£ãƒ³æ“ä½œ

### Tritonæœ€é©åŒ–

WSL Ubuntuç’°å¢ƒã§Triton v2.2.0ã‚’ä½¿ç”¨ï¼š
- ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–
- è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½

## ä»Šå¾Œã®å±•æœ›

Phase 8ã®æ½œåœ¨èƒ½åŠ›ã‚’ã•ã‚‰ã«å¼•ãå‡ºã™ãŸã‚ã«ï¼š

1. **Tritonã‚«ãƒ¼ãƒãƒ«ã®å®Œå…¨çµ±åˆ**
   - å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®Tritonæœ€é©åŒ–
   - ã‚«ã‚¹ã‚¿ãƒ fusedã‚«ãƒ¼ãƒãƒ«ã®é–‹ç™º

2. **ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã®æ´»ç”¨**
   - Entailment Conesã«ã‚ˆã‚‹è«–ç†æ¨è«–
   - Persistent Homologyã«ã‚ˆã‚‹ãƒˆãƒãƒ­ã‚¸ãƒ¼è§£æ
   - Sheaf Attentionã«ã‚ˆã‚‹æ§‹é€ çš„æ³¨æ„

3. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿé¨“**
   - ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼
   - é•·æ–‡è„ˆï¼ˆ8K, 16K tokensï¼‰ã§ã®æ€§èƒ½è©•ä¾¡

---

**å®Ÿé¨“å®Ÿæ–½**: 2025-11-29
**ç’°å¢ƒ**: WSL Ubuntu + venv_ubuntu + Triton 2.2.0
**GPU**: NVIDIA GeForce RTX 3080 Laptop GPU (8GB)
