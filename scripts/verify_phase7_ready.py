#!/usr/bin/env python3
"""
Phase 7 Ë®ìÁ∑¥Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ„Çπ„ÇØ„É™„Éó„Éà

„Åì„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅØ„ÄÅPhase 7 Chat AIË®ìÁ∑¥„ÇíÈñãÂßã„Åô„ÇãÂâç„Å´
ÂøÖË¶Å„Å™Áí∞Â¢É„Åå„Åô„Åπ„Å¶Êï¥„Å£„Å¶„ÅÑ„Çã„Åã„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ

Usage:
    python scripts/verify_phase7_ready.py
    make verify-phase7
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))


def check_python():
    """Python„Éê„Éº„Ç∏„Éß„É≥„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("1. Python Version Check")
    version = sys.version_info
    if version.major >= 3 and version.minor >= 10:
        print(f"   ‚úì Python {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"   ‚úó Python {version.major}.{version.minor} (3.10+ required)")
        return False


def check_cuda():
    """CUDA„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("\n2. CUDA Check")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"   ‚úì CUDA available")
            print(f"   ‚úì GPU: {gpu_name}")
            print(f"   ‚úì VRAM: {vram_gb:.1f} GB")
            
            if vram_gb < 8:
                print(f"   ‚ö† Warning: VRAM < 8GB. May need to reduce model size.")
            return True
        else:
            print("   ‚úó CUDA not available")
            return False
    except ImportError:
        print("   ‚úó PyTorch not installed")
        return False


def check_triton():
    """Triton„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("\n3. Triton Check")
    try:
        import triton
        version = getattr(triton, '__version__', 'unknown')
        print(f"   ‚úì Triton {version} installed")
        
        # Á∞°Âçò„Å™„Ç´„Éº„Éç„É´„ÉÜ„Çπ„Éà
        try:
            import triton.language as tl
            print("   ‚úì Triton language module available")
            return True
        except Exception as e:
            print(f"   ‚ö† Triton language import warning: {e}")
            return True  # Triton„ÅØ„ÅÇ„Çã„ÅåË≠¶Âëä
    except ImportError:
        print("   ‚ö† Triton not installed (PyTorch fallback will be used)")
        return False


def check_datasets():
    """„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("\n4. Dataset Check")
    
    # dataset_mixing.yaml
    config_path = Path("configs/dataset_mixing.yaml")
    if not config_path.exists():
        print("   ‚úó configs/dataset_mixing.yaml not found")
        print("   ‚Üí Run 'make recipe' to configure datasets")
        return False
    
    print(f"   ‚úì {config_path} exists")
    
    # „Éá„Éº„Çø„Çª„ÉÉ„Éà„Éï„Ç©„É´„ÉÄ
    import yaml
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)
    
    datasets = cfg.get('datasets', {})
    found = 0
    missing = []
    
    for name, info in datasets.items():
        path = Path(info.get('path', ''))
        bin_file = path / 'train.bin'
        idx_file = path / 'train.idx'
        
        if bin_file.exists() and idx_file.exists():
            found += 1
            print(f"   ‚úì {name}: {path}")
        else:
            missing.append(name)
    
    if missing:
        print(f"   ‚ö† Missing datasets: {', '.join(missing)}")
        print("   ‚Üí Run 'make data-lite' or 'make data' to download")
    
    if found > 0:
        print(f"   ‚úì {found}/{len(datasets)} datasets ready")
        return True
    else:
        print("   ‚úó No datasets found")
        return False


def check_model_config():
    """„É¢„Éá„É´Ë®≠ÂÆö„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("\n5. Model Config Check")
    
    config_path = Path("configs/phase7_max_push.yaml")
    if not config_path.exists():
        print(f"   ‚úó {config_path} not found")
        return False
    
    import yaml
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)
    
    print(f"   ‚úì {config_path} exists")
    print(f"   - d_model: {cfg.get('d_model', 'N/A')}")
    print(f"   - n_layers: {cfg.get('n_layers', 'N/A')}")
    print(f"   - n_seq: {cfg.get('n_seq', 'N/A')}")
    print(f"   - batch_size: {cfg.get('batch_size', 'N/A')}")
    print(f"   - gradient_accumulation: {cfg.get('gradient_accumulation_steps', 'N/A')}")
    
    return True


def check_imports():
    """ÂøÖË¶Å„Å™„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ"""
    print("\n6. Module Import Check")
    
    modules = [
        ("torch", "PyTorch"),
        ("yaml", "PyYAML"),
        ("numpy", "NumPy"),
    ]
    
    all_ok = True
    for module, name in modules:
        try:
            __import__(module)
            print(f"   ‚úì {name}")
        except ImportError:
            print(f"   ‚úó {name} not installed")
            all_ok = False
    
    # „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É¢„Ç∏„É•„Éº„É´
    try:
        from src.utils.data_utils import get_mixed_data_loader
        print("   ‚úì src.utils.data_utils")
    except ImportError as e:
        print(f"   ‚ö† src.utils.data_utils: {e}")
    
    try:
        from src.models.phase7.integrated_model import Phase7IntegratedModel
        print("   ‚úì src.models.phase7.integrated_model")
    except ImportError as e:
        print(f"   ‚ö† src.models.phase7: {e}")
    
    return all_ok


def test_model_creation():
    """„É¢„Éá„É´‰ΩúÊàê„ÉÜ„Çπ„Éà"""
    print("\n7. Model Creation Test")
    
    try:
        import torch
        import gc
        
        # „É°„É¢„É™„ÇØ„É™„Ç¢
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Â∞è„Åï„ÅÑ„É¢„Éá„É´„Åß„ÉÜ„Çπ„Éà
        from scripts.train_phase7_max import Phase7MaxModel
        
        model = Phase7MaxModel(
            vocab_size=1000,
            d_model=256,
            n_layers=2,
            n_seq=64,
            num_heads=4,
            embed_rank=32,
            ffn_rank=32,
            head_rank=32,
            use_checkpoint=True,
        )
        
        if torch.cuda.is_available():
            model = model.cuda().half()
            
            # Forward pass test
            x = torch.randint(0, 1000, (1, 64), device='cuda')
            with torch.cuda.amp.autocast():
                out = model(x)
            
            print(f"   ‚úì Model created successfully")
            print(f"   ‚úì Forward pass OK (output shape: {out.shape})")
            
            # Backward pass test
            loss = out.mean()
            loss.backward()
            print("   ‚úì Backward pass OK")
            
            del model, x, out, loss
            gc.collect()
            torch.cuda.empty_cache()
            
            return True
        else:
            print("   ‚ö† CUDA not available, skipping GPU test")
            return True
            
    except Exception as e:
        print(f"   ‚úó Model test failed: {e}")
        return False


def main():
    print("=" * 60)
    print("Phase 7 Chat AI Training Environment Check")
    print("=" * 60)
    
    results = {
        "Python": check_python(),
        "CUDA": check_cuda(),
        "Triton": check_triton(),
        "Datasets": check_datasets(),
        "Config": check_model_config(),
        "Imports": check_imports(),
        "Model": test_model_creation(),
    }
    
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    
    all_pass = True
    critical_fail = False
    
    for name, passed in results.items():
        status = "‚úì" if passed else "‚úó"
        print(f"  {status} {name}")
        if not passed:
            all_pass = False
            if name in ["Python", "CUDA", "Model"]:
                critical_fail = True
    
    print()
    
    if all_pass:
        print("üéâ All checks passed! Ready to train.")
        print("\nStart training with:")
        print("  make train-chat")
        print("\nOr test with dummy data:")
        print("  make train-chat-test")
        return 0
    elif critical_fail:
        print("‚ùå Critical checks failed. Please fix before training.")
        return 1
    else:
        print("‚ö† Some checks failed, but training may still work.")
        print("\nTry:")
        print("  make train-chat-test  # Test with dummy data first")
        return 0


if __name__ == "__main__":
    sys.exit(main())
