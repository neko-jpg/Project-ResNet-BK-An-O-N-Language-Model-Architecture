#!/usr/bin/env python3
"""
Phase 8 Optimization Benchmark - Rigorous Performance Testing

åŽ³æ ¼ãªãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆï¼šæœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ« vs ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

æ¸¬å®šé …ç›®:
1. å€‹åˆ¥ã‚«ãƒ¼ãƒãƒ«ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ï¼ˆå„æœ€é©åŒ–ã‚’å€‹åˆ¥ã«æ¸¬å®šï¼‰
2. ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ï¼ˆå…¨æœ€é©åŒ–ã®ç·åˆåŠ¹æžœï¼‰
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
4. çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆè¤‡æ•°å›žå®Ÿè¡Œã§æ¨™æº–åå·®ã‚’è¨ˆç®—ï¼‰

Usage:
    python scripts/benchmark_optimizations.py
    python scripts/benchmark_optimizations.py --full-model
    python scripts/benchmark_optimizations.py --iterations 100
"""

import argparse
import gc
import sys
import time
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Callable
import statistics

import torch
import torch.nn as nn

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent))


@dataclass
class BenchmarkResult:
    """Benchmark result container."""
    name: str
    baseline_ms: float
    optimized_ms: float
    speedup: float
    baseline_std: float
    optimized_std: float
    iterations: int
    
    def __str__(self):
        return (
            f"{self.name}: "
            f"Baseline={self.baseline_ms:.3f}ms (Â±{self.baseline_std:.3f}), "
            f"Optimized={self.optimized_ms:.3f}ms (Â±{self.optimized_std:.3f}), "
            f"Speedup={self.speedup:.2f}x"
        )


def benchmark_function(
    func: Callable,
    args: tuple,
    warmup_iterations: int = 10,
    benchmark_iterations: int = 50,
    device: torch.device = None
) -> Tuple[float, float]:
    """
    Run a rigorous benchmark of a function.
    
    Args:
        func: Function to benchmark
        args: Arguments to pass to function
        warmup_iterations: Number of warmup runs
        benchmark_iterations: Number of measured runs
        device: CUDA device for synchronization
    
    Returns:
        (mean_ms, std_ms): Mean and standard deviation in milliseconds
    """
    # Warmup
    for _ in range(warmup_iterations):
        _ = func(*args)
        if device and device.type == 'cuda':
            torch.cuda.synchronize()
    
    # Benchmark
    times = []
    for _ in range(benchmark_iterations):
        if device and device.type == 'cuda':
            torch.cuda.synchronize()
        
        start = time.perf_counter()
        _ = func(*args)
        
        if device and device.type == 'cuda':
            torch.cuda.synchronize()
        
        end = time.perf_counter()
        times.append((end - start) * 1000)  # Convert to ms
    
    mean_ms = statistics.mean(times)
    std_ms = statistics.stdev(times) if len(times) > 1 else 0.0
    
    return mean_ms, std_ms


def benchmark_mobius_operations(
    batch_size: int = 8,
    seq_len: int = 512,
    d_model: int = 256,
    iterations: int = 50,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark MÃ¶bius operations: fused vs baseline."""
    print("\nðŸ“Š MÃ¶bius Operations Benchmark")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Create test data
    x = torch.randn(batch_size, seq_len, d_model, device=device)
    a = torch.randn(batch_size, seq_len, d_model, device=device)
    c = 1.0
    
    # Baseline implementation
    def mobius_add_baseline(x, a, c):
        x2 = (x * x).sum(dim=-1, keepdim=True)
        a2 = (a * a).sum(dim=-1, keepdim=True)
        xa = (x * a).sum(dim=-1, keepdim=True)
        
        numer = (1 + 2 * c * xa + c * a2) * x + (1 - c * x2) * a
        denom = 1 + 2 * c * xa + c * c * x2 * a2
        denom = torch.clamp(denom, min=1e-7)
        
        return numer / denom
    
    # Try optimized version
    try:
        from src.kernels.hyperbolic_mobius_chain import mobius_add_fused
        
        # Benchmark baseline
        baseline_ms, baseline_std = benchmark_function(
            mobius_add_baseline, (x, a, c), 
            benchmark_iterations=iterations, device=device
        )
        
        # Benchmark optimized
        optimized_ms, optimized_std = benchmark_function(
            mobius_add_fused, (x, a, c),
            benchmark_iterations=iterations, device=device
        )
        
        speedup = baseline_ms / optimized_ms if optimized_ms > 0 else 1.0
        
        result = BenchmarkResult(
            name="MÃ¶bius Addition",
            baseline_ms=baseline_ms,
            optimized_ms=optimized_ms,
            speedup=speedup,
            baseline_std=baseline_std,
            optimized_std=optimized_std,
            iterations=iterations
        )
        print(result)
        return result
        
    except ImportError as e:
        print(f"âš  Fused MÃ¶bius not available: {e}")
        return None


def benchmark_green_function_cache(
    batch_size: int = 8,
    seq_len: int = 512,
    d_model: int = 256,
    iterations: int = 50,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark Green function caching."""
    print("\nðŸ“Š Green Function Cache Benchmark")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        from src.kernels.green_function_cache import GreenFunctionCache
        
        # Simulate G_ii computation (expensive operation)
        def compute_g_ii(x):
            # Simulate expensive computation
            result = torch.zeros(x.shape[0], x.shape[1], dtype=torch.complex64, device=x.device)
            for _ in range(5):  # Simulate multiple operations
                norm = x.norm(dim=-1)
                result = result + torch.complex(norm, norm * 0.1)
            return result
        
        cache = GreenFunctionCache(cache_size=256)
        
        # Test with repeated inputs (should hit cache)
        x = torch.randn(batch_size, seq_len, d_model, device=device)
        
        # Baseline: always compute
        def baseline_compute():
            return compute_g_ii(x)
        
        # Cached: use cache
        def cached_compute():
            return cache.get_or_compute(x, compute_g_ii)
        
        # Warmup cache
        for _ in range(10):
            cache.get_or_compute(x, compute_g_ii)
        
        # Benchmark baseline
        baseline_ms, baseline_std = benchmark_function(
            baseline_compute, (),
            benchmark_iterations=iterations, device=device
        )
        
        # Benchmark cached (should hit cache every time now)
        optimized_ms, optimized_std = benchmark_function(
            cached_compute, (),
            benchmark_iterations=iterations, device=device
        )
        
        speedup = baseline_ms / optimized_ms if optimized_ms > 0 else 1.0
        
        result = BenchmarkResult(
            name="Green Function Cache",
            baseline_ms=baseline_ms,
            optimized_ms=optimized_ms,
            speedup=speedup,
            baseline_std=baseline_std,
            optimized_std=optimized_std,
            iterations=iterations
        )
        print(result)
        print(f"   Cache stats: {cache.get_stats()}")
        return result
        
    except ImportError as e:
        print(f"âš  Green Function Cache not available: {e}")
        return None


def benchmark_ssm_scan(
    batch_size: int = 8,
    seq_len: int = 512,
    d_model: int = 256,
    d_state: int = 64,
    iterations: int = 50,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark SSM scan: parallel vs sequential."""
    print("\nðŸ“Š SSM Scan Benchmark (Parallel vs Sequential)")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        from src.kernels.low_rank_ssm_scan import LowRankSSMScan, parallel_prefix_scan
        from src.models.phase8.ar_ssm_fusion import AdaptiveRankSSM
        
        # Create models
        ssm_baseline = AdaptiveRankSSM(d_model, d_state, max_rank=16).to(device)
        ssm_optimized = LowRankSSMScan(d_model, d_state, rank=16).to(device)
        
        # Test data
        x = torch.randn(batch_size, seq_len, d_model, device=device)
        
        # Benchmark baseline (sequential)
        def baseline_forward():
            return ssm_baseline(x)
        
        # Benchmark optimized (parallel scan)
        def optimized_forward():
            return ssm_optimized(x)
        
        baseline_ms, baseline_std = benchmark_function(
            baseline_forward, (),
            benchmark_iterations=iterations, device=device
        )
        
        optimized_ms, optimized_std = benchmark_function(
            optimized_forward, (),
            benchmark_iterations=iterations, device=device
        )
        
        speedup = baseline_ms / optimized_ms if optimized_ms > 0 else 1.0
        
        result = BenchmarkResult(
            name="SSM Scan (Parallel)",
            baseline_ms=baseline_ms,
            optimized_ms=optimized_ms,
            speedup=speedup,
            baseline_std=baseline_std,
            optimized_std=optimized_std,
            iterations=iterations
        )
        print(result)
        return result
        
    except ImportError as e:
        print(f"âš  SSM Scan not available: {e}")
        return None


def benchmark_hyperbolic_distance(
    batch_size: int = 8,
    seq_len: int = 512,
    d_model: int = 256,
    iterations: int = 50,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark hyperbolic distance computation."""
    print("\nðŸ“Š Hyperbolic Distance Benchmark")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        from src.kernels.hyperbolic_distance_batch import (
            BatchedHyperbolicDistance,
            poincare_distance_from_origin
        )
        
        # Test data
        x = torch.randn(batch_size, seq_len, d_model, device=device) * 0.5  # Keep in ball
        
        # Baseline
        def baseline_distance():
            c = 1.0
            sqrt_c = 1.0
            x_norm = x.norm(dim=-1)
            scaled_norm = torch.clamp(sqrt_c * x_norm, max=1.0 - 1e-7)
            return (2.0 / sqrt_c) * torch.atanh(scaled_norm)
        
        # Optimized
        dist_module = BatchedHyperbolicDistance(curvature=1.0)
        
        def optimized_distance():
            return dist_module(x)
        
        baseline_ms, baseline_std = benchmark_function(
            baseline_distance, (),
            benchmark_iterations=iterations, device=device
        )
        
        optimized_ms, optimized_std = benchmark_function(
            optimized_distance, (),
            benchmark_iterations=iterations, device=device
        )
        
        speedup = baseline_ms / optimized_ms if optimized_ms > 0 else 1.0
        
        result = BenchmarkResult(
            name="Hyperbolic Distance",
            baseline_ms=baseline_ms,
            optimized_ms=optimized_ms,
            speedup=speedup,
            baseline_std=baseline_std,
            optimized_std=optimized_std,
            iterations=iterations
        )
        print(result)
        return result
        
    except ImportError as e:
        print(f"âš  Hyperbolic Distance not available: {e}")
        return None


def benchmark_scattering_gate(
    batch_size: int = 8,
    seq_len: int = 512,
    d_model: int = 256,
    num_heads: int = 8,
    iterations: int = 50,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark scattering gate: fused vs baseline."""
    print("\nðŸ“Š Scattering Gate Benchmark")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        from src.kernels.scattering_gate_fused import FusedScatteringGate
        from src.models.phase8.bk_core_hyperbolic import ScatteringGate
        
        # Create modules
        gate_baseline = ScatteringGate(d_model).to(device)
        gate_optimized = FusedScatteringGate(d_model).to(device)
        
        # Test data
        G_ii = torch.complex(
            torch.randn(batch_size, seq_len, device=device),
            torch.randn(batch_size, seq_len, device=device) * 0.1
        )
        attn = torch.randn(batch_size, num_heads, seq_len, seq_len, device=device)
        attn = torch.softmax(attn, dim=-1)
        
        def baseline_forward():
            return gate_baseline(G_ii, attn)
        
        def optimized_forward():
            return gate_optimized(G_ii, attn)
        
        baseline_ms, baseline_std = benchmark_function(
            baseline_forward, (),
            benchmark_iterations=iterations, device=device
        )
        
        optimized_ms, optimized_std = benchmark_function(
            optimized_forward, (),
            benchmark_iterations=iterations, device=device
        )
        
        speedup = baseline_ms / optimized_ms if optimized_ms > 0 else 1.0
        
        result = BenchmarkResult(
            name="Scattering Gate",
            baseline_ms=baseline_ms,
            optimized_ms=optimized_ms,
            speedup=speedup,
            baseline_std=baseline_std,
            optimized_std=optimized_std,
            iterations=iterations
        )
        print(result)
        return result
        
    except ImportError as e:
        print(f"âš  Scattering Gate not available: {e}")
        return None


def benchmark_full_model(
    batch_size: int = 1,
    seq_len: int = 512,
    d_model: int = 256,
    n_layers: int = 4,
    iterations: int = 20,
    device: torch.device = None
) -> Optional[BenchmarkResult]:
    """Benchmark full model forward pass with and without optimizations."""
    print("\nðŸ“Š Full Model Benchmark (Forward Pass)")
    print("-" * 50)
    
    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        from src.models.phase8.integrated_model import Phase8IntegratedModel, Phase8Config
        
        # Create model with optimizations disabled
        config_baseline = Phase8Config(
            vocab_size=1000,
            d_model=d_model,
            n_layers=n_layers,
            n_seq=seq_len,
            num_heads=4,
            use_bk_hyperbolic=True,
            use_ar_ssm_fusion=True,
            low_rank_ffn=True,
            low_rank_attention=True,
            use_bitnet=False,  # Disable for faster test
        )
        
        # Test data
        x = torch.randint(0, 1000, (batch_size, seq_len), device=device)
        
        # Create baseline model
        print("   Creating baseline model...")
        model_baseline = Phase8IntegratedModel(config_baseline).to(device)
        model_baseline.eval()
        
        def baseline_forward():
            with torch.no_grad():
                return model_baseline(x)
        
        # Benchmark baseline
        print("   Benchmarking baseline...")
        baseline_ms, baseline_std = benchmark_function(
            baseline_forward, (),
            warmup_iterations=5,
            benchmark_iterations=iterations, 
            device=device
        )
        
        # Clear memory
        del model_baseline
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Note: The "optimized" model uses the same code paths now
        # since we integrated the optimizations. To truly compare,
        # we would need to disable the optimizations explicitly.
        # For now, we report the current performance.
        
        print(f"   Baseline: {baseline_ms:.3f}ms (Â±{baseline_std:.3f})")
        print(f"   Note: Optimizations are now integrated into the model")
        
        result = BenchmarkResult(
            name="Full Model (Forward)",
            baseline_ms=baseline_ms,
            optimized_ms=baseline_ms,  # Same since integrated
            speedup=1.0,
            baseline_std=baseline_std,
            optimized_std=baseline_std,
            iterations=iterations
        )
        
        return result
        
    except Exception as e:
        print(f"âš  Full model benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return None


def print_summary(results: List[BenchmarkResult]):
    """Print benchmark summary."""
    print("\n" + "=" * 60)
    print("ðŸ“ˆ BENCHMARK SUMMARY")
    print("=" * 60)
    
    total_speedup = 1.0
    count = 0
    
    for result in results:
        if result is not None and result.speedup > 0:
            status = "âœ…" if result.speedup >= 1.0 else "âŒ"
            print(f"{status} {result.name}: {result.speedup:.2f}x speedup")
            total_speedup *= result.speedup
            count += 1
    
    if count > 0:
        geometric_mean = total_speedup ** (1/count)
        print("-" * 60)
        print(f"ðŸ“Š Geometric Mean Speedup: {geometric_mean:.2f}x")
    
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="Phase 8 Optimization Benchmark")
    parser.add_argument("--iterations", type=int, default=50, help="Benchmark iterations")
    parser.add_argument("--batch-size", type=int, default=8, help="Batch size")
    parser.add_argument("--seq-len", type=int, default=512, help="Sequence length")
    parser.add_argument("--d-model", type=int, default=256, help="Model dimension")
    parser.add_argument("--full-model", action="store_true", help="Run full model benchmark")
    parser.add_argument("--cpu", action="store_true", help="Force CPU benchmark")
    args = parser.parse_args()
    
    device = torch.device('cpu') if args.cpu else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("=" * 60)
    print("ðŸ§ª Phase 8 Optimization Benchmark")
    print("=" * 60)
    print(f"Device: {device}")
    print(f"Iterations: {args.iterations}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Sequence Length: {args.seq_len}")
    print(f"Model Dimension: {args.d_model}")
    
    if torch.cuda.is_available() and device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    results = []
    
    # Individual kernel benchmarks
    results.append(benchmark_mobius_operations(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model,
        iterations=args.iterations,
        device=device
    ))
    
    results.append(benchmark_green_function_cache(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model,
        iterations=args.iterations,
        device=device
    ))
    
    results.append(benchmark_ssm_scan(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model,
        iterations=args.iterations,
        device=device
    ))
    
    results.append(benchmark_hyperbolic_distance(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model,
        iterations=args.iterations,
        device=device
    ))
    
    results.append(benchmark_scattering_gate(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model,
        iterations=args.iterations,
        device=device
    ))
    
    # Full model benchmark if requested
    if args.full_model:
        results.append(benchmark_full_model(
            batch_size=1,
            seq_len=args.seq_len,
            d_model=args.d_model,
            n_layers=4,
            iterations=min(args.iterations, 20),
            device=device
        ))
    
    # Summary
    print_summary([r for r in results if r is not None])


if __name__ == "__main__":
    main()
