#!/usr/bin/env python3
"""
Phase 8 Training Script - 10B Parameter Scale Model (Optimized)
(ResNetBK + HTT + Hybrid Hyperbolic Attention + Low-Rank BitNet)

Features:
- 10B Parameter Scale Support via >99% Compression
- Low-Rank FFN & Attention with BitNet 1.58-bit Quantization
- BK-Core Hyperbolic Integration
- AR-SSM Hyperbolic Fusion
- Gradient Accumulation for Low-VRAM Training
- LR Scheduler (Linear Warmup + Cosine Decay)
- EMA (Exponential Moving Average)
- Label Smoothing
- Proper Checkpoint Saving

Usage:
    python scripts/train_phase8.py --config configs/phase8_10b_japanese.yaml
"""

import argparse
import copy
import json
import math
import os
import sys
import time
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from datetime import datetime
from collections import OrderedDict

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.phase8.integrated_model import Phase8IntegratedModel, Phase8Config
from src.optimizers.muon import Muon

# Import new Riemannian-Muon-Bit optimizer (Phase 1: Hyperbolic Training)
try:
    from src.optimizers.riemannian_muon_bit import RiemannianMuonBit, create_riemannian_muon_bit
    _RIEMANNIAN_MUON_AVAILABLE = True
except ImportError:
    _RIEMANNIAN_MUON_AVAILABLE = False
    RiemannianMuonBit = None

# Import BK-HyperSGD optimizer (Phase 1: ResNet-BK specialized optimizer)
try:
    from src.optimizers.bk_hyper_sgd import BKHyperSGD, create_bk_hyper_sgd, get_bk_parameter_groups
    _BK_HYPER_SGD_AVAILABLE = True
except ImportError:
    _BK_HYPER_SGD_AVAILABLE = False
    BKHyperSGD = None
    create_bk_hyper_sgd = None

# Import BK Isometry Initialization (Phase 1: Energy-preserving init)
try:
    from src.models.phase8.bk_isometry_init import BKIsometryInitializer, apply_bk_isometry_init
    _BK_ISOMETRY_AVAILABLE = True
except ImportError:
    _BK_ISOMETRY_AVAILABLE = False
    BKIsometryInitializer = None
    apply_bk_isometry_init = None

# Lazy import for data_utils (requires datasets library)
def get_mixed_data_loader(*args, **kwargs):
    from src.utils.data_utils import get_mixed_data_loader as _loader
    return _loader(*args, **kwargs)

# Import Phase 8 optimization kernels
try:
    from src.kernels.resonance_adaptive_curvature import ResonanceAdaptiveCurvature, StabilityMonitor
    _RESONANCE_AVAILABLE = True
except ImportError:
    _RESONANCE_AVAILABLE = False
    ResonanceAdaptiveCurvature = None
    StabilityMonitor = None

# Import Gradient Teleportation (#9)
try:
    from src.kernels.gradient_teleportation import GradientTeleporter, create_gradient_teleporter
    _GRADIENT_TELEPORT_AVAILABLE = True
except ImportError:
    _GRADIENT_TELEPORT_AVAILABLE = False
    GradientTeleporter = None
    create_gradient_teleporter = None

# Import Revolutionary Training (7 algorithms)
try:
    from src.training.revolutionary_trainer import (
        RevolutionaryTrainer,
        RevolutionaryConfig,
        create_revolutionary_trainer,
    )
    _REVOLUTIONARY_AVAILABLE = True
except ImportError:
    _REVOLUTIONARY_AVAILABLE = False
    RevolutionaryTrainer = None
    RevolutionaryConfig = None
    create_revolutionary_trainer = None

# Import Gradient Sanitization (Moonshot #13)
try:
    from src.training.gradient_sanitization import (
        GradientSanitizer,
        GradientSanitizationConfig,
        create_gradient_sanitizer,
    )
    _GRADIENT_SANITIZER_AVAILABLE = True
except ImportError:
    _GRADIENT_SANITIZER_AVAILABLE = False
    GradientSanitizer = None
    create_gradient_sanitizer = None

# Import Gradient Feeder V2 (Adaptive Clipping Threshold)
# Now uses pre-built C++ extension for instant load (no JIT delay)
try:
    from src.training.gradient_feeder import GradientFeederV2
    _GRADIENT_FEEDER_AVAILABLE = True
except ImportError:
    _GRADIENT_FEEDER_AVAILABLE = False
    GradientFeederV2 = None

# Import Stability Suite (Moonshot #14 - Comprehensive NaN Elimination)
try:
    from src.training.stability_suite import (
        StabilityManager,
        StabilityConfig,
        create_stability_manager,
        BackwardHookNaNEliminator,
        LayerwiseGradientScaler,
    )
    _STABILITY_SUITE_AVAILABLE = True
except ImportError:
    _STABILITY_SUITE_AVAILABLE = False
    StabilityManager = None
    create_stability_manager = None

# Import Async Checkpoint Saver (Prevents training slowdown after saves)
try:
    from src.training.async_checkpoint import (
        AsyncCheckpointSaver,
        aggressive_memory_cleanup,
        force_cuda_memory_defrag,
    )
    _ASYNC_CHECKPOINT_AVAILABLE = True
except ImportError:
    _ASYNC_CHECKPOINT_AVAILABLE = False
    AsyncCheckpointSaver = None
    aggressive_memory_cleanup = None
    force_cuda_memory_defrag = None

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)


# =============================================================================
# EMA (Exponential Moving Average)
# =============================================================================
class EMA:
    """
    Exponential Moving Average of model weights.
    Updates shadow weights as: shadow = decay * shadow + (1 - decay) * model_weights
    """
    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self._init_shadow()
    
    def _init_shadow(self):
        """Initialize shadow weights from model."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        """Update shadow weights with current model weights."""
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.shadow[name] = (
                    self.decay * self.shadow[name] + 
                    (1 - self.decay) * param.data
                )
    
    def apply_shadow(self):
        """Apply shadow weights to model (for evaluation)."""
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        """Restore original weights after evaluation."""
        for name, param in self.model.named_parameters():
            if name in self.backup:
                param.data = self.backup[name]
        self.backup = {}
    
    def state_dict(self) -> Dict:
        """Return EMA state for checkpointing."""
        return {
            'decay': self.decay,
            'shadow': {k: v.cpu() for k, v in self.shadow.items()}
        }
    
    def load_state_dict(self, state_dict: Dict):
        """Load EMA state from checkpoint."""
        self.decay = state_dict['decay']
        device = next(self.model.parameters()).device
        self.shadow = {k: v.to(device) for k, v in state_dict['shadow'].items()}


# =============================================================================
# Learning Rate Scheduler (Linear Warmup + Cosine Decay)
# =============================================================================
class CosineWarmupScheduler:
    """
    Learning rate scheduler with linear warmup and cosine decay.
    
    LR schedule:
    - Warmup (0 to warmup_steps): Linear from 0 to peak_lr
    - Decay (warmup_steps to total_steps): Cosine from peak_lr to min_lr
    """
    def __init__(
        self,
        optimizer: optim.Optimizer,
        warmup_steps: int,
        total_steps: int,
        peak_lr: float,
        min_lr: float = 1e-6
    ):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.peak_lr = peak_lr
        self.min_lr = min_lr
        self.current_step = 0
        
        # FIX: Initialize with correct LR (not 0.0)
        # When warmup_steps=0, start at peak_lr immediately
        initial_lr = self.get_lr(0)
        self._last_lr = initial_lr
        
        # Apply initial LR to optimizer param_groups
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = initial_lr
    
    def get_lr(self, step: int) -> float:
        """Calculate learning rate for given step."""
        if step < self.warmup_steps:
            # Linear warmup
            return self.peak_lr * step / max(1, self.warmup_steps)
        else:
            # Cosine decay
            progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)
            progress = min(1.0, progress)  # Clamp to [0, 1]
            return self.min_lr + 0.5 * (self.peak_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
    
    def step(self):
        """Update learning rate for current step."""
        self.current_step += 1
        lr = self.get_lr(self.current_step)
        self._last_lr = lr
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_last_lr(self) -> float:
        """Return last computed learning rate."""
        return self._last_lr
    
    def state_dict(self) -> Dict:
        """Return scheduler state for checkpointing."""
        return {
            'current_step': self.current_step,
            'warmup_steps': self.warmup_steps,
            'total_steps': self.total_steps,
            'peak_lr': self.peak_lr,
            'min_lr': self.min_lr
        }
    
    def load_state_dict(self, state_dict: Dict):
        """Load scheduler state from checkpoint."""
        self.current_step = state_dict['current_step']
        self.warmup_steps = state_dict['warmup_steps']
        self.total_steps = state_dict['total_steps']
        self.peak_lr = state_dict['peak_lr']
        self.min_lr = state_dict['min_lr']


# =============================================================================
# Training Configuration
# =============================================================================
@dataclass
class Phase8TrainingConfig:
    """Phase 8 Training Configuration"""
    # Model Architecture (10B Scale Target: d_model=4096, n_layers=48)
    d_model: int = 4096
    n_layers: int = 48
    n_seq: int = 512
    num_heads: int = 32
    htt_rank: int = 16
    hyperbolic_window_size: int = 64
    
    # Phase 8 Specifics
    use_bk_hyperbolic: bool = True
    use_ar_ssm_fusion: bool = True
    
    # Compression (Critical for 10B)
    low_rank_ffn: bool = True
    low_rank_attention: bool = True
    low_rank_rank: int = 64
    use_bitnet: bool = True
    
    # Training
    batch_size: int = 1
    grad_accum_steps: int = 16
    epochs: int = 1
    learning_rate: float = 0.02
    min_lr: float = 1e-6
    weight_decay: float = 0.01
    warmup_steps: int = 500  # Reduced from 2000
    max_steps: Optional[int] = None
    bootstrap_lr: float = 0.0           # Optional LR for first few optimizer steps (stability bootstrap)
    bootstrap_steps: int = 0            # How many optimizer steps to apply bootstrap_lr

    # Optimizer (AdamW settings)
    optimizer_type: str = 'adamw'
    beta1: float = 0.9
    beta2: float = 0.95
    eps: float = 1e-8
    
    # Bootstrap LR (Shock Therapy for saddle point escape)
    # Use very high LR until loss decreases significantly
    bootstrap_lr: float = 2.0  # High LR for bootstrap phase
    bootstrap_steps: int = 500  # Maximum steps (safety limit)
    bootstrap_exit_threshold: float = 0.5  # Exit when loss drops by this amount
    bootstrap_skip_on_resume: bool = True  # Default: skip bootstrap on resume (stability)
    
    # Gradient - stricter clipping during warmup to prevent NaN
    grad_clip_warmup: float = 0.01  # Very strict during warmup (was 0.1)
    grad_clip_train: float = 1.0
    grad_skip_threshold: float = 10.0
    warmup_stability_steps: int = 100  # Extra-strict clipping for first N steps
    
    # Async checkpoint for speed
    async_checkpoint: bool = True  # Use async checkpoint to avoid training slowdown
    
    # EMA
    use_ema: bool = True
    ema_decay: float = 0.999
    
    # Regularization
    label_smoothing: float = 0.1
    dropout: float = 0.1
    
    # Optimization
    use_mixed_precision: bool = True
    use_gradient_checkpointing: bool = True
    use_triton_kernel: bool = True
    triton_kernel_version: str = 'fast'
    
    # Riemannian Resonant Tunneling (HTT Optimization)
    use_resonant_htt: bool = False
    resonant_num_cores: int = 4
    use_zeta_init: bool = True
    
    # Data
    data_limit: int = 100_000_000
    vocab_size: int = 50257
    
    # Logging
    log_interval: int = 10
    save_interval: int = 500  # Checkpoint every 500 steps (with async save for speed)
    eval_interval: int = 200
    save_dir: str = "checkpoints/phase8"
    
    # Device
    device: str = "auto"
    seed: int = 42
    
    # Runtime
    dry_run: bool = False
    dataset_path: str = "configs/dataset_mixing.yaml"
    resume_from: Optional[str] = None
    compile: bool = False
    
    # Moonshot Optimizations
    use_resonance_locked: bool = False  # #6: Skip updates when gradient SNR is low (DISABLED for speed)
    resonance_gns_threshold: float = 5.0  # Gradient Noise Scale threshold
    use_time_reversed: bool = False  # #10: Train on reversed sequences (DISABLED by default - 2x overhead)
    time_reversed_weight: float = 0.5  # Weight for reversed loss
    
    # #3 Eigenvalue Precomputation
    use_green_function_lut: bool = True
    green_function_lut_size: int = 1024
    
    # #7 Scattering-Aware Attention Pruning
    use_scattering_pruning: bool = True
    scattering_threshold: float = 0.1
    
    # #8 Hyperbolic MoE
    use_hyperbolic_moe: bool = False  # Model arch change, optional
    hmoe_num_experts: int = 8
    hmoe_top_k: int = 2
    
    # #9 Gradient Teleportation
    use_gradient_teleportation: bool = True
    teleport_strength: float = 0.1
    
    # #11 Holographic Compression
    use_holographic_kv_cache: bool = False  # Experimental, optional
    holographic_compression_ratio: float = 0.25
    
    # #12 Superposition Training 
    use_superposition_training: bool = False  # Heavy, optional
    superposition_particles: int = 5
    
    # Revolutionary Training Algorithms (7 algorithms)
    # Phase-based auto-scheduling: algorithms activate based on training progress
    # - Warmup (0-10%): OFF (focus on stability)
    # - Early (10-30%): holographic, closed_form only
    # - Mid (30-70%): + topological, zeta
    # - Late (70-100%): ALL algorithms enabled
    use_revolutionary_training: bool = True  # Master switch
    revolutionary_auto_schedule: bool = True  # Auto ON/OFF based on phase
    revolutionary_algorithms: str = "holographic,closed_form,topological,retrocausal,zeta,sheaf,diffractive"
    # Fast-start controls for resume flows (activate revolutionary algos immediately after resume)
    revolutionary_fast_start: bool = True  # Enable fast-start for initial stabilization
    revolutionary_fast_start_window: int = 200  # First N optimizer steps always-on
    revolutionary_min_stable_steps: int = 50  # Default stability requirement when not in fast-start window


def parse_args() -> Phase8TrainingConfig:
    parser = argparse.ArgumentParser(description="Phase 8 Training Script (Optimized)")
    
    # Model
    parser.add_argument("--d-model", type=int, default=4096)
    parser.add_argument("--n-layers", type=int, default=48)
    parser.add_argument("--n-seq", type=int, default=512)
    parser.add_argument("--num-heads", type=int, default=32)
    parser.add_argument("--low-rank-rank", type=int, default=64)
    
    # Flags
    parser.add_argument("--no-low-rank-ffn", action="store_false", dest="low_rank_ffn")
    parser.add_argument("--no-low-rank-attn", action="store_false", dest="low_rank_attention")
    parser.add_argument("--no-bitnet", action="store_false", dest="use_bitnet")
    parser.add_argument("--no-bk-hyperbolic", action="store_false", dest="use_bk_hyperbolic")
    parser.add_argument("--no-ar-ssm", action="store_false", dest="use_ar_ssm_fusion")
    parser.add_argument("--no-ema", action="store_false", dest="use_ema")
    
    # Training
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--grad-accum-steps", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=0.02)
    parser.add_argument("--min-lr", type=float, default=1e-6)
    parser.add_argument("--warmup-steps", type=int, default=500)
    parser.add_argument("--max-steps", type=int, default=None)
    parser.add_argument("--bootstrap-lr", type=float, default=None)
    parser.add_argument("--bootstrap-steps", type=int, default=None)
    parser.add_argument("--bootstrap-skip-on-resume", action="store_true", dest="bootstrap_skip_on_resume", default=None,
                        help="Skip bootstrap LR when resuming from checkpoint (default: skip)")
    parser.add_argument("--dry-run", action="store_true")
    
    # Optimizer
    parser.add_argument("--optimizer", type=str, default="adamw", choices=["adamw", "muon", "riemannian_muon", "bk_hyper_sgd"])
    parser.add_argument("--beta1", type=float, default=0.9)
    parser.add_argument("--beta2", type=float, default=0.95)
    parser.add_argument("--weight-decay", type=float, default=0.01)
    
    # Regularization
    parser.add_argument("--label-smoothing", type=float, default=0.1)
    parser.add_argument("--ema-decay", type=float, default=0.999)
    
    # Revolutionary training controls
    parser.add_argument("--use-revolutionary-training", action="store_true", dest="use_revolutionary_training", default=None)
    parser.add_argument("--no-revolutionary-training", action="store_false", dest="use_revolutionary_training")
    parser.add_argument("--revolutionary-auto-schedule", action="store_true", dest="revolutionary_auto_schedule", default=None)
    parser.add_argument("--no-revolutionary-auto-schedule", action="store_false", dest="revolutionary_auto_schedule")
    parser.add_argument("--revolutionary-algorithms", type=str, default=None)
    parser.add_argument("--revolutionary-fast-start", action="store_true", dest="revolutionary_fast_start", default=None,
                        help="Force revolutionary algorithms to start immediately (overrides stability delay)")
    parser.add_argument("--revolutionary-fast-start-window", type=int, default=None,
                        help="Number of optimizer steps to force-enable revolutionary algorithms (0 disables)")
    parser.add_argument("--revolutionary-min-stable-steps", type=int, default=None,
                        help="Stable steps required before enabling revolutionary algorithms when not in fast-start window")
    
    # Gradient
    parser.add_argument("--grad-clip-warmup", type=float, default=0.1)
    parser.add_argument("--grad-clip-train", type=float, default=1.0)
    parser.add_argument("--grad-skip-threshold", type=float, default=10.0)
    
    # Optimization
    parser.add_argument("--extreme-compression", action="store_true")
    parser.add_argument("--ultra-compression", action="store_true")
    parser.add_argument("--compile", action="store_true")
    parser.add_argument("--resonant-htt", action="store_true",
        help="Enable Riemannian Resonant Tunneling for optimal tensor geometry")
    
    # Config file support
    parser.add_argument("--config", type=str, default=None)
    parser.add_argument("--dataset", type=str, default=None)
    parser.add_argument("--resume-from", type=str, default=None)
    parser.add_argument("--save-dir", type=str, default=None)
    parser.add_argument("--save-interval", type=int, default=500)

    parser.set_defaults(
        low_rank_ffn=True, 
        low_rank_attention=True, 
        use_bitnet=True, 
        use_bk_hyperbolic=True, 
        use_ar_ssm_fusion=True,
        use_ema=True
    )
    
    args = parser.parse_args()

    # Load YAML config if provided
    yaml_config = {}
    if args.config:
        import yaml
        with open(args.config, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
    
    # Helper to get config value (CLI > YAML > default)
    def get_val(name, default):
        cli_val = getattr(args, name, None)
        yaml_val = yaml_config.get(name, yaml_config.get(name.replace('_', '-'), None))
        if cli_val is not None and cli_val != default:
            return cli_val
        if yaml_val is not None:
            return yaml_val
        return default if cli_val is None else cli_val
    
    # Extreme Compression Logic
    if args.extreme_compression:
        print("ðŸš€ Extreme Compression Enabled (Target: 8GB VRAM)")
        args.low_rank_rank = 16
    
    if args.ultra_compression:
        print("ðŸŒŒ Ultra Compression Enabled (Target: <3GB VRAM)")
        args.low_rank_rank = 8
    
    # Build config
    config = Phase8TrainingConfig(
        d_model=get_val('d_model', 4096),
        n_layers=get_val('n_layers', 48),
        n_seq=get_val('n_seq', 512),
        num_heads=get_val('num_heads', 32),
        low_rank_rank=get_val('low_rank_rank', 64),
        low_rank_ffn=get_val('low_rank_ffn', True),
        low_rank_attention=get_val('low_rank_attention', True),
        use_bitnet=get_val('use_bitnet', True),
        use_bk_hyperbolic=get_val('use_bk_hyperbolic', True),
        use_ar_ssm_fusion=get_val('use_ar_ssm_fusion', True),
        batch_size=get_val('batch_size', 1),
        grad_accum_steps=get_val('grad_accum_steps', get_val('gradient_accumulation_steps', 16)),
        epochs=get_val('epochs', 1),
        learning_rate=get_val('learning_rate', 0.02),
        min_lr=get_val('min_lr', 1e-6),
        warmup_steps=get_val('warmup_steps', 500),
        max_steps=get_val('max_steps', None),
        bootstrap_lr=get_val('bootstrap_lr', args.bootstrap_lr if args.bootstrap_lr is not None else 0.0),
        bootstrap_steps=get_val('bootstrap_steps', args.bootstrap_steps if args.bootstrap_steps is not None else 0),
        bootstrap_skip_on_resume=get_val('bootstrap_skip_on_resume', True),
        optimizer_type=args.optimizer,  # CLI always takes priority for optimizer
        beta1=get_val('beta1', 0.9),
        beta2=get_val('beta2', 0.95),
        eps=get_val('eps', 1e-8),
        weight_decay=get_val('weight_decay', 0.01),
        grad_clip_warmup=get_val('grad_clip_warmup', get_val('max_grad_norm', 0.1)),
        grad_clip_train=get_val('grad_clip_train', 1.0),
        grad_skip_threshold=get_val('grad_skip_threshold', 10.0),
        use_ema=get_val('use_ema', True),
        ema_decay=get_val('ema_decay', 0.999),
        label_smoothing=get_val('label_smoothing', 0.1),
        dropout=get_val('dropout', 0.1),
        use_mixed_precision=get_val('use_mixed_precision', True),
        use_gradient_checkpointing=get_val('use_gradient_checkpointing', True),
        use_triton_kernel=get_val('use_triton_kernel', True),
        # Riemannian Resonant Tunneling
        use_resonant_htt=get_val('use_resonant_htt', False),
        resonant_num_cores=get_val('resonant_num_cores', 4),
        use_zeta_init=get_val('use_zeta_init', True),
        vocab_size=get_val('vocab_size', 50257),
        save_interval=get_val('save_interval', 500),
        save_dir=get_val('save_dir', 'checkpoints/phase8'),
        dry_run=args.dry_run,
        dataset_path=args.dataset if args.dataset else get_val('dataset_path', 'configs/dataset_mixing.yaml'),
        resume_from=args.resume_from,
        compile=args.compile,
        use_revolutionary_training=get_val('use_revolutionary_training', True),
        revolutionary_auto_schedule=get_val('revolutionary_auto_schedule', True),
        revolutionary_algorithms=get_val('revolutionary_algorithms', "holographic,closed_form,topological,retrocausal,zeta,sheaf,diffractive"),
        revolutionary_fast_start=get_val('revolutionary_fast_start', True),
        revolutionary_fast_start_window=get_val('revolutionary_fast_start_window', 200),
        revolutionary_min_stable_steps=get_val('revolutionary_min_stable_steps', 50),
    )
    
    return config


def init_weights(model: nn.Module):
    """
    Initialize model weights with proper schemes.
    - Attention: Xavier
    - FFN: He (Kaiming)
    - LayerNorm: gamma=1, beta=0
    - Embedding: N(0, 0.02)
    """
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        
        if 'embedding' in name.lower() or 'embed' in name.lower():
            # Skip adapter parameters (they have their own init)
            if 'adapter' in name:
                continue

            # Muon + Large Model (327M): éžå¸¸ã«ä¿å®ˆçš„ãªåˆæœŸåŒ–
            nn.init.normal_(param, mean=0.0, std=0.001)  # 0.02 â†’ 0.001 (BERT/GPTæŽ¨å¥¨å€¤)
            # çµ¶å¯¾å€¤ã‚’Â±0.01ã«åˆ¶é™ï¼ˆå‹¾é…çˆ†ç™ºé˜²æ­¢ï¼‰
            with torch.no_grad():
                param.data.clamp_(-0.01, 0.01)
        elif 'layernorm' in name.lower() or 'layer_norm' in name.lower():
            if 'weight' in name or 'gamma' in name:
                nn.init.ones_(param)
            elif 'bias' in name or 'beta' in name:
                nn.init.zeros_(param)
        elif 'attn' in name.lower() or 'attention' in name.lower():
            if param.dim() >= 2:
                nn.init.xavier_uniform_(param)
        elif 'ffn' in name.lower() or 'mlp' in name.lower() or 'fc' in name.lower():
            if param.dim() >= 2:
                nn.init.kaiming_uniform_(param, a=math.sqrt(5))
        elif param.dim() >= 2:
            nn.init.xavier_uniform_(param)


def create_model(config: Phase8TrainingConfig, vocab_size: int, device: torch.device) -> Phase8IntegratedModel:
    model_config = Phase8Config(
        vocab_size=vocab_size,
        d_model=config.d_model,
        n_layers=config.n_layers,
        n_seq=config.n_seq,
        num_heads=config.num_heads,
        htt_rank=config.htt_rank,
        hyperbolic_window_size=config.hyperbolic_window_size,
        
        # Phase 8
        use_bk_hyperbolic=config.use_bk_hyperbolic,
        use_ar_ssm_fusion=config.use_ar_ssm_fusion,
        
        # Compression
        low_rank_ffn=config.low_rank_ffn,
        low_rank_attention=config.low_rank_attention,
        low_rank_rank=config.low_rank_rank,
        use_bitnet=config.use_bitnet,
        
        # Optimization
        use_gradient_checkpointing=config.use_gradient_checkpointing,
        use_mixed_precision=config.use_mixed_precision,
        use_triton_kernel=config.use_triton_kernel,
        triton_kernel_version=config.triton_kernel_version,
        
        # Riemannian Resonant Tunneling
        use_resonant_htt=config.use_resonant_htt,
        resonant_num_cores=config.resonant_num_cores,
        use_zeta_init=config.use_zeta_init,
    )
    
    model = Phase8IntegratedModel(model_config)
    
    # Apply weight initialization
    # Use BK Isometry initialization if available (Phase 1: energy-preserving)
    if _BK_ISOMETRY_AVAILABLE:
        print("ðŸ§¬ Applying BK Isometry Initialization (energy-preserving)...")
        stats = apply_bk_isometry_init(model, base_gain=1.0, curvature=-1.0, verbose=False)
        print(f"   Unitary: {stats.get('unitary_count', 0)}, Hyperbolic: {stats.get('hyperbolic_count', 0)}, Euclidean: {stats.get('euclidean_count', 0)}")
    else:
        init_weights(model)
    
    # ========== Global Gradient Sanitization ==========
    # Register gradient hooks on ALL parameters to sanitize NaN/Inf
    # CRITICAL: Use small non-zero values (1e-6) instead of 0 to allow gradient flow
    def create_sanitize_hook(param_name):
        def sanitize_grad(grad):
            if grad is not None:
                # Replace NaN/Inf with small values (NOT zero - that kills learning)
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    grad = torch.nan_to_num(grad, nan=1e-6, posinf=1.0, neginf=-1.0)
                # Clamp to Â±50.0 (relaxed from Â±10.0 - gradient was too weak at 0.15)
                return torch.clamp(grad, -50.0, 50.0)
            return grad
        return sanitize_grad
    
    num_hooks = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            param.register_hook(create_sanitize_hook(name))
            num_hooks += 1
    print(f"âœ” Gradient sanitization hooks applied to {num_hooks} parameters (NaNâ†’1e-6, clamp Â±5.0)")
    
    model = model.to(device)
    
    return model


def cleanup_old_checkpoints(save_dir: str, max_keep: int = 2):
    """
    Keep only the latest N checkpoints, delete older ones.
    
    Args:
        save_dir: Directory containing checkpoints
        max_keep: Maximum number of checkpoints to keep (default: 2)
    """
    import glob
    
    # Find all step_*.pt files
    pattern = os.path.join(save_dir, "step_*.pt")
    checkpoints = glob.glob(pattern)
    
    if len(checkpoints) <= max_keep:
        return  # Nothing to delete
    
    # Sort by step number (extract from filename)
    def get_step(path):
        try:
            basename = os.path.basename(path)
            return int(basename.replace("step_", "").replace(".pt", ""))
        except:
            return 0
    
    checkpoints.sort(key=get_step)
    
    # Delete oldest checkpoints
    to_delete = checkpoints[:-max_keep]
    for ckpt in to_delete:
        try:
            os.remove(ckpt)
            print(f"ðŸ—‘ï¸ Deleted old checkpoint: {os.path.basename(ckpt)}")
        except Exception as e:
            print(f"âš  Failed to delete {ckpt}: {e}")


def save_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler: CosineWarmupScheduler,
    scaler: torch.cuda.amp.GradScaler,
    ema: Optional[EMA],
    step: int,
    epoch: int,
    loss: float,
    config: Phase8TrainingConfig,
    revolutionary_trainer: Optional['RevolutionaryTrainer'] = None,
):
    """Save complete checkpoint including all training state."""
    import gc
    
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    # CRITICAL: Access underlying model for torch.compile'd models
    # This avoids resetting the compiled graph which causes 3x slowdown!
    model_to_save = model
    if hasattr(model, '_orig_mod'):
        model_to_save = model._orig_mod  # torch.compile'd model
    
    # Build checkpoint dict
    checkpoint = {
        'step': step,
        'epoch': epoch,
        'loss': loss,
        'model_state_dict': model_to_save.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'config': asdict(config),
    }
    
    if ema is not None:
        checkpoint['ema_state_dict'] = ema.state_dict()
    
    # CRITICAL: Save revolutionary trainer state for proper resume
    if revolutionary_trainer is not None:
        checkpoint['revolutionary_trainer_state_dict'] = revolutionary_trainer.state_dict()
    
    # Save and immediately free memory
    torch.save(checkpoint, path)
    print(f"\nðŸ’¾ Checkpoint saved: {path}")
    
    # Aggressively free checkpoint memory
    for key in list(checkpoint.keys()):
        del checkpoint[key]
    del checkpoint
    
    # Force garbage collection
    gc.collect()


def load_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler: CosineWarmupScheduler,
    scaler: torch.cuda.amp.GradScaler,
    ema: Optional[EMA],
    device: torch.device,
    revolutionary_trainer: Optional['RevolutionaryTrainer'] = None,
) -> Tuple[int, int, float]:
    """Load checkpoint and return (step, epoch, loss)."""
    print(f"Loading checkpoint from {path}...")
    
    checkpoint = torch.load(path, map_location=device)
    
    # Load model
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    # Load optimizer
    if 'optimizer_state_dict' in checkpoint:
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        except Exception as e:
            print(f"âš  Could not load optimizer state: {e}")
    
    # Load scheduler
    if 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # Load Scaler (PyTorch 2.2 uses cuda.amp)
    if 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    # Load EMA
    if ema is not None and 'ema_state_dict' in checkpoint:
        ema.load_state_dict(checkpoint['ema_state_dict'])
    
    # CRITICAL: Load revolutionary trainer state for proper resume
    # This ensures phase-based scheduling continues correctly
    if revolutionary_trainer is not None and 'revolutionary_trainer_state_dict' in checkpoint:
        revolutionary_trainer.load_state_dict(checkpoint['revolutionary_trainer_state_dict'])
    
    step = checkpoint.get('step', 0)
    epoch = checkpoint.get('epoch', 0)
    loss = checkpoint.get('loss', 0.0)
    
    print(f"âœ” Checkpoint loaded: step={step}, epoch={epoch}, loss={loss:.4f}")
    
    return step, epoch, loss


def save_training_log(log: Dict, path: str):
    """Save training log to JSON file."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(log, f, indent=2, ensure_ascii=False, default=str)


def train_phase8():
    config = parse_args()
    
    # Set precision and CUDA options
    torch.set_float32_matmul_precision('high')
    torch.backends.cudnn.benchmark = True
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Phase 8 Training (10B Scale - Optimized) on {device}")

    # Initialize Triton Mode
    if device.type == "cuda" and config.use_triton_kernel:
        try:
            from src.models.bk_core import set_triton_mode
            set_triton_mode(True)
            print("âœ” Triton Mode Enabled")
        except ImportError:
            print("âš  Triton mode not available")

    # Log VRAM
    if torch.cuda.is_available():
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"Detected VRAM: {vram:.2f} GB")

    # Print config summary
    print(f"Config: d_model={config.d_model}, n_layers={config.n_layers}")
    print(f"Compression: LowRankFFN={config.low_rank_ffn}, LowRankAttn={config.low_rank_attention}, BitNet={config.use_bitnet}")
    print(f"Rank: {config.low_rank_rank}, Grad Accum: {config.grad_accum_steps}")
    print(f"Optimizer: {config.optimizer_type.upper()}, LR: {config.learning_rate}, Warmup: {config.warmup_steps}")
    print(f"EMA: {config.use_ema}, Label Smoothing: {config.label_smoothing}")
    
    # Dataset - Load FIRST to get actual vocab_size
    steps_per_epoch = 100  # Default for dry run
    actual_vocab_size = config.vocab_size
    dataset = None
    
    # CRITICAL FIX: Override warmup_steps for dry-run BEFORE optimizer creation
    # Without this, warmup_steps=2000 means lrâ‰ˆ0 for the entire dry-run
    if config.dry_run:
        dry_run_steps = 200
        dry_run_optimizer_steps = dry_run_steps // config.grad_accum_steps
        original_warmup = config.warmup_steps
        original_lr = config.learning_rate
        
        # CRITICAL: Skip warmup entirely and use config LR for dry-run
        config.warmup_steps = 0  # No warmup - start at peak LR immediately
        # NOTE: learning_rate is now respected from config (no override)
        
        print(f"âš¡ Dry-run LR: {config.learning_rate}")
        print("Dry Run: Using dummy data")
        
        # Downsize the model for fast feedback during dry-run
        # Step 2 scale-up test: d_model=2048, n_layers=24
        if config.d_model > 4096:
            print(f"âš¡ Dry-run d_model {config.d_model} â†’ 4096")
            config.d_model = 4096
        if config.n_layers > 48:
            print(f"âš¡ Dry-run n_layers {config.n_layers} â†’ 48")
            config.n_layers = 48
        if config.num_heads > 32:
            print(f"âš¡ Dry-run num_heads {config.num_heads} â†’ 32")
            config.num_heads = 32
        if config.low_rank_rank > 64:
            print(f"âš¡ Dry-run low_rank_rank {config.low_rank_rank} â†’ 64")
            config.low_rank_rank = 64
    else:
        print(f"Loading dataset from {config.dataset_path}...")
        try:
            dataset, vocab, steps_per_epoch = get_mixed_data_loader(
                config.dataset_path,
                batch_size=config.batch_size,
                n_seq=config.n_seq,
                total_tokens=config.data_limit,
                seed=config.seed,
                vocab_size=config.vocab_size
            )
            # Update vocab_size if dataset has larger tokens
            # vocab is a dict with 'stoi', 'itos', 'vocab_size' keys
            actual_vocab_size = vocab['vocab_size'] if isinstance(vocab, dict) else vocab
            if actual_vocab_size != config.vocab_size:
                print(f"[Model] Using dataset vocab_size: {actual_vocab_size} (config was {config.vocab_size})")
                config.vocab_size = actual_vocab_size
            print(f"Dataset loaded. Steps per epoch: {steps_per_epoch}")
        except Exception as e:
            print(f"âš  Failed to load dataset: {e}")
            sys.exit(1)
    
    # Create model with actual vocab_size
    model = create_model(config, config.vocab_size, device)
    
    # Apply torch.compile() for additional speedup
    if config.compile and hasattr(torch, 'compile'):
        try:
            print("ðŸ”§ Applying torch.compile()...")
            model = torch.compile(model, mode="reduce-overhead", fullgraph=False)
            print("âœ” torch.compile() applied successfully")
        except Exception as e:
            print(f"âš  torch.compile() failed (continuing without): {e}")
    
    # Parameter Count
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total Parameters: {total_params:,}")
    print(f"Trainable Parameters: {trainable_params:,}")
    
    # Optimizer - BK-HyperSGD ONLY (ResNet-BKå°‚ç”¨)
    # AdamW, Muon, RiemannianMuon ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ
    if not _BK_HYPER_SGD_AVAILABLE:
        raise RuntimeError("BK-HyperSGD is required but not available! Check src/optimizers/bk_hyper_sgd.py")
    
    print("ðŸ§¬ Using BK-HyperSGD Optimizer (ResNet-BK Specialized - å”¯ä¸€ã®ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶)")
    print("   - Cayley retraction for unitary layers (v_proj, output_proj)")
    print("   - Lorentz exp map for hyperbolic layers")
    print("   - Symplectic integration for Hamiltonian structure")
    if not config.dry_run and config.bootstrap_lr > 0 and config.bootstrap_steps > 0:
        print(f"   âš¡ Bootstrap LR enabled: {config.bootstrap_lr} for first {config.bootstrap_steps} optimizer steps")
    
    # Get parameter groups with geometry-aware learning rates
    param_groups = get_bk_parameter_groups(
        model,
        base_lr=config.learning_rate,
        unitary_lr_scale=0.1,      # BK-Core needs smaller LR
        hyperbolic_lr_scale=0.5,   # Hyperbolic moderate LR
        symplectic_lr_scale=0.3,   # Symplectic balanced LR
    )
    
    # CRITICAL: max_grad_norm was crushing gradients (set to 1.0)
    # Set to 5.0 for stable training
    optimizer_max_grad = 50.0  # Relaxed: 10.0 â†’ 50.0 (gradient was 0.15, too weak)
    if config.dry_run:
        optimizer_max_grad = 1000.0  # Relaxed for dry-run
    
    optimizer = BKHyperSGD(
        param_groups,
        lr=config.learning_rate,
        momentum=0.5,  # Reduced from 0.9 to allow more gradient flow
        curvature=-1.0,
        unitarity_strength=0.1,
        use_cayley=True,
        use_lorentz=True,
        max_grad_norm=optimizer_max_grad,
        weight_decay=config.weight_decay,
    )
    
    # Set parameter names and re-classify
    optimizer.set_param_names(model)
    
    # Print parameter group stats
    stats = optimizer.get_statistics()
    print(f"   Parameter groups: {stats['param_type_counts']}")
    
    # DEBUG: Compare optimizer params vs model params
    optimizer_param_count = sum(len(group['params']) for group in param_groups)
    model_param_count = sum(1 for _ in model.parameters())
    trainable_param_count = sum(1 for p in model.parameters() if p.requires_grad)
    print(f"   [DEBUG] Optimizer params: {optimizer_param_count}, Model params: {model_param_count}, Trainable: {trainable_param_count}")
    if config.dry_run:
        print(f"   [DRY-RUN] BK-HyperSGD max_grad_norm set to {optimizer_max_grad}")
    if optimizer_param_count != trainable_param_count:
        print(f"   âš ï¸ WARNING: Optimizer has FEWER params than model! Some weights won't train.")
    
    # Scaler for mixed precision
    # NOTE: Disable scaler for BK-HyperSGD in dry run to avoid unscale issues
    use_scaler = config.use_mixed_precision and not config.dry_run
    scaler = torch.cuda.amp.GradScaler(enabled=use_scaler)
    if not use_scaler and config.dry_run:
        print("   âš  GradScaler disabled during dry run for BK-HyperSGD compatibility")
    
    # EMA
    ema = EMA(model, decay=config.ema_decay) if config.use_ema else None
    if ema:
        print(f"âœ” EMA Enabled (decay={config.ema_decay})")
    
    # Calculate total steps
    total_steps = config.max_steps if config.max_steps else steps_per_epoch * config.epochs
    
    # LR Scheduler
    scheduler = CosineWarmupScheduler(
        optimizer=optimizer,
        warmup_steps=config.warmup_steps,
        total_steps=total_steps,
        peak_lr=config.learning_rate,
        min_lr=config.min_lr
    )
    print(f"âœ” LR Scheduler: Warmup {config.warmup_steps} steps, Cosine decay to {config.min_lr}")
    
    # Resume from checkpoint if specified
    start_step = 0
    start_epoch = 0
    if config.resume_from and os.path.exists(config.resume_from):
        start_step, start_epoch, _ = load_checkpoint(
            config.resume_from, model, optimizer, scheduler, scaler, ema, device
        )
        # Force LR from config (allows changing LR on resume)
        for param_group in optimizer.param_groups:
            param_group['lr'] = config.learning_rate
        # Also update scheduler so it reflects and displays correct LR
        if hasattr(scheduler, 'base_lrs'):
            scheduler.base_lrs = [config.learning_rate] * len(scheduler.base_lrs)
        scheduler.peak_lr = config.learning_rate
        scheduler._last_lr = config.learning_rate
        print(f"  â†’ LR forced to config value: {config.learning_rate}")
    
    # Training state
    model.train()
    step = start_step
    optimizer_step = 0
    total_loss = 0.0
    skip_count = 0
    
    # Bootstrap LR state tracking
    bootstrap_initial_loss = None  # Track initial loss for adaptive exit
    # By default, apply bootstrap even when resuming (can skip via config.bootstrap_skip_on_resume)
    bootstrap_completed = False
    if config.bootstrap_skip_on_resume and start_step > 0:
        bootstrap_completed = True
        if config.bootstrap_lr > 0:
            print(f"   âš¡ Bootstrap skipped (config bootstrap_skip_on_resume=True, resume step {start_step})")
    
    # Resonance-Adaptive Curvature Optimizer (Phase 8 optimization)
    resonance_curvature = None
    stability_monitor = None
    if _RESONANCE_AVAILABLE:
        try:
            resonance_curvature = ResonanceAdaptiveCurvature(
                model=model,
                initial_curvature=1.0,
                min_curvature=0.1,
                max_curvature=2.0,
                resonance_threshold=0.5,
                adjustment_rate=0.01,
            )
            stability_monitor = StabilityMonitor(
                window_size=100,
                nan_threshold=5,
                exploding_grad_threshold=100.0,
            )
            print("âœ” Resonance-Adaptive Curvature & Stability Monitor Enabled")
        except Exception as e:
            print(f"âš  Resonance optimizers not available: {e}")
    
    # Gradient Teleportation (#9) - Scheduler controlled
    gradient_teleporter = None
    teleporter_hooks_active = False  # Track hook state for scheduler
    if _GRADIENT_TELEPORT_AVAILABLE and config.use_gradient_teleportation:
        try:
            gradient_teleporter = create_gradient_teleporter(
                model=model,
                teleport_strength=config.teleport_strength,
                use_dyson=True,
            )
            # Don't register hooks yet - scheduler will enable after warmup
            print(f"âœ” Gradient Teleportation Prepared (scheduler-controlled, strength={config.teleport_strength})")
        except Exception as e:
            print(f"âš  Gradient Teleportation not available: {e}")
    
    # Revolutionary Training (7 algorithms integration)
    revolutionary_trainer = None
    if _REVOLUTIONARY_AVAILABLE and config.use_revolutionary_training:
        try:
            # Parse enabled algorithms from config
            enabled_algos = config.revolutionary_algorithms.split(',')
            rev_config = RevolutionaryConfig(
                use_holographic='holographic' in enabled_algos,
                use_closed_form='closed_form' in enabled_algos,
                use_topological='topological' in enabled_algos,
                use_retrocausal='retrocausal' in enabled_algos,
                use_zeta='zeta' in enabled_algos,
                use_sheaf='sheaf' in enabled_algos,
                use_diffractive='diffractive' in enabled_algos,
                learning_rate=config.learning_rate,
                log_interval=config.log_interval,
                # CRITICAL: Pass total_steps for phase-based scheduling
                # This ensures proper resume from checkpoints
                total_steps=total_steps,
            )
            revolutionary_trainer = RevolutionaryTrainer(model, rev_config, device)
            # Start in warmup mode - weight modifications disabled until warmup completes
            revolutionary_trainer.set_warmup_mode(True)
            
            # CRITICAL: Restore revolutionary_trainer state if resuming from checkpoint
            # This must happen AFTER revolutionary_trainer is created
            if config.resume_from and os.path.exists(config.resume_from):
                try:
                    ckpt = torch.load(config.resume_from, map_location=device)
                    if 'revolutionary_trainer_state_dict' in ckpt:
                        revolutionary_trainer.load_state_dict(ckpt['revolutionary_trainer_state_dict'])
                        # Update step_count to match global step for phase calculation
                        revolutionary_trainer.step_count = start_step
                    else:
                        # Old checkpoint without this state - sync step_count manually
                        revolutionary_trainer.step_count = start_step
                        print(f"  â„¹ï¸ Revolutionary trainer step synced to {start_step} (no saved state)")
                    del ckpt  # Free memory
                except Exception as e:
                    print(f"  âš  Could not restore revolutionary trainer state: {e}")
                    revolutionary_trainer.step_count = start_step
            
            if getattr(config, 'revolutionary_auto_schedule', True):
                print(f"âœ” Revolutionary Training Enabled (Phase-based Auto-Schedule)")
                print(f"  â””â”€ Warmup(0-10%): OFF | Early(10-30%): 1/5 | Mid(30-70%): 1/3 | Late(70-100%): 1/2")
            else:
                print(f"âœ” Revolutionary Training Enabled: {config.revolutionary_algorithms}")
            if getattr(config, 'revolutionary_fast_start', False) and getattr(config, 'revolutionary_fast_start_window', 0) > 0:
                print(f"  âš¡ Revolutionary fast-start configured: first {config.revolutionary_fast_start_window} optimizer steps = always-on")
        except Exception as e:
            print(f"âš  Revolutionary Training not available: {e}")
    
    # Gradient Sanitization (DISABLED - was limiting gradients too much)
    gradient_sanitizer = None
    # if _GRADIENT_SANITIZER_AVAILABLE:
    #     try:
    #         gradient_sanitizer = create_gradient_sanitizer(
    #             model=model,
    #             use_spectral_norm=True,
    #             use_outlier_detection=True,
    #             use_momentum_smoothing=True,
    #             emergency_grad_max=0.5,
    #         )
    #         print(f"âœ” Gradient Sanitization Enabled (Moonshot #13)")
    #     except Exception as e:
    #         print(f"âš  Gradient Sanitization not available: {e}")
    
    # Stability Suite (DISABLED - was limiting gradients too much)
    stability_manager = None
    # if _STABILITY_SUITE_AVAILABLE:
    #     try:
    #         stability_manager = create_stability_manager(
    #             model=model,
    #             aggressive=True,
    #         )
    #         print(f"âœ” Stability Suite Enabled (Moonshot #14)")
    #         print(f"  â””â”€ Backward Hooks | Layerwise Scaling | Loss Smoothing | Adaptive Precision")
    #     except Exception as e:
    #         print(f"âš  Stability Suite not available: {e}")
    
    # Async Checkpoint Saver - DISABLED
    # Was causing progressive slowdown (5.94s â†’ 20.64s/it) due to GIL contention
    # and memory issues. Using sync-only approach instead.
    async_saver = None
    # if _ASYNC_CHECKPOINT_AVAILABLE:
    #     try:
    #         async_saver = AsyncCheckpointSaver(max_queue_size=2)
    #         print(f"âœ” Async Checkpoint Saver Enabled (background saving)")
    #     except Exception as e:
    #         print(f"âš  Async Checkpoint Saver not available: {e}")
    
    # JSON Log
    training_log = {
        'config': asdict(config),
        'start_time': datetime.now().isoformat(),
        'total_params': total_params,
        'steps': [],
    }
    
    # Dry run mock dataset
    if config.dry_run:
        # Reduce grad_accum_steps for faster iteration (more optimizer steps)
        original_grad_accum = config.grad_accum_steps
        # Run without accumulation to get more optimizer steps during the short dry-run
        config.grad_accum_steps = 1
        print(f"âš¡ Dry-run: grad_accum_steps {original_grad_accum} â†’ {config.grad_accum_steps}")
        
        # Disable features that cause NaN during dry-run (but keep safe revolutionary algos)
        print("âš¡ Dry-run: Adjusting features for stability...")
        config.use_time_reversed = False
        config.use_resonance_locked = False
        config.use_gradient_teleportation = False
        config.use_superposition_training = False
        print("   - Time-Reversed Training: OFF")
        print("   - Resonance-Locked Training: OFF")
        
        # Revolutionary Training: Enable ONLY safe algorithms for compression testing
        # closed_form: doesn't rely on gradients (avoids compression gradient issues)
        # zeta: finds important dimensions (efficient with limited info)
        # holographic: FFT-based (averages out quantization noise)
        if config.use_revolutionary_training:
            config.revolutionary_algorithms = "closed_form,zeta,holographic"
            print(f"   - Revolutionary Training: ON (safe algos only: {config.revolutionary_algorithms})")
        else:
            print("   - Revolutionary Training: OFF")
        
        # Run 250 steps for extended scale-up testing
        steps_to_run = min(250, config.max_steps) if config.max_steps else 250
        print(f"Dry Run: Running {steps_to_run} steps (grad_accum={config.grad_accum_steps})...")
        
        class MockDataset:
            def __init__(self, vocab_size: int):
                # Predict next token (shifted copy task) so loss should fall quickly
                self.vocab_size = vocab_size
            
            def iter_epoch(self, epoch):
                for _ in range(steps_to_run):
                    x = torch.randint(0, self.vocab_size, (config.batch_size, config.n_seq))
                    # Predict a fixed token (class 0) to guarantee a learnable signal
                    targets = torch.zeros_like(x)
                    yield x, targets.reshape(-1)
        
        dataset = MockDataset(config.vocab_size)
        steps_per_epoch = steps_to_run
        total_steps = steps_to_run
        scheduler.total_steps = total_steps  # Align scheduler with shorter dry-run
    
    # Progress bar - use ABSOLUTE step counts for consistency with schedulers
    # This ensures pbar, LR scheduler, and revolutionary trainer all use the same step reference
    pbar = tqdm(
        total=total_steps,  # FIXED: Use absolute total (195312), not remaining
        initial=start_step,  # FIXED: Start from resume point (2480), not 0
        disable=not TQDM_AVAILABLE,
        desc="Training"
    )
    
    # === STEP TIMING LOG SYSTEM ===
    # Logs ALL steps to file for analysis
    import time as _step_time
    _step_timing_log_path = os.path.join(config.save_dir, "step_timing_log.txt")
    _step_timing_buffer = []
    _last_checkpoint_step = start_step  # Track when last checkpoint was saved
    
    def _log_step_timing(step, phase, timing_dict, to_file=True):
        """Log step timing - DISABLED for speed"""
        # NOTE: Logging disabled for maximum training speed
        # To re-enable, uncomment the following:
        # line = f"[Step {step:6d}] {phase:12s} | " + " | ".join(
        #     f"{k}={v:.1f}ms" if isinstance(v, float) else f"{k}={v}"
        #     for k, v in timing_dict.items()
        # )
        # if phase == 'POST-CKPT':
        #     print(f"  â±ï¸ {line}")
        # if to_file:
        #     with open(_step_timing_log_path, 'a') as f:
        #         f.write(f"{datetime.now().isoformat()} {line}\n")
        pass
    
    # Training loop
    for epoch in range(start_epoch, config.epochs):
        for x, y in dataset.iter_epoch(epoch):
            step += 1
            
            # === STEP TIMING START ===
            _step_start_time = _step_time.perf_counter()
            _step_timings = {}
            
            if step <= start_step:
                continue
            
            x, y = x.to(device), y.to(device)
            
            # Initialize gradient metrics (will be updated during optimizer step)
            grad_norm_raw = 0.0
            grad_norm = 0.0
            
            # Stability-aware feature gating (real training only)
            stable_steps = getattr(train_phase8, '_stable_steps', 0)
            time_reversed_active = (
                not config.dry_run
                and config.use_time_reversed
                and stable_steps >= 50  # enable after 50 stable steps
            )
            resonance_locked_active = (
                not config.dry_run
                and config.use_resonance_locked
                and stable_steps >= 30  # enable after 30 stable steps
            )
            if time_reversed_active and not getattr(train_phase8, '_time_rev_on', False):
                print(f"   ðŸ” Time-Reversed Training ACTIVATED (stable_steps={stable_steps})")
                train_phase8._time_rev_on = True
            if resonance_locked_active and not getattr(train_phase8, '_res_lock_on', False):
                print(f"   ðŸ”’ Resonance-Locked Training ACTIVATED (stable_steps={stable_steps})")
                train_phase8._res_lock_on = True
            
            # Forward pass with mixed precision
            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=config.use_mixed_precision):
                # Only collect diagnostics when logging (reduces overhead ~10%)
                collect_diag = (step % config.log_interval == 0)
                logits, diagnostics = model(x, return_diagnostics=collect_diag)
                logits = logits.view(-1, config.vocab_size)
                
                # Check for NaN/Inf in logits
                if torch.isnan(logits).any() or torch.isinf(logits).any():
                    nan_count = torch.isnan(logits).sum().item()
                    inf_count = torch.isinf(logits).sum().item()
                    # For dry-run: continue with clamped logits instead of skipping
                    if config.dry_run:
                        logits = torch.nan_to_num(logits, nan=0.0, posinf=100.0, neginf=-100.0)
                        print(f"âš  [DRY-RUN] Clamped NaN/Inf logits at step {step} (NaN: {nan_count}, Inf: {inf_count})")
                    else:
                        print(f"ðŸš¨ NaN/Inf in logits at step {step}! (NaN: {nan_count}, Inf: {inf_count})")
                        optimizer.zero_grad()
                        skip_count += 1
                        total_loss = 0.0
                        pbar.update(1)
                        continue
                
                # Loss with label smoothing (forward direction)
                loss_forward = F.cross_entropy(logits, y, label_smoothing=config.label_smoothing)
                
                # DEBUG: Print logits stats to verify model output is changing (DISABLED for speed)
                # if step <= 8 or step % 20 == 0:
                #     with torch.no_grad():
                #         logits_mean = logits.mean().item()
                #         logits_std = logits.std().item()
                #         logits_max = logits.max().item()
                #         print(f"  [LOGITS] Step {step}: mean={logits_mean:.4f}, std={logits_std:.4f}, max={logits_max:.4f}, loss={loss_forward.item():.4f}")
                
                # Time-Reversed Training (#10 Moonshot)
                # Train on reversed sequences for bi-directional consistency
                if time_reversed_active:
                    x_rev = x.flip(1)  # Reverse sequence
                    y_rev = y.view(x.shape[0], -1).flip(1).view(-1)  # Reverse targets
                    logits_rev, _ = model(x_rev, return_diagnostics=False)
                    logits_rev = logits_rev.view(-1, config.vocab_size)
                    loss_backward = F.cross_entropy(logits_rev, y_rev, label_smoothing=config.label_smoothing)
                    loss = (1 - config.time_reversed_weight) * loss_forward + config.time_reversed_weight * loss_backward
                else:
                    loss = loss_forward
                
                loss = loss / config.grad_accum_steps
            
            # Apply loss smoothing from Stability Manager (prevents sharp spikes)
            if stability_manager is not None:
                loss = stability_manager.process_loss(loss)
            
            # Check for NaN/Inf loss
            if torch.isnan(loss) or torch.isinf(loss):
                # For dry-run: use small valid loss instead of skipping
                if config.dry_run:
                    loss = torch.tensor(10.0, device=loss.device, requires_grad=True)
                    print(f"âš  [DRY-RUN] Replaced NaN/Inf loss with 10.0 at step {step}")
                else:
                    print(f"ðŸš¨ NaN/Inf loss at step {step}! Value: {loss.item() if not torch.isnan(loss) else 'NaN'}")
                    optimizer.zero_grad()
                    skip_count += 1
                    total_loss = 0.0
                    pbar.update(1)
                    continue
            
            # Backward (hooks from Stability Manager will catch NaN gradients here)
            # For dry-run: skip scaler to avoid GradScaler state issues
            if config.dry_run:
                loss.backward()
            else:
                scaler.scale(loss).backward()
            total_loss += loss.item() * config.grad_accum_steps
            
            # Optimizer step (every grad_accum_steps)
            if step % config.grad_accum_steps == 0:
                optimizer_step += 1
                
                # Unscale gradients (skip for dry-run to avoid state corruption)
                if not config.dry_run:
                    scaler.unscale_(optimizer)
                
                # NOTE: Per-parameter gradient clipping REMOVED - was killing gradient flow
                # The Stability Suite backward hooks (Â±10.0) handle NaN prevention now
                
                # === Gradient Sanitization (Moonshot #13) ===
                # This replaces manual NaN checking with comprehensive gradient cleaning
                nan_grad_count = 0
                if gradient_sanitizer is not None:
                    sanitize_stats = gradient_sanitizer.sanitize_gradients()
                    nan_grad_count = sanitize_stats.get('nan_fixed', 0) + sanitize_stats.get('inf_fixed', 0)
                    
                    if nan_grad_count > 0 and not hasattr(train_phase8, '_nan_debug_printed'):
                        train_phase8._nan_debug_printed = True
                        print(f"âš  Gradient Sanitizer fixed {nan_grad_count} NaN/Inf values at step {step}")
                        print(f"  Outliers removed: {sanitize_stats.get('outliers_removed', 0)}")
                        print(f"  Emergency recoveries: {sanitize_stats.get('emergency_recovery', 0)}")
                else:
                    # Fallback: Manual NaN/Inf check (if sanitizer not available)
                    nan_layers = []
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                                nan_grad_count += 1
                                if len(nan_layers) < 5:
                                    nan_layers.append(name)
                                param.grad.data = torch.nan_to_num(param.grad.data, nan=0.0, posinf=0.0, neginf=0.0)
                    
                    if nan_grad_count > 0 and not hasattr(train_phase8, '_nan_debug_printed'):
                        train_phase8._nan_debug_printed = True
                        print(f"âš  NaN/Inf in {nan_grad_count} parameter gradients at step {step}")
                        print(f"  First problematic layers: {nan_layers}")
                    elif nan_grad_count > 0:
                        # Subsequent NaN warnings (shorter message)
                        pass  # Silent after first warning
                
                # ===== Gradient Norm Computation \u0026 Clipping (Muon Optimized) =====
                
                # === PRE-CLIPPING FOR MUON (DISABLED - causing loss stagnation) ===
                # NOTE: Muon's internal stabilization handles gradient control.
                # Keeping external clipping minimal to allow gradient flow.
                # The main clip_grad_norm_ at L1101 provides sufficient safety.
                
                # Compute raw gradient norm (now reflects pre-clipped values for Muon)
                with torch.no_grad():
                    total_norm_sq = sum(p.grad.pow(2).sum() for p in model.parameters() if p.grad is not None)
                    grad_norm_raw = torch.sqrt(total_norm_sq).item()
                
                # Handle NaN/Inf in raw norm
                if math.isnan(grad_norm_raw) or math.isinf(grad_norm_raw):
                    grad_norm_raw = 0.0
                    grad_norm = 0.0
                else:
                    if config.dry_run:
                        # Let gradients flow freely in dry-run to observe real learning
                        grad_norm = grad_norm_raw
                    else:
                        # === ADAPTIVE GRADIENT CONTROL (GradientFeederV2.1 Hybrid) ===
                        # Hybrid control: threshold for explosion, scaling for vanishing
                        if 'gradient_feeder' not in dir(train_phase8):
                            if _GRADIENT_FEEDER_AVAILABLE:
                                train_phase8.gradient_feeder = GradientFeederV2(
                                    target_low=0.5,
                                    target_high=3.0,
                                    initial_threshold=50.0,
                                    reaction_speed=0.4,
                                )
                                print("âœ” GradientFeederV2.1 initialized (hybrid: threshold + scaling)")
                            else:
                                train_phase8.gradient_feeder = None
                        
                        feeder = getattr(train_phase8, 'gradient_feeder', None)
                        if feeder is not None:
                            # V2.1: Returns threshold, scale, and stats
                            adaptive_threshold, scale_factor, feeder_stats = feeder.feed(grad_norm_raw)
                            
                            # Step 1: Apply emergency scaling if needed (vanishing prevention)
                            if scale_factor > 1.0:
                                feeder.apply_scaling(model, scale_factor)
                            
                            # Step 2: Apply clipping with adaptive threshold (explosion prevention)
                            grad_norm = torch.nn.utils.clip_grad_norm_(
                                model.parameters(), 
                                max_norm=adaptive_threshold,
                                error_if_nonfinite=False
                            ).item()
                            
                            # Log feeder action (less frequently)
                            if step % (config.log_interval * 5) == 0 and feeder_stats.action != 'hold':
                                print(f"   âš¡ GradFeeder: {feeder_stats.action} thr={adaptive_threshold:.1f} scale={scale_factor:.2f} (grad={grad_norm_raw:.3f}â†’{feeder_stats.grad_norm_output:.3f})")
                        else:
                            # Fallback: static clipping
                            max_grad_norm = 50.0
                            grad_norm = torch.nn.utils.clip_grad_norm_(
                                model.parameters(), 
                                max_norm=max_grad_norm,
                                error_if_nonfinite=False
                            ).item()
                        
                        # Fallback if clip_grad_norm_ fails
                        if math.isnan(grad_norm) or math.isinf(grad_norm):
                            grad_norm = grad_norm_raw
                
                # Failsafe: Skip if raw norm is extremely large (gradient explosion)
                # NOTE: For dry-run, we disable this entirely to test if model can learn
                effective_skip_threshold = config.grad_skip_threshold
                if config.dry_run:
                    # Dry-run: disable skip entirely to see actual training behavior
                    effective_skip_threshold = float('inf')
                elif config.optimizer_type in ('muon', 'riemannian_muon'):
                    # Muon/RiemannianMuonBit use orthogonalization which makes raw grad_norm misleading
                    effective_skip_threshold = 10000.0
                
                if grad_norm_raw > effective_skip_threshold:
                    # åˆæœŸã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
                    if step <= 10:
                        print(f"  ðŸ” Step {step} DEBUG: grad_norm_raw={grad_norm_raw:.2f}, threshold={effective_skip_threshold}")
                        print(f"     Largest gradient layers:")
                        layer_grads = []
                        for name, param in model.named_parameters():
                            if param.grad is not None:
                                g_norm = param.grad.norm().item()
                                if g_norm > 1.0:  # Only show significant gradients
                                    layer_grads.append((name, g_norm))
                        # Sort and show top 5
                        layer_grads.sort(key=lambda x: -x[1])
                        for name, g_norm in layer_grads[:5]:
                            print(f"       {name}: {g_norm:.2f}")
                    
                    print(f"âš  Grad norm {grad_norm_raw:.2f} > {effective_skip_threshold}, skipping step")
                    optimizer.zero_grad()
                    if not config.dry_run:
                        scaler.update()  # Reset scaler state
                    skip_count += 1
                    total_loss = 0.0
                    pbar.update(1)
                    continue
                
                # Resonance-Locked Training (#6 Moonshot)
                # Skip updates when gradient SNR is low (high noise)
                resonance_skip = False
                if resonance_locked_active and step > config.warmup_steps:
                    # Approximate Gradient Noise Scale (GNS)
                    # GNS â‰ˆ grad_variance / grad_meanÂ² 
                    # High GNS = noisy gradient, skip update
                    grads = [p.grad for p in model.parameters() if p.grad is not None]
                    if grads:
                        all_grads = torch.cat([g.flatten() for g in grads])
                        grad_mean = all_grads.mean().abs()
                        grad_var = all_grads.var()
                        gns = (grad_var / (grad_mean ** 2 + 1e-8)).item()
                        
                        if gns > config.resonance_gns_threshold:
                            # High noise, skip update (resonance not achieved)
                            resonance_skip = True
                            skip_count += 1
                            if step % 50 == 0:
                                print(f"ðŸ”’ Resonance skip: GNS={gns:.2f} > {config.resonance_gns_threshold}")
                
                if resonance_skip:
                    optimizer.zero_grad()
                    total_loss = 0.0
                    pbar.update(1)
                    continue
                
                # NOTE: Redundant 2nd gradient clipping REMOVED (was causing loss stagnation)
                # The clip_grad_norm_ at L1101-1104 already handles clipping.
                
                # Optimizer step
                # For dry-run: call optimizer.step() directly to ensure updates happen
                # GradScaler may skip updates if it detects inf, causing loss stagnation
                
                # DEBUG: Save first param weight before step (DISABLED for speed - causes GPUâ†’CPU transfer)
                # first_param = next(model.parameters())
                # weight_before = first_param.data.flatten()[0].item()
                
                # Bootstrap LR override (adaptive exit based on loss decrease)
                # Continues until: (1) loss drops by threshold, OR (2) max steps reached
                current_loss = total_loss / max(1, step % config.grad_accum_steps if step % config.grad_accum_steps != 0 else config.grad_accum_steps)
                
                # Track initial loss for bootstrap (only on fresh start, not resume)
                if bootstrap_initial_loss is None and not config.dry_run and not bootstrap_completed:
                    bootstrap_initial_loss = current_loss
                    if config.bootstrap_lr > 0:
                        print(f"  ðŸ“Š Bootstrap initial loss: {bootstrap_initial_loss:.4f}")
                
                # Check if we should exit bootstrap (loss decreased enough)
                loss_decrease = (bootstrap_initial_loss - current_loss) if bootstrap_initial_loss else 0.0
                
                using_bootstrap = (
                    not config.dry_run
                    and not bootstrap_completed
                    and config.bootstrap_lr > 0
                    and config.bootstrap_steps > 0
                    and optimizer_step < config.bootstrap_steps
                    and loss_decrease < config.bootstrap_exit_threshold
                )
                
                if using_bootstrap:
                    for group in optimizer.param_groups:
                        group['lr'] = config.bootstrap_lr
                elif not bootstrap_completed and not config.dry_run and config.bootstrap_lr > 0:
                    # Bootstrap just ended - log reason
                    if loss_decrease >= config.bootstrap_exit_threshold:
                        print(f"  ðŸŽ¯ Bootstrap completed! Loss decreased by {loss_decrease:.4f} (threshold: {config.bootstrap_exit_threshold})")
                    elif optimizer_step >= config.bootstrap_steps:
                        print(f"  âš ï¸ Bootstrap ended at max steps ({config.bootstrap_steps}) - loss decrease was {loss_decrease:.4f}")
                    bootstrap_completed = True
                    # Restore normal LR
                    for group in optimizer.param_groups:
                        group['lr'] = config.learning_rate
                
                if config.dry_run:
                    optimizer.step()
                else:
                    scaler.step(optimizer)
                    scaler.update()
                
                # DEBUG: Check weight change after step (DISABLED for speed)
                # weight_after = first_param.data.flatten()[0].item()
                # weight_diff = abs(weight_after - weight_before)
                
                # Print debug info on first few optimizer steps (DISABLED for speed)
                # if optimizer_step <= 3:
                #     print(f"  [DEBUG] Optimizer step {optimizer_step}: weight_diff={weight_diff:.2e}, lr={scheduler.get_last_lr():.2e}")
                #     if weight_diff == 0:
                #         print(f"  âš ï¸ WARNING: Weights did NOT change! Optimizer may not be working.")
                
                optimizer.zero_grad()
                
                # LR scheduler step
                if not config.dry_run and not using_bootstrap:
                    scheduler.step()
                
                # EMA update
                if ema is not None:
                    ema.update()
                
                # === State-Based Scheduler ===
                # Enable advanced features only when training is stable
                # Criteria: stable_steps consecutive steps with healthy metrics
                
                # Compute avg_loss for stability check
                current_avg_loss = total_loss / config.grad_accum_steps
                
                # Check current state stability
                is_stable = (
                    not math.isnan(grad_norm) and 
                    grad_norm <= 5.0 and            # Healthy grad norm
                    grad_norm > 0.01 and            # Not dead gradients
                    current_avg_loss < 20.0 and     # Loss not exploded
                    nan_grad_count == 0             # No NaN/Inf in gradients this step
                )
                
                # Track consecutive stable steps
                if is_stable:
                    stable_steps = getattr(train_phase8, '_stable_steps', 0) + 1
                else:
                    stable_steps = 0
                train_phase8._stable_steps = stable_steps
                
                # Thresholds for enabling features (in stable steps)
                STABLE_FOR_REVOLUTIONARY = getattr(config, 'revolutionary_min_stable_steps', 50)
                STABLE_FOR_TELEPORTATION = 100  # Need 100 stable steps
                
                # Revolutionary Training - State-based control
                if revolutionary_trainer is not None:
                    try:
                        # CRITICAL: Sync step_count with global step for correct phase calculation
                        # This ensures phase = step / total_steps uses absolute progress
                        revolutionary_trainer.step_count = step
                        
                        fast_start_window = getattr(config, 'revolutionary_fast_start_window', 0)
                        # Fast-start keyed to optimizer steps (after resume, we still want immediate activation even with large grad_accum)
                        fast_start_active = (
                            getattr(config, 'revolutionary_fast_start', False)
                            and optimizer_step < fast_start_window
                        )
                        
                        if fast_start_active:
                            # Immediately enable revolutionary algorithms for the first few optimizer steps,
                            # then hand back to the scheduler.
                            revolutionary_trainer.set_warmup_mode(False)
                            should_apply = True  # every step during fast start
                            if not getattr(train_phase8, '_rev_fast_start_logged', False):
                                print(f"  âš¡ Revolutionary fast-start active (global step {step}, optimizer steps 1-{fast_start_window})")
                                train_phase8._rev_fast_start_logged = True
                        elif getattr(config, 'revolutionary_auto_schedule', True):
                            if stable_steps < STABLE_FOR_REVOLUTIONARY:
                                # Not stable enough: keep warmup mode
                                revolutionary_trainer.set_warmup_mode(True)
                                should_apply = False
                            elif stable_steps < STABLE_FOR_REVOLUTIONARY * 2:
                                # Stable but cautious: apply every 5 steps
                                revolutionary_trainer.set_warmup_mode(False)
                                should_apply = (step % 5 == 0)
                            elif stable_steps < STABLE_FOR_REVOLUTIONARY * 4:
                                # Very stable: apply every 3 steps
                                should_apply = (step % 3 == 0)
                            else:
                                # Maximum stability: apply every 2 steps
                                should_apply = (step % 2 == 0)
                        else:
                            revolutionary_trainer.set_warmup_mode(False)
                            should_apply = True
                        
                        if should_apply:
                            loss_fn = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)
                            rev_loss, rev_metrics = revolutionary_trainer.train_step(x, y, loss_fn)
                            algo_used = rev_metrics.get('algorithm', 'unknown')
                            if step % config.log_interval == 0:
                                stability_level = "ðŸ”´ warmup" if stable_steps < STABLE_FOR_REVOLUTIONARY else "ðŸŸ¡ cautious" if stable_steps < STABLE_FOR_REVOLUTIONARY * 2 else "ðŸŸ¢ stable" if stable_steps < STABLE_FOR_REVOLUTIONARY * 4 else "ðŸŒŸ optimal"
                                print(f"  ðŸ”„ Revolutionary [{stability_level}]: {algo_used}")
                    except Exception as e:
                        # Silently skip (dtype mismatch during fast-start is expected)
                        pass
                
                # Gradient Teleportation - State-based control
                if gradient_teleporter is not None:
                    if stable_steps < STABLE_FOR_TELEPORTATION:
                        # Not stable enough: disable teleportation
                        if teleporter_hooks_active:
                            gradient_teleporter.remove_hooks()
                            teleporter_hooks_active = False
                    else:
                        # Stable: enable teleportation
                        if not teleporter_hooks_active:
                            gradient_teleporter.register_hooks()
                            teleporter_hooks_active = True
                            print(f"  âš¡ Gradient Teleportation activated (stable_steps={stable_steps})")


                # Resonance-Adaptive Curvature step (Phase 8 optimization)
                if resonance_curvature is not None and 'phase8' in (diagnostics or {}):
                    phase8_diag = diagnostics.get('phase8', {})
                    if 'G_ii_mean' in phase8_diag:
                        # Create a dummy G_ii tensor from diagnostics
                        g_ii_val = phase8_diag.get('G_ii_mean', 0.0)
                        g_ii_tensor = torch.tensor([g_ii_val], device=device, dtype=torch.complex64)
                        res_diag = resonance_curvature.step(g_ii_tensor)
                
                # Compute metrics FIRST (before resetting total_loss)
                avg_loss = total_loss / config.grad_accum_steps
                ppl = math.exp(min(avg_loss, 20.0))
                
                # CRITICAL: Reset total_loss AFTER computing avg_loss
                total_loss = 0.0
                
                # Stability Monitor update (now avg_loss is defined)
                had_nan = False
                if stability_monitor is not None:
                    status = stability_monitor.update(
                        loss=avg_loss,
                        grad_norm=grad_norm,
                        had_nan=had_nan
                    )
                    if status.get('warning'):
                        print(f"âš  Stability: {status['warning']}")
                
                # Stability Manager step (Moonshot #14)
                if stability_manager is not None:
                    stability_manager.step()
                
                current_lr = scheduler.get_last_lr()
                if using_bootstrap:
                    current_lr = [config.bootstrap_lr]
                
                # Extract first element if list (scheduler.get_last_lr() returns list)
                lr_value = current_lr[0] if isinstance(current_lr, list) else current_lr
                
                # Update progress bar (shorter description to show ETA)
                pbar.set_description(f"E{epoch}")
                pbar.set_postfix({
                    'loss': f'{avg_loss:.3f}',
                    'ppl': f'{ppl:.0f}',
                    'lr': f'{lr_value:.1e}',
                    'gN': f'{grad_norm:.2f}',  # Clipped norm
                    'gR': f'{grad_norm_raw:.2f}',  # Raw norm
                })
                
                # Log step
                step_log = {
                    'step': step,
                    'optimizer_step': optimizer_step,
                    'epoch': epoch,
                    'loss': avg_loss,
                    'ppl': ppl,
                    'lr': current_lr,
                    'grad_norm': grad_norm,  # Clipped
                    'grad_norm_raw': grad_norm_raw,  # Before clipping
                    'grad_clip': config.grad_clip_train if hasattr(config, 'grad_clip_train') else None,
                }
                
                # Add Muon-specific metrics (if using Muon optimizer)
                if config.optimizer_type == 'muon' and hasattr(optimizer, 'get_muon_metrics'):
                    muon_metrics = optimizer.get_muon_metrics()
                    if muon_metrics.get('stabilization_enabled', False):
                        step_log['muon_phase'] = muon_metrics.get('phase', 'unknown')
                        step_log['muon_nan_count'] = muon_metrics.get('ortho_nan_count', 0)
                        # Log detailed Muon info periodically
                        if step % config.log_interval == 0 and 'avg_grad_norm' in muon_metrics:
                            print(f"  ðŸ”§ Muon [{muon_metrics.get('phase', 'N/A')}]: "
                                  f"AvgGrad={muon_metrics.get('avg_grad_norm', 0):.3f}, "
                                  f"NaN={muon_metrics.get('total_nan_count', 0)}")
                
                # Add diagnostics (only if collected)
                if diagnostics is not None:
                    for k, v in diagnostics.items():
                        if isinstance(v, torch.Tensor):
                            step_log[k] = v.item() if v.numel() == 1 else v.mean().item()
                        elif isinstance(v, (int, float)):
                            step_log[k] = v
                
                training_log['steps'].append(step_log)
                
                # Save checkpoint at save_interval
                if optimizer_step > 0 and optimizer_step % (config.save_interval // config.grad_accum_steps) == 0:
                    ckpt_path = os.path.join(config.save_dir, f"step_{step}.pt")
                    
                    # === ASYNC CHECKPOINT SAVE (Minimal Blocking) ===
                    # Strategy: Quick copy to CPU dict, then background thread for disk I/O
                    import threading
                    
                    def _async_save_checkpoint(path, checkpoint_data, save_dir, training_log_copy):
                        """Background thread: save checkpoint and cleanup old ones"""
                        try:
                            # Save checkpoint (all data already on CPU)
                            torch.save(checkpoint_data, path)
                            print(f"\n  ï¿½ Checkpoint saved: {path}")
                            
                            # Cleanup old checkpoints in background
                            cleanup_old_checkpoints(save_dir, max_keep=2)
                            
                            # Save training log
                            log_path = os.path.join(save_dir, "training_log.json")
                            save_training_log(training_log_copy, log_path)
                        except Exception as e:
                            print(f"\n  âš  Checkpoint save failed: {e}")
                    
                    # Quick: Build checkpoint dict with CPU copies (this is the blocking part)
                    # Access underlying model for torch.compile'd models
                    _model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model
                    
                    # Copy state dicts to CPU (minimal blocking)
                    _ckpt_data = {
                        'step': step,
                        'epoch': epoch,
                        'loss': avg_loss,
                        'model_state_dict': {k: v.cpu().clone() for k, v in _model_to_save.state_dict().items()},
                        'optimizer_state_dict': {k: (v.cpu().clone() if isinstance(v, torch.Tensor) else v) 
                                                 for k, v in optimizer.state_dict().items()},
                        'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else {},
                        'scaler_state_dict': scaler.state_dict(),
                        'config': asdict(config) if hasattr(config, '__dataclass_fields__') else config,
                    }
                    if ema is not None and hasattr(ema, 'state_dict'):
                        _ema_state = ema.state_dict()
                        _ckpt_data['ema_state_dict'] = {k: (v.cpu().clone() if isinstance(v, torch.Tensor) else v) 
                                                        for k, v in _ema_state.items()} if isinstance(_ema_state, dict) else _ema_state
                    if revolutionary_trainer is not None and hasattr(revolutionary_trainer, 'state_dict'):
                        _ckpt_data['revolutionary_trainer_state_dict'] = revolutionary_trainer.state_dict()
                    
                    # Copy training log for background save
                    _training_log_copy = {
                        'last_update': datetime.now().isoformat(),
                        'steps': training_log['steps'][-1000:] if len(training_log['steps']) > 1000 else training_log['steps'][:],
                    }
                    
                    # Launch background thread for disk I/O (non-blocking)
                    _save_thread = threading.Thread(
                        target=_async_save_checkpoint,
                        args=(ckpt_path, _ckpt_data, config.save_dir, _training_log_copy),
                        daemon=True
                    )
                    _save_thread.start()
                    
                    # Update tracking (no memory cleanup - let Python GC handle it naturally)
                    _last_checkpoint_step = step
                
                total_loss = 0.0
            
            # === STEP TIMING END === (DISABLED for speed)
            # NOTE: Step timing logging disabled - removes ~100ms overhead per step
            # torch.cuda.synchronize()  # DISABLED - major sync overhead
            # _step_total_ms = (_step_time.perf_counter() - _step_start_time) * 1000
            # _step_timings['total'] = _step_total_ms
            # _step_timings['mode'] = 'POST-CKPT' if (step > _last_checkpoint_step and step <= _last_checkpoint_step + 34) else 'NORMAL'
            # _log_step_timing(step, _step_timings['mode'], _step_timings)
            pass
            
            pbar.update(1)
            
            if config.max_steps and step >= config.max_steps:
                break
        
        if config.max_steps and step >= config.max_steps:
            break
    
    pbar.close()
    
    # Shutdown async checkpoint saver (wait for pending saves)
    if async_saver is not None:
        print("â³ Waiting for pending checkpoint saves...")
        async_saver.shutdown(timeout=60.0)
        print("âœ” All checkpoints saved")
    
    print("\nâœ… Training Complete!")
    print(f"Total steps: {step}, Optimizer steps: {optimizer_step}, Skipped: {skip_count}")
    
    # Dry-run stability summary
    if config.dry_run:
        print("\n" + "=" * 50)
        print("ðŸ§ª DRY RUN STABILITY SUMMARY")
        print("=" * 50)
        
        # Collect grad norms from training log
        grad_norms = [s.get('grad_norm', 0) for s in training_log['steps'] if s.get('grad_norm') is not None]
        losses = [s.get('loss', 0) for s in training_log['steps'] if s.get('loss') is not None]
        
        # Count NaN/Inf warnings (approximate from skip_count)
        nan_count = skip_count
        
        # Compute grad norm stats
        if grad_norms:
            grad_min = min(grad_norms)
            grad_max = max(grad_norms)
            grad_avg = sum(grad_norms) / len(grad_norms)
        else:
            grad_min = grad_max = grad_avg = 0
        
        # Check criteria (relaxed thresholds)
        nan_ok = nan_count == 0
        grad_ok = grad_max <= 10.0 and grad_avg <= 5.0  # Relaxed from avg<=2.0
        loss_ok = len(losses) >= 2 and losses[-1] < losses[0] * 1.1  # Loss not exploding
        
        # Check for NaN warnings in gradients (separate from skipped steps)
        has_nan_warnings = hasattr(train_phase8, '_nan_debug_printed')
        
        print(f"\nðŸ“Š Results:")
        print(f"   NaN/Inf skipped steps: {nan_count} {'âœ…' if nan_ok else 'âŒ'}")
        if has_nan_warnings:
            print(f"   NaN/Inf in gradients: âš ï¸  detected (zeroed out - training continues)")
        print(f"   Grad norm range: {grad_min:.3f} ~ {grad_max:.3f} (avg: {grad_avg:.3f}) {'âœ…' if grad_ok else 'âŒ'}")
        if losses:
            print(f"   Loss: {losses[0]:.3f} â†’ {losses[-1]:.3f} {'âœ…' if loss_ok else 'âŒ'}")
        
        print("\n" + "-" * 50)
        if nan_ok and grad_ok and not has_nan_warnings:
            print("ðŸŽ‰ STABILITY CHECK PASSED!")
            print("   Safe to run: make train-japanese")
        elif nan_ok and grad_ok:
            print("ðŸŸ¡ STABILITY CHECK PASSED (with warnings)")
            print("   NaN/Inf in gradients detected but zeroed out")
            print("   Training can proceed but investigate source")
        else:
            print("âš ï¸  STABILITY CHECK FAILED")
            if not nan_ok:
                print("   â†’ NaN/Inf detected - check model initialization")
            if not grad_ok:
                print("   â†’ Grad norm too high - reduce learning rate")
        print("=" * 50)
    else:
        # Save final checkpoint (only for real training)
        final_path = os.path.join(config.save_dir, "phase8_10b_final.pt")
        save_checkpoint(
            final_path, model, optimizer, scheduler, scaler, ema,
            step, epoch, avg_loss if 'avg_loss' in dir() else 0.0, config
        )
    
    # Save final training log
    training_log['end_time'] = datetime.now().isoformat()
    training_log['final_step'] = step
    training_log['final_loss'] = training_log['steps'][-1]['loss'] if training_log['steps'] else None
    log_path = os.path.join(config.save_dir, "training_log.json")
    save_training_log(training_log, log_path)
    print(f"ðŸ“ Training log saved to {log_path}")


if __name__ == "__main__":
    train_phase8()
