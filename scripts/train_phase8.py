#!/usr/bin/env python3
"""
Phase 8 Training Script - 10B Parameter Scale Model (Optimized)
(ResNetBK + HTT + Hybrid Hyperbolic Attention + Low-Rank BitNet)

Features:
- 10B Parameter Scale Support via >99% Compression
- Low-Rank FFN & Attention with BitNet 1.58-bit Quantization
- BK-Core Hyperbolic Integration
- AR-SSM Hyperbolic Fusion
- Gradient Accumulation for Low-VRAM Training
- LR Scheduler (Linear Warmup + Cosine Decay)
- EMA (Exponential Moving Average)
- Label Smoothing
- Proper Checkpoint Saving

Usage:
    python scripts/train_phase8.py --config configs/phase8_10b_japanese.yaml
"""

import argparse
import copy
import json
import math
import os
import sys
import time
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from datetime import datetime
from collections import OrderedDict

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.phase8.integrated_model import Phase8IntegratedModel, Phase8Config
from src.optimizers.muon import Muon

# Import new Riemannian-Muon-Bit optimizer (Phase 1: Hyperbolic Training)
try:
    from src.optimizers.riemannian_muon_bit import RiemannianMuonBit, create_riemannian_muon_bit
    _RIEMANNIAN_MUON_AVAILABLE = True
except ImportError:
    _RIEMANNIAN_MUON_AVAILABLE = False
    RiemannianMuonBit = None

# Import BK-HyperSGD optimizer (Phase 1: ResNet-BK specialized optimizer)
try:
    from src.optimizers.bk_hyper_sgd import BKHyperSGD, create_bk_hyper_sgd, get_bk_parameter_groups
    _BK_HYPER_SGD_AVAILABLE = True
except ImportError:
    _BK_HYPER_SGD_AVAILABLE = False
    BKHyperSGD = None
    create_bk_hyper_sgd = None

# Import BK Isometry Initialization (Phase 1: Energy-preserving init)
try:
    from src.models.phase8.bk_isometry_init import BKIsometryInitializer, apply_bk_isometry_init
    _BK_ISOMETRY_AVAILABLE = True
except ImportError:
    _BK_ISOMETRY_AVAILABLE = False
    BKIsometryInitializer = None
    apply_bk_isometry_init = None

# Lazy import for data_utils (requires datasets library)
def get_mixed_data_loader(*args, **kwargs):
    from src.utils.data_utils import get_mixed_data_loader as _loader
    return _loader(*args, **kwargs)

# Import Phase 8 optimization kernels
try:
    from src.kernels.resonance_adaptive_curvature import ResonanceAdaptiveCurvature, StabilityMonitor
    _RESONANCE_AVAILABLE = True
except ImportError:
    _RESONANCE_AVAILABLE = False
    ResonanceAdaptiveCurvature = None
    StabilityMonitor = None

# Import Gradient Teleportation (#9)
try:
    from src.kernels.gradient_teleportation import GradientTeleporter, create_gradient_teleporter
    _GRADIENT_TELEPORT_AVAILABLE = True
except ImportError:
    _GRADIENT_TELEPORT_AVAILABLE = False
    GradientTeleporter = None
    create_gradient_teleporter = None

# Import Revolutionary Training (7 algorithms)
try:
    from src.training.revolutionary_trainer import (
        RevolutionaryTrainer,
        RevolutionaryConfig,
        create_revolutionary_trainer,
    )
    _REVOLUTIONARY_AVAILABLE = True
except ImportError:
    _REVOLUTIONARY_AVAILABLE = False
    RevolutionaryTrainer = None
    RevolutionaryConfig = None
    create_revolutionary_trainer = None

# Import Gradient Sanitization (Moonshot #13)
try:
    from src.training.gradient_sanitization import (
        GradientSanitizer,
        GradientSanitizationConfig,
        create_gradient_sanitizer,
    )
    _GRADIENT_SANITIZER_AVAILABLE = True
except ImportError:
    _GRADIENT_SANITIZER_AVAILABLE = False
    GradientSanitizer = None
    create_gradient_sanitizer = None

# Import Stability Suite (Moonshot #14 - Comprehensive NaN Elimination)
try:
    from src.training.stability_suite import (
        StabilityManager,
        StabilityConfig,
        create_stability_manager,
        BackwardHookNaNEliminator,
        LayerwiseGradientScaler,
    )
    _STABILITY_SUITE_AVAILABLE = True
except ImportError:
    _STABILITY_SUITE_AVAILABLE = False
    StabilityManager = None
    create_stability_manager = None

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)


# =============================================================================
# EMA (Exponential Moving Average)
# =============================================================================
class EMA:
    """
    Exponential Moving Average of model weights.
    Updates shadow weights as: shadow = decay * shadow + (1 - decay) * model_weights
    """
    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self._init_shadow()
    
    def _init_shadow(self):
        """Initialize shadow weights from model."""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        """Update shadow weights with current model weights."""
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.shadow[name] = (
                    self.decay * self.shadow[name] + 
                    (1 - self.decay) * param.data
                )
    
    def apply_shadow(self):
        """Apply shadow weights to model (for evaluation)."""
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        """Restore original weights after evaluation."""
        for name, param in self.model.named_parameters():
            if name in self.backup:
                param.data = self.backup[name]
        self.backup = {}
    
    def state_dict(self) -> Dict:
        """Return EMA state for checkpointing."""
        return {
            'decay': self.decay,
            'shadow': {k: v.cpu() for k, v in self.shadow.items()}
        }
    
    def load_state_dict(self, state_dict: Dict):
        """Load EMA state from checkpoint."""
        self.decay = state_dict['decay']
        device = next(self.model.parameters()).device
        self.shadow = {k: v.to(device) for k, v in state_dict['shadow'].items()}


# =============================================================================
# Learning Rate Scheduler (Linear Warmup + Cosine Decay)
# =============================================================================
class CosineWarmupScheduler:
    """
    Learning rate scheduler with linear warmup and cosine decay.
    
    LR schedule:
    - Warmup (0 to warmup_steps): Linear from 0 to peak_lr
    - Decay (warmup_steps to total_steps): Cosine from peak_lr to min_lr
    """
    def __init__(
        self,
        optimizer: optim.Optimizer,
        warmup_steps: int,
        total_steps: int,
        peak_lr: float,
        min_lr: float = 1e-6
    ):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.peak_lr = peak_lr
        self.min_lr = min_lr
        self.current_step = 0
        self._last_lr = 0.0
    
    def get_lr(self, step: int) -> float:
        """Calculate learning rate for given step."""
        if step < self.warmup_steps:
            # Linear warmup
            return self.peak_lr * step / max(1, self.warmup_steps)
        else:
            # Cosine decay
            progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)
            progress = min(1.0, progress)  # Clamp to [0, 1]
            return self.min_lr + 0.5 * (self.peak_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
    
    def step(self):
        """Update learning rate for current step."""
        self.current_step += 1
        lr = self.get_lr(self.current_step)
        self._last_lr = lr
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_last_lr(self) -> float:
        """Return last computed learning rate."""
        return self._last_lr
    
    def state_dict(self) -> Dict:
        """Return scheduler state for checkpointing."""
        return {
            'current_step': self.current_step,
            'warmup_steps': self.warmup_steps,
            'total_steps': self.total_steps,
            'peak_lr': self.peak_lr,
            'min_lr': self.min_lr
        }
    
    def load_state_dict(self, state_dict: Dict):
        """Load scheduler state from checkpoint."""
        self.current_step = state_dict['current_step']
        self.warmup_steps = state_dict['warmup_steps']
        self.total_steps = state_dict['total_steps']
        self.peak_lr = state_dict['peak_lr']
        self.min_lr = state_dict['min_lr']


# =============================================================================
# Training Configuration
# =============================================================================
@dataclass
class Phase8TrainingConfig:
    """Phase 8 Training Configuration"""
    # Model Architecture (10B Scale Target: d_model=4096, n_layers=48)
    d_model: int = 4096
    n_layers: int = 48
    n_seq: int = 512
    num_heads: int = 32
    htt_rank: int = 16
    hyperbolic_window_size: int = 64
    
    # Phase 8 Specifics
    use_bk_hyperbolic: bool = True
    use_ar_ssm_fusion: bool = True
    
    # Compression (Critical for 10B)
    low_rank_ffn: bool = True
    low_rank_attention: bool = True
    low_rank_rank: int = 64
    use_bitnet: bool = True
    
    # Training
    batch_size: int = 1
    grad_accum_steps: int = 16
    epochs: int = 1
    learning_rate: float = 0.02
    min_lr: float = 1e-6
    weight_decay: float = 0.01
    warmup_steps: int = 500  # Reduced from 2000
    max_steps: Optional[int] = None
    
    # Optimizer (AdamW settings)
    optimizer_type: str = 'adamw'
    beta1: float = 0.9
    beta2: float = 0.95
    eps: float = 1e-8
    
    # Gradient - stricter clipping during warmup to prevent NaN
    grad_clip_warmup: float = 0.01  # Very strict during warmup (was 0.1)
    grad_clip_train: float = 1.0
    grad_skip_threshold: float = 10.0
    warmup_stability_steps: int = 100  # Extra-strict clipping for first N steps
    
    # EMA
    use_ema: bool = True
    ema_decay: float = 0.999
    
    # Regularization
    label_smoothing: float = 0.1
    dropout: float = 0.1
    
    # Optimization
    use_mixed_precision: bool = True
    use_gradient_checkpointing: bool = True
    use_triton_kernel: bool = True
    triton_kernel_version: str = 'fast'
    
    # Data
    data_limit: int = 100_000_000
    vocab_size: int = 50257
    
    # Logging
    log_interval: int = 10
    save_interval: int = 500
    eval_interval: int = 200
    save_dir: str = "checkpoints/phase8"
    
    # Device
    device: str = "auto"
    seed: int = 42
    
    # Runtime
    dry_run: bool = False
    dataset_path: str = "configs/dataset_mixing.yaml"
    resume_from: Optional[str] = None
    compile: bool = False
    
    # Moonshot Optimizations
    use_resonance_locked: bool = True  # #6: Skip updates when gradient SNR is low
    resonance_gns_threshold: float = 5.0  # Gradient Noise Scale threshold
    use_time_reversed: bool = False  # #10: Train on reversed sequences (DISABLED by default - 2x overhead)
    time_reversed_weight: float = 0.5  # Weight for reversed loss
    
    # #3 Eigenvalue Precomputation
    use_green_function_lut: bool = True
    green_function_lut_size: int = 1024
    
    # #7 Scattering-Aware Attention Pruning
    use_scattering_pruning: bool = True
    scattering_threshold: float = 0.1
    
    # #8 Hyperbolic MoE
    use_hyperbolic_moe: bool = False  # Model arch change, optional
    hmoe_num_experts: int = 8
    hmoe_top_k: int = 2
    
    # #9 Gradient Teleportation
    use_gradient_teleportation: bool = True
    teleport_strength: float = 0.1
    
    # #11 Holographic Compression
    use_holographic_kv_cache: bool = False  # Experimental, optional
    holographic_compression_ratio: float = 0.25
    
    # #12 Superposition Training 
    use_superposition_training: bool = False  # Heavy, optional
    superposition_particles: int = 5
    
    # Revolutionary Training Algorithms (7 algorithms)
    # Phase-based auto-scheduling: algorithms activate based on training progress
    # - Warmup (0-10%): OFF (focus on stability)
    # - Early (10-30%): holographic, closed_form only
    # - Mid (30-70%): + topological, zeta
    # - Late (70-100%): ALL algorithms enabled
    use_revolutionary_training: bool = True  # Master switch
    revolutionary_auto_schedule: bool = True  # Auto ON/OFF based on phase
    revolutionary_algorithms: str = "holographic,closed_form,topological,retrocausal,zeta,sheaf,diffractive"


def parse_args() -> Phase8TrainingConfig:
    parser = argparse.ArgumentParser(description="Phase 8 Training Script (Optimized)")
    
    # Model
    parser.add_argument("--d-model", type=int, default=4096)
    parser.add_argument("--n-layers", type=int, default=48)
    parser.add_argument("--n-seq", type=int, default=512)
    parser.add_argument("--num-heads", type=int, default=32)
    parser.add_argument("--low-rank-rank", type=int, default=64)
    
    # Flags
    parser.add_argument("--no-low-rank-ffn", action="store_false", dest="low_rank_ffn")
    parser.add_argument("--no-low-rank-attn", action="store_false", dest="low_rank_attention")
    parser.add_argument("--no-bitnet", action="store_false", dest="use_bitnet")
    parser.add_argument("--no-bk-hyperbolic", action="store_false", dest="use_bk_hyperbolic")
    parser.add_argument("--no-ar-ssm", action="store_false", dest="use_ar_ssm_fusion")
    parser.add_argument("--no-ema", action="store_false", dest="use_ema")
    
    # Training
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--grad-accum-steps", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=0.02)
    parser.add_argument("--min-lr", type=float, default=1e-6)
    parser.add_argument("--warmup-steps", type=int, default=500)
    parser.add_argument("--max-steps", type=int, default=None)
    parser.add_argument("--dry-run", action="store_true")
    
    # Optimizer
    parser.add_argument("--optimizer", type=str, default="adamw", choices=["adamw", "muon", "riemannian_muon", "bk_hyper_sgd"])
    parser.add_argument("--beta1", type=float, default=0.9)
    parser.add_argument("--beta2", type=float, default=0.95)
    parser.add_argument("--weight-decay", type=float, default=0.01)
    
    # Regularization
    parser.add_argument("--label-smoothing", type=float, default=0.1)
    parser.add_argument("--ema-decay", type=float, default=0.999)
    
    # Gradient
    parser.add_argument("--grad-clip-warmup", type=float, default=0.1)
    parser.add_argument("--grad-clip-train", type=float, default=1.0)
    parser.add_argument("--grad-skip-threshold", type=float, default=10.0)
    
    # Optimization
    parser.add_argument("--extreme-compression", action="store_true")
    parser.add_argument("--ultra-compression", action="store_true")
    parser.add_argument("--compile", action="store_true")
    
    # Config file support
    parser.add_argument("--config", type=str, default=None)
    parser.add_argument("--dataset", type=str, default=None)
    parser.add_argument("--resume-from", type=str, default=None)
    parser.add_argument("--save-dir", type=str, default=None)
    parser.add_argument("--save-interval", type=int, default=500)

    parser.set_defaults(
        low_rank_ffn=True, 
        low_rank_attention=True, 
        use_bitnet=True, 
        use_bk_hyperbolic=True, 
        use_ar_ssm_fusion=True,
        use_ema=True
    )
    
    args = parser.parse_args()

    # Load YAML config if provided
    yaml_config = {}
    if args.config:
        import yaml
        with open(args.config, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
    
    # Helper to get config value (CLI > YAML > default)
    def get_val(name, default):
        cli_val = getattr(args, name, None)
        yaml_val = yaml_config.get(name, yaml_config.get(name.replace('_', '-'), None))
        if cli_val is not None and cli_val != default:
            return cli_val
        if yaml_val is not None:
            return yaml_val
        return default if cli_val is None else cli_val
    
    # Extreme Compression Logic
    if args.extreme_compression:
        print("üöÄ Extreme Compression Enabled (Target: 8GB VRAM)")
        args.low_rank_rank = 16
    
    if args.ultra_compression:
        print("üåå Ultra Compression Enabled (Target: <3GB VRAM)")
        args.low_rank_rank = 8
    
    # Build config
    config = Phase8TrainingConfig(
        d_model=get_val('d_model', 4096),
        n_layers=get_val('n_layers', 48),
        n_seq=get_val('n_seq', 512),
        num_heads=get_val('num_heads', 32),
        low_rank_rank=get_val('low_rank_rank', 64),
        low_rank_ffn=get_val('low_rank_ffn', True),
        low_rank_attention=get_val('low_rank_attention', True),
        use_bitnet=get_val('use_bitnet', True),
        use_bk_hyperbolic=get_val('use_bk_hyperbolic', True),
        use_ar_ssm_fusion=get_val('use_ar_ssm_fusion', True),
        batch_size=get_val('batch_size', 1),
        grad_accum_steps=get_val('grad_accum_steps', get_val('gradient_accumulation_steps', 16)),
        epochs=get_val('epochs', 1),
        learning_rate=get_val('learning_rate', 0.02),
        min_lr=get_val('min_lr', 1e-6),
        warmup_steps=get_val('warmup_steps', 500),
        max_steps=get_val('max_steps', None),
        optimizer_type=args.optimizer,  # CLI always takes priority for optimizer
        beta1=get_val('beta1', 0.9),
        beta2=get_val('beta2', 0.95),
        eps=get_val('eps', 1e-8),
        weight_decay=get_val('weight_decay', 0.01),
        grad_clip_warmup=get_val('grad_clip_warmup', get_val('max_grad_norm', 0.1)),
        grad_clip_train=get_val('grad_clip_train', 1.0),
        grad_skip_threshold=get_val('grad_skip_threshold', 10.0),
        use_ema=get_val('use_ema', True),
        ema_decay=get_val('ema_decay', 0.999),
        label_smoothing=get_val('label_smoothing', 0.1),
        dropout=get_val('dropout', 0.1),
        use_mixed_precision=get_val('use_mixed_precision', True),
        use_gradient_checkpointing=get_val('use_gradient_checkpointing', True),
        use_triton_kernel=get_val('use_triton_kernel', True),
        vocab_size=get_val('vocab_size', 50257),
        save_interval=get_val('save_interval', 500),
        save_dir=get_val('save_dir', 'checkpoints/phase8'),
        dry_run=args.dry_run,
        dataset_path=args.dataset if args.dataset else get_val('dataset_path', 'configs/dataset_mixing.yaml'),
        resume_from=args.resume_from,
        compile=args.compile,
    )
    
    return config


def init_weights(model: nn.Module):
    """
    Initialize model weights with proper schemes.
    - Attention: Xavier
    - FFN: He (Kaiming)
    - LayerNorm: gamma=1, beta=0
    - Embedding: N(0, 0.02)
    """
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        
        if 'embedding' in name.lower() or 'embed' in name.lower():
            # Muon + Large Model (327M): ÈùûÂ∏∏„Å´‰øùÂÆàÁöÑ„Å™ÂàùÊúüÂåñ
            nn.init.normal_(param, mean=0.0, std=0.001)  # 0.02 ‚Üí 0.001 (BERT/GPTÊé®Â•®ÂÄ§)
            # Áµ∂ÂØæÂÄ§„Çí¬±0.01„Å´Âà∂ÈôêÔºàÂãæÈÖçÁàÜÁô∫Èò≤Ê≠¢Ôºâ
            with torch.no_grad():
                param.data.clamp_(-0.01, 0.01)
        elif 'layernorm' in name.lower() or 'layer_norm' in name.lower():
            if 'weight' in name or 'gamma' in name:
                nn.init.ones_(param)
            elif 'bias' in name or 'beta' in name:
                nn.init.zeros_(param)
        elif 'attn' in name.lower() or 'attention' in name.lower():
            if param.dim() >= 2:
                nn.init.xavier_uniform_(param)
        elif 'ffn' in name.lower() or 'mlp' in name.lower() or 'fc' in name.lower():
            if param.dim() >= 2:
                nn.init.kaiming_uniform_(param, a=math.sqrt(5))
        elif param.dim() >= 2:
            nn.init.xavier_uniform_(param)


def create_model(config: Phase8TrainingConfig, vocab_size: int, device: torch.device) -> Phase8IntegratedModel:
    model_config = Phase8Config(
        vocab_size=vocab_size,
        d_model=config.d_model,
        n_layers=config.n_layers,
        n_seq=config.n_seq,
        num_heads=config.num_heads,
        htt_rank=config.htt_rank,
        hyperbolic_window_size=config.hyperbolic_window_size,
        
        # Phase 8
        use_bk_hyperbolic=config.use_bk_hyperbolic,
        use_ar_ssm_fusion=config.use_ar_ssm_fusion,
        
        # Compression
        low_rank_ffn=config.low_rank_ffn,
        low_rank_attention=config.low_rank_attention,
        low_rank_rank=config.low_rank_rank,
        use_bitnet=config.use_bitnet,
        
        # Optimization
        use_gradient_checkpointing=config.use_gradient_checkpointing,
        use_mixed_precision=config.use_mixed_precision,
        use_triton_kernel=config.use_triton_kernel,
        triton_kernel_version=config.triton_kernel_version,
    )
    
    model = Phase8IntegratedModel(model_config)
    
    # Apply weight initialization
    # Use BK Isometry initialization if available (Phase 1: energy-preserving)
    if _BK_ISOMETRY_AVAILABLE:
        print("üß¨ Applying BK Isometry Initialization (energy-preserving)...")
        stats = apply_bk_isometry_init(model, base_gain=1.0, curvature=-1.0, verbose=False)
        print(f"   Unitary: {stats.get('unitary_count', 0)}, Hyperbolic: {stats.get('hyperbolic_count', 0)}, Euclidean: {stats.get('euclidean_count', 0)}")
    else:
        init_weights(model)
    
    # ========== Global Gradient Sanitization ==========
    # Register gradient hooks on ALL parameters to sanitize NaN/Inf
    # CRITICAL: Use small non-zero values (1e-6) instead of 0 to allow gradient flow
    def create_sanitize_hook(param_name):
        def sanitize_grad(grad):
            if grad is not None:
                # Replace NaN/Inf with small values (NOT zero - that kills learning)
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    grad = torch.nan_to_num(grad, nan=1e-6, posinf=1.0, neginf=-1.0)
                # CHANGED: ¬±10.0 ‚Üí ¬±1.0 for KPI compliance (grad_norm ‚â§ 10)
                # With 966 params: max grad_norm = sqrt(966 * 1^2) ‚âà 31
                return torch.clamp(grad, -1.0, 1.0)
            return grad
        return sanitize_grad
    
    num_hooks = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            param.register_hook(create_sanitize_hook(name))
            num_hooks += 1
    print(f"‚úî Gradient sanitization hooks applied to {num_hooks} parameters (NaN‚Üí1e-6, clamp ¬±10.0)")
    
    model = model.to(device)
    
    return model


def cleanup_old_checkpoints(save_dir: str, max_keep: int = 2):
    """
    Keep only the latest N checkpoints, delete older ones.
    
    Args:
        save_dir: Directory containing checkpoints
        max_keep: Maximum number of checkpoints to keep (default: 2)
    """
    import glob
    
    # Find all step_*.pt files
    pattern = os.path.join(save_dir, "step_*.pt")
    checkpoints = glob.glob(pattern)
    
    if len(checkpoints) <= max_keep:
        return  # Nothing to delete
    
    # Sort by step number (extract from filename)
    def get_step(path):
        try:
            basename = os.path.basename(path)
            return int(basename.replace("step_", "").replace(".pt", ""))
        except:
            return 0
    
    checkpoints.sort(key=get_step)
    
    # Delete oldest checkpoints
    to_delete = checkpoints[:-max_keep]
    for ckpt in to_delete:
        try:
            os.remove(ckpt)
            print(f"üóëÔ∏è Deleted old checkpoint: {os.path.basename(ckpt)}")
        except Exception as e:
            print(f"‚ö† Failed to delete {ckpt}: {e}")


def save_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler: CosineWarmupScheduler,
    scaler: torch.cuda.amp.GradScaler,
    ema: Optional[EMA],
    step: int,
    epoch: int,
    loss: float,
    config: Phase8TrainingConfig
):
    """Save complete checkpoint including all training state."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    checkpoint = {
        'step': step,
        'epoch': epoch,
        'loss': loss,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'config': asdict(config),
    }
    
    if ema is not None:
        checkpoint['ema_state_dict'] = ema.state_dict()
    
    torch.save(checkpoint, path)
    print(f"\nüíæ Checkpoint saved: {path}")


def load_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler: CosineWarmupScheduler,
    scaler: torch.cuda.amp.GradScaler,
    ema: Optional[EMA],
    device: torch.device
) -> Tuple[int, int, float]:
    """Load checkpoint and return (step, epoch, loss)."""
    print(f"Loading checkpoint from {path}...")
    
    checkpoint = torch.load(path, map_location=device)
    
    # Load model
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    # Load optimizer
    if 'optimizer_state_dict' in checkpoint:
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        except Exception as e:
            print(f"‚ö† Could not load optimizer state: {e}")
    
    # Load scheduler
    if 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # Load Scaler (PyTorch 2.2 uses cuda.amp)
    if 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    # Load EMA
    if ema is not None and 'ema_state_dict' in checkpoint:
        ema.load_state_dict(checkpoint['ema_state_dict'])
    
    step = checkpoint.get('step', 0)
    epoch = checkpoint.get('epoch', 0)
    loss = checkpoint.get('loss', 0.0)
    
    print(f"‚úî Checkpoint loaded: step={step}, epoch={epoch}, loss={loss:.4f}")
    
    return step, epoch, loss


def save_training_log(log: Dict, path: str):
    """Save training log to JSON file."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(log, f, indent=2, ensure_ascii=False, default=str)


def train_phase8():
    config = parse_args()
    
    # Set precision and CUDA options
    torch.set_float32_matmul_precision('high')
    torch.backends.cudnn.benchmark = True
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Phase 8 Training (10B Scale - Optimized) on {device}")

    # Initialize Triton Mode
    if device.type == "cuda" and config.use_triton_kernel:
        try:
            from src.models.bk_core import set_triton_mode
            set_triton_mode(True)
            print("‚úî Triton Mode Enabled")
        except ImportError:
            print("‚ö† Triton mode not available")

    # Log VRAM
    if torch.cuda.is_available():
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"Detected VRAM: {vram:.2f} GB")

    # Print config summary
    print(f"Config: d_model={config.d_model}, n_layers={config.n_layers}")
    print(f"Compression: LowRankFFN={config.low_rank_ffn}, LowRankAttn={config.low_rank_attention}, BitNet={config.use_bitnet}")
    print(f"Rank: {config.low_rank_rank}, Grad Accum: {config.grad_accum_steps}")
    print(f"Optimizer: {config.optimizer_type.upper()}, LR: {config.learning_rate}, Warmup: {config.warmup_steps}")
    print(f"EMA: {config.use_ema}, Label Smoothing: {config.label_smoothing}")
    
    # Dataset - Load FIRST to get actual vocab_size
    steps_per_epoch = 100  # Default for dry run
    actual_vocab_size = config.vocab_size
    dataset = None
    
    # CRITICAL FIX: Override warmup_steps for dry-run BEFORE optimizer creation
    # Without this, warmup_steps=2000 means lr‚âà0 for the entire dry-run
    if config.dry_run:
        dry_run_steps = 200
        dry_run_optimizer_steps = dry_run_steps // config.grad_accum_steps
        original_warmup = config.warmup_steps
        config.warmup_steps = min(3, max(1, dry_run_optimizer_steps // 4))  # Very short warmup
        print(f"‚ö° Dry-run warmup adjustment: {original_warmup} ‚Üí {config.warmup_steps} optimizer steps")
        print("Dry Run: Using dummy data")
    else:
        print(f"Loading dataset from {config.dataset_path}...")
        try:
            dataset, vocab, steps_per_epoch = get_mixed_data_loader(
                config.dataset_path,
                batch_size=config.batch_size,
                n_seq=config.n_seq,
                total_tokens=config.data_limit,
                seed=config.seed,
                vocab_size=config.vocab_size
            )
            # Update vocab_size if dataset has larger tokens
            # vocab is a dict with 'stoi', 'itos', 'vocab_size' keys
            actual_vocab_size = vocab['vocab_size'] if isinstance(vocab, dict) else vocab
            if actual_vocab_size != config.vocab_size:
                print(f"[Model] Using dataset vocab_size: {actual_vocab_size} (config was {config.vocab_size})")
                config.vocab_size = actual_vocab_size
            print(f"Dataset loaded. Steps per epoch: {steps_per_epoch}")
        except Exception as e:
            print(f"‚ö† Failed to load dataset: {e}")
            sys.exit(1)
    
    # Create model with actual vocab_size
    model = create_model(config, config.vocab_size, device)
    
    # Apply torch.compile() for additional speedup
    if config.compile and hasattr(torch, 'compile'):
        try:
            print("üîß Applying torch.compile()...")
            model = torch.compile(model, mode="reduce-overhead", fullgraph=False)
            print("‚úî torch.compile() applied successfully")
        except Exception as e:
            print(f"‚ö† torch.compile() failed (continuing without): {e}")
    
    # Parameter Count
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total Parameters: {total_params:,}")
    print(f"Trainable Parameters: {trainable_params:,}")
    
    # Optimizer - BK-HyperSGD ONLY (ResNet-BKÂ∞ÇÁî®)
    # AdamW, Muon, RiemannianMuon „ÅØÂâäÈô§„Åï„Çå„Åæ„Åó„Åü
    if not _BK_HYPER_SGD_AVAILABLE:
        raise RuntimeError("BK-HyperSGD is required but not available! Check src/optimizers/bk_hyper_sgd.py")
    
    print("üß¨ Using BK-HyperSGD Optimizer (ResNet-BK Specialized - ÂîØ‰∏Ä„ÅÆ„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂)")
    print("   - Cayley retraction for unitary layers (v_proj, output_proj)")
    print("   - Lorentz exp map for hyperbolic layers")
    print("   - Symplectic integration for Hamiltonian structure")
    
    # Get parameter groups with geometry-aware learning rates
    param_groups = get_bk_parameter_groups(
        model,
        base_lr=config.learning_rate,
        unitary_lr_scale=0.1,      # BK-Core needs smaller LR
        hyperbolic_lr_scale=0.5,   # Hyperbolic moderate LR
        symplectic_lr_scale=0.3,   # Symplectic balanced LR
    )
    
    optimizer = BKHyperSGD(
        param_groups,
        lr=config.learning_rate,
        momentum=0.9,
        curvature=-1.0,
        unitarity_strength=0.1,
        use_cayley=True,
        use_lorentz=True,
        max_grad_norm=config.grad_clip_train,
        weight_decay=config.weight_decay,
    )
    
    # Set parameter names and re-classify
    optimizer.set_param_names(model)
    
    # Print parameter group stats
    stats = optimizer.get_statistics()
    print(f"   Parameter groups: {stats['param_type_counts']}")
    
    # Scaler for mixed precision
    # NOTE: Disable scaler for BK-HyperSGD in dry run to avoid unscale issues
    use_scaler = config.use_mixed_precision and not config.dry_run
    scaler = torch.cuda.amp.GradScaler(enabled=use_scaler)
    if not use_scaler and config.dry_run:
        print("   ‚ö† GradScaler disabled during dry run for BK-HyperSGD compatibility")
    
    # EMA
    ema = EMA(model, decay=config.ema_decay) if config.use_ema else None
    if ema:
        print(f"‚úî EMA Enabled (decay={config.ema_decay})")
    
    # Calculate total steps
    total_steps = config.max_steps if config.max_steps else steps_per_epoch * config.epochs
    
    # LR Scheduler
    scheduler = CosineWarmupScheduler(
        optimizer=optimizer,
        warmup_steps=config.warmup_steps,
        total_steps=total_steps,
        peak_lr=config.learning_rate,
        min_lr=config.min_lr
    )
    print(f"‚úî LR Scheduler: Warmup {config.warmup_steps} steps, Cosine decay to {config.min_lr}")
    
    # Resume from checkpoint if specified
    start_step = 0
    start_epoch = 0
    if config.resume_from and os.path.exists(config.resume_from):
        start_step, start_epoch, _ = load_checkpoint(
            config.resume_from, model, optimizer, scheduler, scaler, ema, device
        )
    
    # Training state
    model.train()
    step = start_step
    optimizer_step = 0
    total_loss = 0.0
    skip_count = 0
    
    # Resonance-Adaptive Curvature Optimizer (Phase 8 optimization)
    resonance_curvature = None
    stability_monitor = None
    if _RESONANCE_AVAILABLE:
        try:
            resonance_curvature = ResonanceAdaptiveCurvature(
                model=model,
                initial_curvature=1.0,
                min_curvature=0.1,
                max_curvature=2.0,
                resonance_threshold=0.5,
                adjustment_rate=0.01,
            )
            stability_monitor = StabilityMonitor(
                window_size=100,
                nan_threshold=5,
                exploding_grad_threshold=100.0,
            )
            print("‚úî Resonance-Adaptive Curvature & Stability Monitor Enabled")
        except Exception as e:
            print(f"‚ö† Resonance optimizers not available: {e}")
    
    # Gradient Teleportation (#9) - Scheduler controlled
    gradient_teleporter = None
    teleporter_hooks_active = False  # Track hook state for scheduler
    if _GRADIENT_TELEPORT_AVAILABLE and config.use_gradient_teleportation:
        try:
            gradient_teleporter = create_gradient_teleporter(
                model=model,
                teleport_strength=config.teleport_strength,
                use_dyson=True,
            )
            # Don't register hooks yet - scheduler will enable after warmup
            print(f"‚úî Gradient Teleportation Prepared (scheduler-controlled, strength={config.teleport_strength})")
        except Exception as e:
            print(f"‚ö† Gradient Teleportation not available: {e}")
    
    # Revolutionary Training (7 algorithms integration)
    revolutionary_trainer = None
    if _REVOLUTIONARY_AVAILABLE and config.use_revolutionary_training:
        try:
            # Parse enabled algorithms from config
            enabled_algos = config.revolutionary_algorithms.split(',')
            rev_config = RevolutionaryConfig(
                use_holographic='holographic' in enabled_algos,
                use_closed_form='closed_form' in enabled_algos,
                use_topological='topological' in enabled_algos,
                use_retrocausal='retrocausal' in enabled_algos,
                use_zeta='zeta' in enabled_algos,
                use_sheaf='sheaf' in enabled_algos,
                use_diffractive='diffractive' in enabled_algos,
                learning_rate=config.learning_rate,
                log_interval=config.log_interval,
            )
            revolutionary_trainer = RevolutionaryTrainer(model, rev_config, device)
            # Start in warmup mode - weight modifications disabled until warmup completes
            revolutionary_trainer.set_warmup_mode(True)
            if getattr(config, 'revolutionary_auto_schedule', True):
                print(f"‚úî Revolutionary Training Enabled (Phase-based Auto-Schedule)")
                print(f"  ‚îî‚îÄ Warmup(0-10%): OFF | Early(10-30%): 1/5 | Mid(30-70%): 1/3 | Late(70-100%): 1/2")
            else:
                print(f"‚úî Revolutionary Training Enabled: {config.revolutionary_algorithms}")
        except Exception as e:
            print(f"‚ö† Revolutionary Training not available: {e}")
    
    # Gradient Sanitization (DISABLED - was limiting gradients too much)
    gradient_sanitizer = None
    # if _GRADIENT_SANITIZER_AVAILABLE:
    #     try:
    #         gradient_sanitizer = create_gradient_sanitizer(
    #             model=model,
    #             use_spectral_norm=True,
    #             use_outlier_detection=True,
    #             use_momentum_smoothing=True,
    #             emergency_grad_max=0.5,
    #         )
    #         print(f"‚úî Gradient Sanitization Enabled (Moonshot #13)")
    #     except Exception as e:
    #         print(f"‚ö† Gradient Sanitization not available: {e}")
    
    # Stability Suite (DISABLED - was limiting gradients too much)
    stability_manager = None
    # if _STABILITY_SUITE_AVAILABLE:
    #     try:
    #         stability_manager = create_stability_manager(
    #             model=model,
    #             aggressive=True,
    #         )
    #         print(f"‚úî Stability Suite Enabled (Moonshot #14)")
    #         print(f"  ‚îî‚îÄ Backward Hooks | Layerwise Scaling | Loss Smoothing | Adaptive Precision")
    #     except Exception as e:
    #         print(f"‚ö† Stability Suite not available: {e}")
    
    # JSON Log
    training_log = {
        'config': asdict(config),
        'start_time': datetime.now().isoformat(),
        'total_params': total_params,
        'steps': [],
    }
    
    # Dry run mock dataset
    if config.dry_run:
        # Reduce grad_accum_steps for faster iteration (more optimizer steps)
        original_grad_accum = config.grad_accum_steps
        config.grad_accum_steps = min(4, original_grad_accum)  # Max 4 for dry-run (faster)
        print(f"‚ö° Dry-run: grad_accum_steps {original_grad_accum} ‚Üí {config.grad_accum_steps}")
        
        # CRITICAL: Disable complex features that cause NaN during dry-run
        print("‚ö° Dry-run: Disabling complex features that cause NaN...")
        config.use_time_reversed = False
        config.use_resonance_locked = False
        config.use_gradient_teleportation = False
        config.use_revolutionary_training = False
        config.use_superposition_training = False
        print("   - Time-Reversed Training: OFF")
        print("   - Resonance-Locked Training: OFF")
        print("   - Revolutionary Training: OFF")
        
        # Run 50 steps for quick KPI verification
        steps_to_run = 50
        print(f"Dry Run: Running {steps_to_run} steps (grad_accum={config.grad_accum_steps})...")
        
        class MockDataset:
            def iter_epoch(self, epoch):
                for _ in range(steps_to_run):
                    x = torch.randint(0, config.vocab_size, (config.batch_size, config.n_seq))
                    y = torch.randint(0, config.vocab_size, (config.batch_size * config.n_seq,))
                    yield x, y
        
        dataset = MockDataset()
        steps_per_epoch = steps_to_run
        total_steps = steps_to_run
    
    # Progress bar
    pbar = tqdm(
        total=total_steps - start_step,
        initial=0,
        disable=not TQDM_AVAILABLE,
        desc="Training"
    )
    
    # Training loop
    for epoch in range(start_epoch, config.epochs):
        for x, y in dataset.iter_epoch(epoch):
            step += 1
            
            if step <= start_step:
                continue
            
            x, y = x.to(device), y.to(device)
            
            # Initialize gradient metrics (will be updated during optimizer step)
            grad_norm_raw = 0.0
            grad_norm = 0.0
            
            # Forward pass with mixed precision
            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=config.use_mixed_precision):
                # Only collect diagnostics when logging (reduces overhead ~10%)
                collect_diag = (step % config.log_interval == 0)
                logits, diagnostics = model(x, return_diagnostics=collect_diag)
                logits = logits.view(-1, config.vocab_size)
                
                # Check for NaN/Inf in logits
                if torch.isnan(logits).any() or torch.isinf(logits).any():
                    nan_count = torch.isnan(logits).sum().item()
                    inf_count = torch.isinf(logits).sum().item()
                    # For dry-run: continue with clamped logits instead of skipping
                    if config.dry_run:
                        logits = torch.nan_to_num(logits, nan=0.0, posinf=100.0, neginf=-100.0)
                        print(f"‚ö† [DRY-RUN] Clamped NaN/Inf logits at step {step} (NaN: {nan_count}, Inf: {inf_count})")
                    else:
                        print(f"üö® NaN/Inf in logits at step {step}! (NaN: {nan_count}, Inf: {inf_count})")
                        optimizer.zero_grad()
                        skip_count += 1
                        total_loss = 0.0
                        pbar.update(1)
                        continue
                
                # Loss with label smoothing (forward direction)
                loss_forward = F.cross_entropy(logits, y, label_smoothing=config.label_smoothing)
                
                # Time-Reversed Training (#10 Moonshot)
                # Train on reversed sequences for bi-directional consistency
                if config.use_time_reversed:
                    x_rev = x.flip(1)  # Reverse sequence
                    y_rev = y.view(x.shape[0], -1).flip(1).view(-1)  # Reverse targets
                    logits_rev, _ = model(x_rev, return_diagnostics=False)
                    logits_rev = logits_rev.view(-1, config.vocab_size)
                    loss_backward = F.cross_entropy(logits_rev, y_rev, label_smoothing=config.label_smoothing)
                    loss = (1 - config.time_reversed_weight) * loss_forward + config.time_reversed_weight * loss_backward
                else:
                    loss = loss_forward
                
                loss = loss / config.grad_accum_steps
            
            # Apply loss smoothing from Stability Manager (prevents sharp spikes)
            if stability_manager is not None:
                loss = stability_manager.process_loss(loss)
            
            # Check for NaN/Inf loss
            if torch.isnan(loss) or torch.isinf(loss):
                # For dry-run: use small valid loss instead of skipping
                if config.dry_run:
                    loss = torch.tensor(10.0, device=loss.device, requires_grad=True)
                    print(f"‚ö† [DRY-RUN] Replaced NaN/Inf loss with 10.0 at step {step}")
                else:
                    print(f"üö® NaN/Inf loss at step {step}! Value: {loss.item() if not torch.isnan(loss) else 'NaN'}")
                    optimizer.zero_grad()
                    skip_count += 1
                    total_loss = 0.0
                    pbar.update(1)
                    continue
            
            # Backward (hooks from Stability Manager will catch NaN gradients here)
            # For dry-run: skip scaler to avoid GradScaler state issues
            if config.dry_run:
                loss.backward()
            else:
                scaler.scale(loss).backward()
            total_loss += loss.item() * config.grad_accum_steps
            
            # Optimizer step (every grad_accum_steps)
            if step % config.grad_accum_steps == 0:
                optimizer_step += 1
                
                # Unscale gradients (skip for dry-run to avoid state corruption)
                if not config.dry_run:
                    scaler.unscale_(optimizer)
                
                # NOTE: Per-parameter gradient clipping REMOVED - was killing gradient flow
                # The Stability Suite backward hooks (¬±10.0) handle NaN prevention now
                
                # === Gradient Sanitization (Moonshot #13) ===
                # This replaces manual NaN checking with comprehensive gradient cleaning
                nan_grad_count = 0
                if gradient_sanitizer is not None:
                    sanitize_stats = gradient_sanitizer.sanitize_gradients()
                    nan_grad_count = sanitize_stats.get('nan_fixed', 0) + sanitize_stats.get('inf_fixed', 0)
                    
                    if nan_grad_count > 0 and not hasattr(train_phase8, '_nan_debug_printed'):
                        train_phase8._nan_debug_printed = True
                        print(f"‚ö† Gradient Sanitizer fixed {nan_grad_count} NaN/Inf values at step {step}")
                        print(f"  Outliers removed: {sanitize_stats.get('outliers_removed', 0)}")
                        print(f"  Emergency recoveries: {sanitize_stats.get('emergency_recovery', 0)}")
                else:
                    # Fallback: Manual NaN/Inf check (if sanitizer not available)
                    nan_layers = []
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                                nan_grad_count += 1
                                if len(nan_layers) < 5:
                                    nan_layers.append(name)
                                param.grad.data = torch.nan_to_num(param.grad.data, nan=0.0, posinf=0.0, neginf=0.0)
                    
                    if nan_grad_count > 0 and not hasattr(train_phase8, '_nan_debug_printed'):
                        train_phase8._nan_debug_printed = True
                        print(f"‚ö† NaN/Inf in {nan_grad_count} parameter gradients at step {step}")
                        print(f"  First problematic layers: {nan_layers}")
                    elif nan_grad_count > 0:
                        # Subsequent NaN warnings (shorter message)
                        pass  # Silent after first warning
                
                # ===== Gradient Norm Computation \u0026 Clipping (Muon Optimized) =====
                
                # === PRE-CLIPPING FOR MUON (DISABLED - causing loss stagnation) ===
                # NOTE: Muon's internal stabilization handles gradient control.
                # Keeping external clipping minimal to allow gradient flow.
                # The main clip_grad_norm_ at L1101 provides sufficient safety.
                
                # Compute raw gradient norm (now reflects pre-clipped values for Muon)
                with torch.no_grad():
                    total_norm_sq = sum(p.grad.pow(2).sum() for p in model.parameters() if p.grad is not None)
                    grad_norm_raw = torch.sqrt(total_norm_sq).item()
                
                # Handle NaN/Inf in raw norm
                if math.isnan(grad_norm_raw) or math.isinf(grad_norm_raw):
                    grad_norm_raw = 0.0
                    grad_norm = 0.0
                else:
                    # === GLOBAL GRADIENT NORM CLIPPING (KPI Target: ‚â§ 10.0) ===
                    # Per-element clamp doesn't bound total norm (large tensors still explode)
                    # This normalizes ALL gradients so L2 norm ‚â§ max_norm
                    max_grad_norm = 5.0  # Target: avg ‚â§ 5.0, max ‚â§ 10.0
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        model.parameters(), 
                        max_norm=max_grad_norm,
                        error_if_nonfinite=False  # Don't error on NaN (handled by hooks)
                    ).item()
                    
                    # Fallback if clip_grad_norm_ fails
                    if math.isnan(grad_norm) or math.isinf(grad_norm):
                        grad_norm = grad_norm_raw
                
                # Failsafe: Skip if raw norm is extremely large (gradient explosion)
                # NOTE: For dry-run, we disable this entirely to test if model can learn
                effective_skip_threshold = config.grad_skip_threshold
                if config.dry_run:
                    # Dry-run: disable skip entirely to see actual training behavior
                    effective_skip_threshold = float('inf')
                elif config.optimizer_type in ('muon', 'riemannian_muon'):
                    # Muon/RiemannianMuonBit use orthogonalization which makes raw grad_norm misleading
                    effective_skip_threshold = 10000.0
                
                if grad_norm_raw > effective_skip_threshold:
                    # ÂàùÊúü„Çπ„ÉÜ„ÉÉ„Éó„Åß„Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±„ÇíÂá∫Âäõ
                    if step <= 10:
                        print(f"  üîç Step {step} DEBUG: grad_norm_raw={grad_norm_raw:.2f}, threshold={effective_skip_threshold}")
                        print(f"     Largest gradient layers:")
                        layer_grads = []
                        for name, param in model.named_parameters():
                            if param.grad is not None:
                                g_norm = param.grad.norm().item()
                                if g_norm > 1.0:  # Only show significant gradients
                                    layer_grads.append((name, g_norm))
                        # Sort and show top 5
                        layer_grads.sort(key=lambda x: -x[1])
                        for name, g_norm in layer_grads[:5]:
                            print(f"       {name}: {g_norm:.2f}")
                    
                    print(f"‚ö† Grad norm {grad_norm_raw:.2f} > {effective_skip_threshold}, skipping step")
                    optimizer.zero_grad()
                    if not config.dry_run:
                        scaler.update()  # Reset scaler state
                    skip_count += 1
                    total_loss = 0.0
                    pbar.update(1)
                    continue
                
                # Resonance-Locked Training (#6 Moonshot)
                # Skip updates when gradient SNR is low (high noise)
                resonance_skip = False
                if config.use_resonance_locked and step > config.warmup_steps:
                    # Approximate Gradient Noise Scale (GNS)
                    # GNS ‚âà grad_variance / grad_mean¬≤ 
                    # High GNS = noisy gradient, skip update
                    grads = [p.grad for p in model.parameters() if p.grad is not None]
                    if grads:
                        all_grads = torch.cat([g.flatten() for g in grads])
                        grad_mean = all_grads.mean().abs()
                        grad_var = all_grads.var()
                        gns = (grad_var / (grad_mean ** 2 + 1e-8)).item()
                        
                        if gns > config.resonance_gns_threshold:
                            # High noise, skip update (resonance not achieved)
                            resonance_skip = True
                            skip_count += 1
                            if step % 50 == 0:
                                print(f"üîí Resonance skip: GNS={gns:.2f} > {config.resonance_gns_threshold}")
                
                if resonance_skip:
                    optimizer.zero_grad()
                    total_loss = 0.0
                    pbar.update(1)
                    continue
                
                # NOTE: Redundant 2nd gradient clipping REMOVED (was causing loss stagnation)
                # The clip_grad_norm_ at L1101-1104 already handles clipping.
                
                # Optimizer step
                # For dry-run: call optimizer.step() directly to ensure updates happen
                # GradScaler may skip updates if it detects inf, causing loss stagnation
                if config.dry_run:
                    optimizer.step()
                else:
                    scaler.step(optimizer)
                    scaler.update()
                optimizer.zero_grad()
                
                # LR scheduler step
                scheduler.step()
                
                # EMA update
                if ema is not None:
                    ema.update()
                
                # === State-Based Scheduler ===
                # Enable advanced features only when training is stable
                # Criteria: stable_steps consecutive steps with healthy metrics
                
                # Compute avg_loss for stability check
                current_avg_loss = total_loss / config.grad_accum_steps
                
                # Check current state stability
                is_stable = (
                    not math.isnan(grad_norm) and 
                    grad_norm <= 5.0 and            # Healthy grad norm
                    grad_norm > 0.01 and            # Not dead gradients
                    current_avg_loss < 20.0 and     # Loss not exploded
                    nan_grad_count == 0             # No NaN/Inf in gradients this step
                )
                
                # Track consecutive stable steps
                if is_stable:
                    stable_steps = getattr(train_phase8, '_stable_steps', 0) + 1
                else:
                    stable_steps = 0
                train_phase8._stable_steps = stable_steps
                
                # Thresholds for enabling features (in stable steps)
                STABLE_FOR_REVOLUTIONARY = 50   # Need 50 stable steps
                STABLE_FOR_TELEPORTATION = 100  # Need 100 stable steps
                
                # Revolutionary Training - State-based control
                if revolutionary_trainer is not None:
                    try:
                        if getattr(config, 'revolutionary_auto_schedule', True):
                            if stable_steps < STABLE_FOR_REVOLUTIONARY:
                                # Not stable enough: keep warmup mode
                                revolutionary_trainer.set_warmup_mode(True)
                                should_apply = False
                            elif stable_steps < STABLE_FOR_REVOLUTIONARY * 2:
                                # Stable but cautious: apply every 5 steps
                                revolutionary_trainer.set_warmup_mode(False)
                                should_apply = (step % 5 == 0)
                            elif stable_steps < STABLE_FOR_REVOLUTIONARY * 4:
                                # Very stable: apply every 3 steps
                                should_apply = (step % 3 == 0)
                            else:
                                # Maximum stability: apply every 2 steps
                                should_apply = (step % 2 == 0)
                        else:
                            revolutionary_trainer.set_warmup_mode(False)
                            should_apply = True
                        
                        if should_apply:
                            loss_fn = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)
                            rev_loss, rev_metrics = revolutionary_trainer.train_step(x, y, loss_fn)
                            algo_used = rev_metrics.get('algorithm', 'unknown')
                            if step % config.log_interval == 0:
                                stability_level = "üî¥ warmup" if stable_steps < STABLE_FOR_REVOLUTIONARY else "üü° cautious" if stable_steps < STABLE_FOR_REVOLUTIONARY * 2 else "üü¢ stable" if stable_steps < STABLE_FOR_REVOLUTIONARY * 4 else "üåü optimal"
                                print(f"  üîÑ Revolutionary [{stability_level}]: {algo_used}")
                    except Exception as e:
                        if step % 100 == 0:
                            print(f"  ‚ö† Revolutionary step skipped: {e}")
                
                # Gradient Teleportation - State-based control
                if gradient_teleporter is not None:
                    if stable_steps < STABLE_FOR_TELEPORTATION:
                        # Not stable enough: disable teleportation
                        if teleporter_hooks_active:
                            gradient_teleporter.remove_hooks()
                            teleporter_hooks_active = False
                    else:
                        # Stable: enable teleportation
                        if not teleporter_hooks_active:
                            gradient_teleporter.register_hooks()
                            teleporter_hooks_active = True
                            print(f"  ‚ö° Gradient Teleportation activated (stable_steps={stable_steps})")


                # Resonance-Adaptive Curvature step (Phase 8 optimization)
                if resonance_curvature is not None and 'phase8' in (diagnostics or {}):
                    phase8_diag = diagnostics.get('phase8', {})
                    if 'G_ii_mean' in phase8_diag:
                        # Create a dummy G_ii tensor from diagnostics
                        g_ii_val = phase8_diag.get('G_ii_mean', 0.0)
                        g_ii_tensor = torch.tensor([g_ii_val], device=device, dtype=torch.complex64)
                        res_diag = resonance_curvature.step(g_ii_tensor)
                
                # Compute metrics FIRST (before using avg_loss)
                avg_loss = total_loss / config.grad_accum_steps
                ppl = math.exp(min(avg_loss, 20.0))
                
                # Stability Monitor update (now avg_loss is defined)
                had_nan = False
                if stability_monitor is not None:
                    status = stability_monitor.update(
                        loss=avg_loss,
                        grad_norm=grad_norm,
                        had_nan=had_nan
                    )
                    if status.get('warning'):
                        print(f"‚ö† Stability: {status['warning']}")
                
                # Stability Manager step (Moonshot #14)
                if stability_manager is not None:
                    stability_manager.step()
                
                current_lr = scheduler.get_last_lr()
                
                # Update progress bar (shorter description to show ETA)
                pbar.set_description(f"E{epoch}")
                pbar.set_postfix({
                    'loss': f'{avg_loss:.3f}',
                    'ppl': f'{ppl:.0f}',
                    'lr': f'{current_lr:.1e}',
                    'gN': f'{grad_norm:.2f}',  # Clipped norm
                    'gR': f'{grad_norm_raw:.2f}',  # Raw norm
                })
                
                # Log step
                step_log = {
                    'step': step,
                    'optimizer_step': optimizer_step,
                    'epoch': epoch,
                    'loss': avg_loss,
                    'ppl': ppl,
                    'lr': current_lr,
                    'grad_norm': grad_norm,  # Clipped
                    'grad_norm_raw': grad_norm_raw,  # Before clipping
                    'grad_clip': config.grad_clip_train if hasattr(config, 'grad_clip_train') else None,
                }
                
                # Add Muon-specific metrics (if using Muon optimizer)
                if config.optimizer_type == 'muon' and hasattr(optimizer, 'get_muon_metrics'):
                    muon_metrics = optimizer.get_muon_metrics()
                    if muon_metrics.get('stabilization_enabled', False):
                        step_log['muon_phase'] = muon_metrics.get('phase', 'unknown')
                        step_log['muon_nan_count'] = muon_metrics.get('ortho_nan_count', 0)
                        # Log detailed Muon info periodically
                        if step % config.log_interval == 0 and 'avg_grad_norm' in muon_metrics:
                            print(f"  üîß Muon [{muon_metrics.get('phase', 'N/A')}]: "
                                  f"AvgGrad={muon_metrics.get('avg_grad_norm', 0):.3f}, "
                                  f"NaN={muon_metrics.get('total_nan_count', 0)}")
                
                # Add diagnostics (only if collected)
                if diagnostics is not None:
                    for k, v in diagnostics.items():
                        if isinstance(v, torch.Tensor):
                            step_log[k] = v.item() if v.numel() == 1 else v.mean().item()
                        elif isinstance(v, (int, float)):
                            step_log[k] = v
                
                training_log['steps'].append(step_log)
                
                # Save checkpoint at save_interval
                if optimizer_step > 0 and optimizer_step % (config.save_interval // config.grad_accum_steps) == 0:
                    ckpt_path = os.path.join(config.save_dir, f"step_{step}.pt")
                    save_checkpoint(
                        ckpt_path, model, optimizer, scheduler, scaler, ema,
                        step, epoch, avg_loss, config
                    )
                    
                    # Keep only latest 2 checkpoints
                    cleanup_old_checkpoints(config.save_dir, max_keep=2)
                    
                    # Also save training log
                    log_path = os.path.join(config.save_dir, "training_log.json")
                    training_log['last_update'] = datetime.now().isoformat()
                    save_training_log(training_log, log_path)
                
                total_loss = 0.0
            
            pbar.update(1)
            
            if config.max_steps and step >= config.max_steps:
                break
        
        if config.max_steps and step >= config.max_steps:
            break
    
    pbar.close()
    print("\n‚úÖ Training Complete!")
    print(f"Total steps: {step}, Optimizer steps: {optimizer_step}, Skipped: {skip_count}")
    
    # Dry-run stability summary
    if config.dry_run:
        print("\n" + "=" * 50)
        print("üß™ DRY RUN STABILITY SUMMARY")
        print("=" * 50)
        
        # Collect grad norms from training log
        grad_norms = [s.get('grad_norm', 0) for s in training_log['steps'] if s.get('grad_norm') is not None]
        losses = [s.get('loss', 0) for s in training_log['steps'] if s.get('loss') is not None]
        
        # Count NaN/Inf warnings (approximate from skip_count)
        nan_count = skip_count
        
        # Compute grad norm stats
        if grad_norms:
            grad_min = min(grad_norms)
            grad_max = max(grad_norms)
            grad_avg = sum(grad_norms) / len(grad_norms)
        else:
            grad_min = grad_max = grad_avg = 0
        
        # Check criteria (relaxed thresholds)
        nan_ok = nan_count == 0
        grad_ok = grad_max <= 10.0 and grad_avg <= 5.0  # Relaxed from avg<=2.0
        loss_ok = len(losses) >= 2 and losses[-1] < losses[0] * 1.1  # Loss not exploding
        
        # Check for NaN warnings in gradients (separate from skipped steps)
        has_nan_warnings = hasattr(train_phase8, '_nan_debug_printed')
        
        print(f"\nüìä Results:")
        print(f"   NaN/Inf skipped steps: {nan_count} {'‚úÖ' if nan_ok else '‚ùå'}")
        if has_nan_warnings:
            print(f"   NaN/Inf in gradients: ‚ö†Ô∏è  detected (zeroed out - training continues)")
        print(f"   Grad norm range: {grad_min:.3f} ~ {grad_max:.3f} (avg: {grad_avg:.3f}) {'‚úÖ' if grad_ok else '‚ùå'}")
        if losses:
            print(f"   Loss: {losses[0]:.3f} ‚Üí {losses[-1]:.3f} {'‚úÖ' if loss_ok else '‚ùå'}")
        
        print("\n" + "-" * 50)
        if nan_ok and grad_ok and not has_nan_warnings:
            print("üéâ STABILITY CHECK PASSED!")
            print("   Safe to run: make train-japanese")
        elif nan_ok and grad_ok:
            print("üü° STABILITY CHECK PASSED (with warnings)")
            print("   NaN/Inf in gradients detected but zeroed out")
            print("   Training can proceed but investigate source")
        else:
            print("‚ö†Ô∏è  STABILITY CHECK FAILED")
            if not nan_ok:
                print("   ‚Üí NaN/Inf detected - check model initialization")
            if not grad_ok:
                print("   ‚Üí Grad norm too high - reduce learning rate")
        print("=" * 50)
    else:
        # Save final checkpoint (only for real training)
        final_path = os.path.join(config.save_dir, "phase8_10b_final.pt")
        save_checkpoint(
            final_path, model, optimizer, scheduler, scaler, ema,
            step, epoch, avg_loss if 'avg_loss' in dir() else 0.0, config
        )
    
    # Save final training log
    training_log['end_time'] = datetime.now().isoformat()
    training_log['final_step'] = step
    training_log['final_loss'] = training_log['steps'][-1]['loss'] if training_log['steps'] else None
    log_path = os.path.join(config.save_dir, "training_log.json")
    save_training_log(training_log, log_path)
    print(f"üìù Training log saved to {log_path}")


if __name__ == "__main__":
    train_phase8()
