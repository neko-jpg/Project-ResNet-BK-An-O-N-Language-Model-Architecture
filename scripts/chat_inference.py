#!/usr/bin/env python3
"""
MUSE Chat AI - Phase 8 å¯¾å¿œç‰ˆ
==============================
è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒãƒ£ãƒƒãƒˆã§ãã¾ã™ï¼æ—¥æœ¬èªãƒ»è‹±èªä¸¡å¯¾å¿œã€‚

Usage:
    make chat                    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ãƒãƒ£ãƒƒãƒˆ
    make chat CHECKPOINT=path    # æŒ‡å®šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    python scripts/chat_inference.py --checkpoint checkpoints/phase8_10b_japanese/best.pt
"""

import argparse
import os
import sys
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, List, Dict

sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn.functional as F

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.prompt import Prompt
    from rich.table import Table
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

console = Console() if RICH_AVAILABLE else None


@dataclass
class ChatConfig:
    """ãƒãƒ£ãƒƒãƒˆè¨­å®š"""
    temperature: float = 0.8
    top_k: int = 50
    top_p: float = 0.9
    max_new_tokens: int = 256
    repetition_penalty: float = 1.1


def load_model_phase8(checkpoint_path: str, device: str = "cuda"):
    """Phase 8 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    from src.models.resnet_bk import LanguageModel
    from src.models.config import ResNetBKConfig
    
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # è¨­å®šã‚’å¾©å…ƒ
    if 'config' in ckpt:
        config_dict = ckpt['config']
        if isinstance(config_dict, dict):
            # Filter out unknown keys that aren't in ResNetBKConfig
            import dataclasses
            valid_fields = {f.name for f in dataclasses.fields(ResNetBKConfig)}
            filtered_dict = {k: v for k, v in config_dict.items() if k in valid_fields}
            config = ResNetBKConfig(**filtered_dict)
        else:
            config = config_dict
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        config = ResNetBKConfig(
            d_model=4096,
            n_layers=48,
            n_seq=512,
            vocab_size=32000,  # Japanese tokenizer
        )
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = LanguageModel(config).to(device)
    
    # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    if 'model_state_dict' in ckpt:
        model.load_state_dict(ckpt['model_state_dict'], strict=False)
    else:
        model.load_state_dict(ckpt, strict=False)
    
    model.eval()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"âœ“ Model loaded successfully!")
    print(f"  d_model: {config.d_model}, n_layers: {config.n_layers}")
    print(f"  Parameters: {total_params / 1e6:.1f}M")
    
    return model, config


def load_tokenizer(tokenizer_name: str = "rinna/japanese-gpt-neox-3.6b"):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
    try:
        from transformers import AutoTokenizer
        
        # æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è©¦ã™
        try:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
            print(f"âœ“ Japanese tokenizer loaded: {tokenizer_name}")
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: GPT-2
            tokenizer = AutoTokenizer.from_pretrained("gpt2")
            print("âœ“ GPT-2 tokenizer loaded (fallback)")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        return tokenizer
    except ImportError:
        print("âš  transformers not installed. Using simple tokenizer.")
        return SimpleTokenizer()


class SimpleTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªUTF-8ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
    def __init__(self, vocab_size=32000):
        self.vocab_size = vocab_size
        self.eos_token_id = 2
        self.pad_token_id = 0
    
    def encode(self, text, return_tensors=None, **kwargs):
        # UTF-8ãƒã‚¤ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        ids = [min(b + 3, self.vocab_size - 1) for b in text.encode('utf-8')]
        if return_tensors == "pt":
            return torch.tensor([ids])
        return ids
    
    def decode(self, ids, skip_special_tokens=True):
        if isinstance(ids, torch.Tensor):
            ids = ids.tolist()
        if isinstance(ids[0], list):
            ids = ids[0]
        # ãƒã‚¤ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
        try:
            bytes_list = [max(0, i - 3) for i in ids if i > 2]
            return bytes(bytes_list).decode('utf-8', errors='replace')
        except:
            return ""


@torch.no_grad()
def generate(
    model,
    tokenizer,
    prompt: str,
    config: ChatConfig,
    device: str = "cuda",
    stream: bool = True,
):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    if hasattr(tokenizer, '__call__'):
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    else:
        input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)
    
    generated = input_ids.clone()
    past_tokens = set(input_ids[0].tolist())
    
    for step in range(config.max_new_tokens):
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ¶é™
        if generated.shape[1] > model.n_seq:
            context = generated[:, -model.n_seq:]
        else:
            context = generated
        
        # Forward pass
        with torch.cuda.amp.autocast(enabled=device=="cuda"):
            logits = model(context)
        
        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logits
        next_logits = logits[:, -1, :].float() / config.temperature
        
        # Repetition penalty
        if config.repetition_penalty != 1.0:
            for token_id in past_tokens:
                if token_id < next_logits.shape[-1]:
                    next_logits[0, token_id] /= config.repetition_penalty
        
        # Top-k filtering
        if config.top_k > 0:
            indices_to_remove = next_logits < torch.topk(next_logits, config.top_k)[0][..., -1, None]
            next_logits[indices_to_remove] = float('-inf')
        
        # Top-p (nucleus) filtering
        if config.top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > config.top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            next_logits[indices_to_remove] = float('-inf')
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        probs = F.softmax(next_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        generated = torch.cat([generated, next_token], dim=1)
        past_tokens.add(next_token.item())
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
        if stream:
            token_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)
            print(token_text, end="", flush=True)
        
        # EOS check
        if hasattr(tokenizer, 'eos_token_id') and next_token.item() == tokenizer.eos_token_id:
            break
    
    if stream:
        print()  # æ”¹è¡Œ
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    output_ids = generated[0].tolist()
    return tokenizer.decode(output_ids, skip_special_tokens=True)


def find_latest_checkpoint():
    """æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¢ã™"""
    search_dirs = [
        "checkpoints/phase8_10b_japanese",
        "checkpoints/phase8_10b_rtx3080",
        "checkpoints/phase8",
        "checkpoints/phase7_max_push",
    ]
    
    for dir_path in search_dirs:
        ckpt_dir = Path(dir_path)
        if not ckpt_dir.exists():
            continue
        
        # å„ªå…ˆé †ä½: best.pt > final.pt > phase8_10b_final.pt > step_*.pt
        for name in ["best.pt", "final.pt", "phase8_10b_final.pt"]:
            path = ckpt_dir / name
            if path.exists():
                return str(path)
        
        # step_*.pt ã‹ã‚‰æœ€æ–°
        step_files = list(ckpt_dir.glob("step_*.pt")) + list(ckpt_dir.glob("*.pt"))
        if step_files:
            return str(max(step_files, key=lambda p: p.stat().st_mtime))
    
    return None


def interactive_chat(model, tokenizer, config: ChatConfig, device: str = "cuda"):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰"""
    
    print("\n" + "=" * 60)
    print("ğŸ¤– MUSE Chat AI - Phase 8 Japanese/English")
    print("=" * 60)
    print("ã‚³ãƒãƒ³ãƒ‰ / Commands:")
    print("  /quit, /exit  - çµ‚äº† / Exit")
    print("  /temp <val>   - temperature (ç¾åœ¨: {:.1f})".format(config.temperature))
    print("  /tokens <val> - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° (ç¾åœ¨: {})".format(config.max_new_tokens))
    print("  /clear        - å±¥æ­´ã‚¯ãƒªã‚¢ / Clear history")
    print("  /system <msg> - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š")
    print("=" * 60 + "\n")
    
    history: List[Dict[str, str]] = []
    system_prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    
    while True:
        try:
            user_input = input("You: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼ / Goodbye!")
            break
        
        if not user_input:
            continue
        
        # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
        if user_input.startswith('/'):
            parts = user_input.split(maxsplit=1)
            cmd = parts[0].lower()
            
            if cmd in ['/quit', '/exit', '/q']:
                print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼ / Goodbye!")
                break
            elif cmd == '/temp' and len(parts) > 1:
                try:
                    config.temperature = float(parts[1])
                    print(f"âœ“ Temperature: {config.temperature}")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            elif cmd == '/tokens' and len(parts) > 1:
                try:
                    config.max_new_tokens = int(parts[1])
                    print(f"âœ“ Max tokens: {config.max_new_tokens}")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            elif cmd == '/clear':
                history = []
                print("âœ“ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                continue
            elif cmd == '/system' and len(parts) > 1:
                system_prompt = parts[1]
                print(f"âœ“ System prompt: {system_prompt}")
                continue
            else:
                print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {cmd}")
                continue
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆæ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆå½¢å¼ï¼‰
        prompt = f"### ã‚·ã‚¹ãƒ†ãƒ :\n{system_prompt}\n\n"
        
        # å±¥æ­´ï¼ˆç›´è¿‘3ã‚¿ãƒ¼ãƒ³ï¼‰
        for h in history[-3:]:
            prompt += f"### ãƒ¦ãƒ¼ã‚¶ãƒ¼:\n{h['user']}\n\n### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:\n{h['assistant']}\n\n"
        
        prompt += f"### ãƒ¦ãƒ¼ã‚¶ãƒ¼:\n{user_input}\n\n### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:\n"
        
        # ç”Ÿæˆ
        print("AI: ", end="", flush=True)
        try:
            response = generate(
                model, tokenizer, prompt,
                config=config,
                device=device,
                stream=True,
            )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if "### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:" in response:
                response = response.split("### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:")[-1].strip()
            if "### ãƒ¦ãƒ¼ã‚¶ãƒ¼:" in response:
                response = response.split("### ãƒ¦ãƒ¼ã‚¶ãƒ¼:")[0].strip()
            
            history.append({
                'user': user_input,
                'assistant': response
            })
            
        except Exception as e:
            print(f"\nâŒ Error: {e}")
        
        print()


def main():
    parser = argparse.ArgumentParser(description="MUSE Chat AI - Phase 8")
    parser.add_argument("--checkpoint", type=str, default=None,
                        help="Path to checkpoint (default: auto-detect)")
    parser.add_argument("--prompt", type=str, default=None,
                        help="Single prompt (non-interactive)")
    parser.add_argument("--max-tokens", type=int, default=256,
                        help="Maximum tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.8,
                        help="Sampling temperature")
    parser.add_argument("--tokenizer", type=str, default="rinna/japanese-gpt-neox-3.6b",
                        help="Tokenizer name")
    parser.add_argument("--device", type=str, default="auto",
                        help="Device (cuda/cpu/auto)")
    args = parser.parse_args()
    
    # ãƒ‡ãƒã‚¤ã‚¹
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    if device == "cuda" and not torch.cuda.is_available():
        print("âš  CUDA not available, using CPU")
        device = "cpu"
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¢ç´¢
    checkpoint = args.checkpoint or find_latest_checkpoint()
    
    if checkpoint is None:
        print("âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
        print("\nã¾ãšå­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  make start-japanese   # æ—¥æœ¬èª10Bãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        print("\nã¾ãŸã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®š:")
        print("  python scripts/chat_inference.py --checkpoint path/to/model.pt")
        sys.exit(1)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, model_config = load_model_phase8(checkpoint, device)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = load_tokenizer(args.tokenizer)
    
    # ãƒãƒ£ãƒƒãƒˆè¨­å®š
    chat_config = ChatConfig(
        temperature=args.temperature,
        max_new_tokens=args.max_tokens,
    )
    
    # æ¨è«–
    if args.prompt:
        # å˜ç™ºãƒ¢ãƒ¼ãƒ‰
        print(f"\nPrompt: {args.prompt}")
        print("-" * 40)
        response = generate(
            model, tokenizer, args.prompt,
            config=chat_config,
            device=device,
            stream=True,
        )
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        interactive_chat(model, tokenizer, chat_config, device)


if __name__ == "__main__":
    main()
