#!/usr/bin/env python3
"""
MUSE Chat AI - Phase 8 å¯¾å¿œç‰ˆ
==============================
è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒãƒ£ãƒƒãƒˆã§ãã¾ã™ï¼æ—¥æœ¬èªãƒ»è‹±èªä¸¡å¯¾å¿œã€‚

Usage:
    make chat                    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ãƒãƒ£ãƒƒãƒˆ
    make chat CHECKPOINT=path    # æŒ‡å®šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    python scripts/chat_inference.py --checkpoint checkpoints/phase8_10b_japanese/best.pt
"""

import argparse
import os
import sys
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, List, Dict

sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn.functional as F

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.prompt import Prompt
    from rich.table import Table
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

console = Console() if RICH_AVAILABLE else None


@dataclass
class ChatConfig:
    """ãƒãƒ£ãƒƒãƒˆè¨­å®š"""
    temperature: float = 0.8
    top_k: int = 50
    top_p: float = 0.9
    max_new_tokens: int = 256
    repetition_penalty: float = 1.1


def load_model_phase8(checkpoint_path: str, device: str = "cuda"):
    """Phase 8 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # è¨­å®šã‚’å¾©å…ƒ
    if 'config' in ckpt:
        config_dict = ckpt['config']
        if isinstance(config_dict, dict):
            config = config_dict
        else:
            config = vars(config_dict) if hasattr(config_dict, '__dict__') else {}
    else:
        config = {
            'd_model': 1024,
            'n_layers': 24,
            'n_seq': 512,
            'vocab_size': 50256,
        }
    
    # Phase8IntegratedModel ã‚’è©¦ã™ (æ¨å¥¨)
    try:
        from src.models.phase8.integrated_model import Phase8IntegratedModel
        from src.models.phase8.config import Phase8Config
        
        # Phase8Config ã‚’ä½œæˆ
        phase8_config = Phase8Config(
            d_model=config.get('d_model', 1024),
            n_layers=config.get('n_layers', 24),
            n_seq=config.get('n_seq', 512),
            vocab_size=config.get('vocab_size', 50256),
            num_heads=config.get('num_heads', 16),
            htt_rank=config.get('htt_rank', 16),
            use_resonant_htt=config.get('use_resonant_htt', True),
            resonant_num_cores=config.get('resonant_num_cores', 4),
            use_zeta_init=config.get('use_zeta_init', True),
            low_rank_ffn=config.get('low_rank_ffn', True),
            low_rank_attention=config.get('low_rank_attention', True),
            low_rank_rank=config.get('low_rank_rank', 32),
            use_bk_hyperbolic=config.get('use_bk_hyperbolic', True),
        )
        
        model = Phase8IntegratedModel(phase8_config).to(device)
        print("âœ“ Using Phase8IntegratedModel")
        
    except ImportError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§LanguageModel
        from src.models.resnet_bk import LanguageModel
        from src.models.config import ResNetBKConfig
        
        import dataclasses
        valid_fields = {f.name for f in dataclasses.fields(ResNetBKConfig)}
        filtered_dict = {k: v for k, v in config.items() if k in valid_fields}
        model_config = ResNetBKConfig(**filtered_dict)
        model = LanguageModel(model_config).to(device)
        print("âœ“ Using LanguageModel (fallback)")
    
    # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    if 'model_state_dict' in ckpt:
        model.load_state_dict(ckpt['model_state_dict'], strict=False)
    elif 'state_dict' in ckpt:
        model.load_state_dict(ckpt['state_dict'], strict=False)
    else:
        model.load_state_dict(ckpt, strict=False)
    
    model.eval()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"âœ“ Model loaded successfully!")
    print(f"  d_model: {config.get('d_model', '?')}, n_layers: {config.get('n_layers', '?')}")
    print(f"  Parameters: {total_params / 1e6:.1f}M")
    
    return model, config


def load_tokenizer(tokenizer_name: str = None, vocab_size: int = 50256):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦è‡ªå‹•é¸æŠï¼‰"""
    
    # Auto-detect tokenizer based on vocab_size if not specified
    if tokenizer_name is None or tokenizer_name == "auto":
        if vocab_size >= 30000 and vocab_size <= 33000:
            # Japanese model (rinna) - 32768 vocab
            tokenizer_name = "rinna/japanese-gpt-neox-3.6b"
            print(f"  Auto-detected Japanese tokenizer (vocab_size={vocab_size})")
        else:
            # English model (GPT-2) - 50256 vocab  
            tokenizer_name = "gpt2"
            print(f"  Auto-detected GPT-2 tokenizer (vocab_size={vocab_size})")
    
    try:
        from transformers import AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        print(f"âœ“ Tokenizer loaded: {tokenizer_name}")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Store vocab_size for clipping
        tokenizer._model_vocab_size = vocab_size
        
        return tokenizer
    except ImportError:
        print("âš  transformers not installed. Using simple tokenizer.")
        return SimpleTokenizer(vocab_size)
    except Exception as e:
        print(f"âš  Could not load tokenizer {tokenizer_name}: {e}")
        print("  Falling back to simple tokenizer.")
        return SimpleTokenizer(vocab_size)


class SimpleTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªUTF-8ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
    def __init__(self, vocab_size=32000):
        self.vocab_size = vocab_size
        self.eos_token_id = 2
        self.pad_token_id = 0
    
    def encode(self, text, return_tensors=None, **kwargs):
        # UTF-8ãƒã‚¤ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        ids = [min(b + 3, self.vocab_size - 1) for b in text.encode('utf-8')]
        if return_tensors == "pt":
            return torch.tensor([ids])
        return ids
    
    def decode(self, ids, skip_special_tokens=True):
        if isinstance(ids, torch.Tensor):
            ids = ids.tolist()
        if isinstance(ids[0], list):
            ids = ids[0]
        # ãƒã‚¤ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
        try:
            bytes_list = [max(0, i - 3) for i in ids if i > 2]
            return bytes(bytes_list).decode('utf-8', errors='replace')
        except:
            return ""


@torch.no_grad()
def generate(
    model,
    tokenizer,
    prompt: str,
    config: ChatConfig,
    device: str = "cuda",
    stream: bool = True,
    n_seq: int = 512,  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· (ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šé•·)
    vocab_size: int = 50256,  # ãƒ¢ãƒ‡ãƒ«ã®vocab_sizeï¼ˆã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰
):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
    
    # ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ID (vocab_sizeä»¥ä¸‹ã«åˆ¶é™)
    pad_token_id = getattr(tokenizer, 'pad_token_id', 0) or 0
    pad_token_id = min(pad_token_id, vocab_size - 1)
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡ºåŠ›å½¢å¼ãŒç•°ãªã‚‹
    input_ids = None
    
    # æ–¹æ³•1: tokenizer(prompt) ã‚’è©¦ã™
    try:
        encoded = tokenizer(prompt, return_tensors="pt")
        if hasattr(encoded, 'input_ids'):
            input_ids = encoded.input_ids.to(device)
        elif isinstance(encoded, dict) and 'input_ids' in encoded:
            input_ids = encoded['input_ids'].to(device)
        elif isinstance(encoded, torch.Tensor):
            input_ids = encoded.to(device)
    except Exception as e:
        pass
    
    # æ–¹æ³•2: tokenizer.encode(prompt) ã‚’è©¦ã™
    if input_ids is None:
        try:
            encoded = tokenizer.encode(prompt, return_tensors="pt")
            if isinstance(encoded, torch.Tensor):
                input_ids = encoded.to(device)
                if input_ids.dim() == 1:
                    input_ids = input_ids.unsqueeze(0)
            elif hasattr(encoded, 'input_ids'):
                input_ids = encoded.input_ids.to(device)
            elif isinstance(encoded, dict) and 'input_ids' in encoded:
                input_ids = encoded['input_ids'].to(device)
        except Exception as e:
            pass
    
    # æ–¹æ³•3: tokenizer.encode(prompt) ã§ãƒªã‚¹ãƒˆã‚’å–å¾—
    if input_ids is None:
        try:
            encoded = tokenizer.encode(prompt)
            # ã‚¿ãƒ—ãƒ«ã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨ (input_ids, attention_mask ãªã©)
            if isinstance(encoded, tuple):
                ids = encoded[0]
            elif isinstance(encoded, list):
                ids = encoded
            elif isinstance(encoded, torch.Tensor):
                ids = encoded.tolist()
            else:
                ids = list(encoded)
            
            # ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒªã‚¹ãƒˆã®å ´åˆã¯å±•é–‹
            if isinstance(ids, list) and len(ids) > 0 and isinstance(ids[0], list):
                ids = ids[0]
            
            input_ids = torch.tensor([ids], dtype=torch.long, device=device)
        except Exception as e:
            pass
    
    # æ–¹æ³•4: UTF-8ãƒã‚¤ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰
    if input_ids is None:
        ids = list(prompt.encode('utf-8'))
        input_ids = torch.tensor([ids], dtype=torch.long, device=device)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’vocab_sizeä»¥ä¸‹ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆCUDA OOBã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    input_ids = torch.clamp(input_ids, 0, vocab_size - 1)
    
    generated = input_ids.clone()
    past_tokens = set(input_ids[0].tolist())
    
    for step in range(config.max_new_tokens):
        # ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        current_len = generated.shape[1]
        
        # n_seq ã‚ˆã‚Šé•·ã„å ´åˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆæœ€å¾Œã®n_seqãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
        if current_len > n_seq:
            context = generated[:, -n_seq:]
            prompt_end_idx = n_seq - 1  # æœ€å¾Œã®ä½ç½®
        else:
            # n_seq ã«æº€ãŸãªã„å ´åˆã¯å³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Position 0ã«é…ç½®ï¼‰
            # ã“ã‚Œã¯è¨“ç·´æ™‚ã¨åŒã˜é…ç½®ã‚’ç¶­æŒã™ã‚‹ãŸã‚é‡è¦
            context = generated.clone()
            prompt_end_idx = current_len - 1  # å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ«å°¾ä½ç½®
            if current_len < n_seq:
                padding = torch.full((1, n_seq - current_len), pad_token_id, dtype=torch.long, device=device)
                context = torch.cat([context, padding], dim=1)  # å³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        
        # Forward pass
        with torch.cuda.amp.autocast(enabled=device=="cuda", dtype=torch.bfloat16):
            output = model(context)
            # Phase8IntegratedModel returns (logits, diagnostics) tuple
            if isinstance(output, tuple):
                logits = output[0]
            else:
                logits = output
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ«å°¾ä½ç½®ã®logitsã‚’å–å¾—ï¼ˆå³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆã€prompt_end_idxã‚’ä½¿ç”¨ï¼‰
        next_logits = logits[:, prompt_end_idx, :].float() / config.temperature
        
        # Repetition penalty
        if config.repetition_penalty != 1.0:
            for token_id in past_tokens:
                if token_id < next_logits.shape[-1]:
                    next_logits[0, token_id] /= config.repetition_penalty
        
        # Top-k filtering
        if config.top_k > 0:
            indices_to_remove = next_logits < torch.topk(next_logits, config.top_k)[0][..., -1, None]
            next_logits[indices_to_remove] = float('-inf')
        
        # Top-p (nucleus) filtering
        if config.top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > config.top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            next_logits[indices_to_remove] = float('-inf')
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        probs = F.softmax(next_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        generated = torch.cat([generated, next_token], dim=1)
        past_tokens.add(next_token.item())
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
        if stream:
            token_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)
            print(token_text, end="", flush=True)
        
        # EOS check
        if hasattr(tokenizer, 'eos_token_id') and next_token.item() == tokenizer.eos_token_id:
            break
    
    if stream:
        print()  # æ”¹è¡Œ
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    output_ids = generated[0].tolist()
    return tokenizer.decode(output_ids, skip_special_tokens=True)


def find_latest_checkpoint():
    """æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¢ã™"""
    search_dirs = [
        "checkpoints/phase8_300m_scaling",  # 300M scaling experiment
        "checkpoints/phase8_10b_japanese",
        "checkpoints/phase8_10b_rtx3080",
        "checkpoints/phase8",
        "checkpoints/phase7_max_push",
    ]
    
    for dir_path in search_dirs:
        ckpt_dir = Path(dir_path)
        if not ckpt_dir.exists():
            continue
        
        # å„ªå…ˆé †ä½: best.pt > final.pt > phase8_10b_final.pt > step_*.pt
        for name in ["best.pt", "final.pt", "phase8_10b_final.pt"]:
            path = ckpt_dir / name
            if path.exists():
                return str(path)
        
        # step_*.pt ã‹ã‚‰æœ€æ–°
        step_files = list(ckpt_dir.glob("step_*.pt")) + list(ckpt_dir.glob("*.pt"))
        if step_files:
            return str(max(step_files, key=lambda p: p.stat().st_mtime))
    
    return None


def interactive_chat(model, tokenizer, config: ChatConfig, device: str = "cuda", model_config: dict = None):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰"""
    
    # Get vocab_size from model config
    vocab_size = 50256
    n_seq = 512
    if model_config:
        vocab_size = model_config.get('vocab_size', 50256)
        n_seq = model_config.get('n_seq', 512)
    
    print("\n" + "=" * 60)
    print("ğŸ¤– MUSE Chat AI - Phase 8 Japanese/English")
    print("=" * 60)
    print("ã‚³ãƒãƒ³ãƒ‰ / Commands:")
    print("  /quit, /exit  - çµ‚äº† / Exit")
    print("  /temp <val>   - temperature (ç¾åœ¨: {:.1f})".format(config.temperature))
    print("  /tokens <val> - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° (ç¾åœ¨: {})".format(config.max_new_tokens))
    print("  /clear        - å±¥æ­´ã‚¯ãƒªã‚¢ / Clear history")
    print("  /system <msg> - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š")
    print("=" * 60 + "\n")
    
    history: List[Dict[str, str]] = []
    system_prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    
    while True:
        try:
            user_input = input("You: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼ / Goodbye!")
            break
        
        if not user_input:
            continue
        
        # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
        if user_input.startswith('/'):
            parts = user_input.split(maxsplit=1)
            cmd = parts[0].lower()
            
            if cmd in ['/quit', '/exit', '/q']:
                print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼ / Goodbye!")
                break
            elif cmd == '/temp' and len(parts) > 1:
                try:
                    config.temperature = float(parts[1])
                    print(f"âœ“ Temperature: {config.temperature}")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            elif cmd == '/tokens' and len(parts) > 1:
                try:
                    config.max_new_tokens = int(parts[1])
                    print(f"âœ“ Max tokens: {config.max_new_tokens}")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            elif cmd == '/clear':
                history = []
                print("âœ“ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                continue
            elif cmd == '/system' and len(parts) > 1:
                system_prompt = parts[1]
                print(f"âœ“ System prompt: {system_prompt}")
                continue
            else:
                print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {cmd}")
                continue
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã«åˆã‚ã›ã‚‹: ### æŒ‡ç¤º: / ### å›ç­”:ï¼‰
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯ã€Œ### æŒ‡ç¤º:ã€ã€Œ### å…¥åŠ›:ã€ã€Œ### å›ç­”:ã€å½¢å¼
        prompt = f"### æŒ‡ç¤º:\n{user_input}\n\n### å›ç­”:\n"
        
        # ç”Ÿæˆ
        print("AI: ", end="", flush=True)
        try:
            response = generate(
                model, tokenizer, prompt,
                config=config,
                device=device,
                stream=True,
                n_seq=n_seq,
                vocab_size=vocab_size,
            )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if "### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:" in response:
                response = response.split("### ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:")[-1].strip()
            if "### ãƒ¦ãƒ¼ã‚¶ãƒ¼:" in response:
                response = response.split("### ãƒ¦ãƒ¼ã‚¶ãƒ¼:")[0].strip()
            
            history.append({
                'user': user_input,
                'assistant': response
            })
            
        except Exception as e:
            print(f"\nâŒ Error: {e}")
        
        print()


def main():
    parser = argparse.ArgumentParser(description="MUSE Chat AI - Phase 8")
    parser.add_argument("--checkpoint", type=str, default=None,
                        help="Path to checkpoint (default: auto-detect)")
    parser.add_argument("--prompt", type=str, default=None,
                        help="Single prompt (non-interactive)")
    parser.add_argument("--max-tokens", type=int, default=256,
                        help="Maximum tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.8,
                        help="Sampling temperature")
    parser.add_argument("--tokenizer", type=str, default="auto",
                        help="Tokenizer name (default: auto - detect from vocab_size)")
    parser.add_argument("--device", type=str, default="auto",
                        help="Device (cuda/cpu/auto)")
    args = parser.parse_args()
    
    # ãƒ‡ãƒã‚¤ã‚¹
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    if device == "cuda" and not torch.cuda.is_available():
        print("âš  CUDA not available, using CPU")
        device = "cpu"
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¢ç´¢
    checkpoint = args.checkpoint or find_latest_checkpoint()
    
    if checkpoint is None:
        print("âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
        print("\nã¾ãšå­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  make start-japanese   # æ—¥æœ¬èª10Bãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        print("\nã¾ãŸã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®š:")
        print("  python scripts/chat_inference.py --checkpoint path/to/model.pt")
        sys.exit(1)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, model_config = load_model_phase8(checkpoint, device)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆvocab_sizeã«åŸºã¥ã„ã¦è‡ªå‹•é¸æŠï¼‰
    vocab_size = model_config.get('vocab_size', 50256)
    tokenizer_name = model_config.get('tokenizer_name', args.tokenizer)  # Use config tokenizer if available
    tokenizer = load_tokenizer(tokenizer_name, vocab_size=vocab_size)
    
    # ãƒãƒ£ãƒƒãƒˆè¨­å®š
    chat_config = ChatConfig(
        temperature=args.temperature,
        max_new_tokens=args.max_tokens,
    )
    
    # æ¨è«–
    if args.prompt:
        # å˜ç™ºãƒ¢ãƒ¼ãƒ‰
        print(f"\nPrompt: {args.prompt}")
        print("-" * 40)
        response = generate(
            model, tokenizer, args.prompt,
            config=chat_config,
            device=device,
            stream=True,
            n_seq=model_config.get('n_seq', 512),
            vocab_size=vocab_size,
        )
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        interactive_chat(model, tokenizer, chat_config, device, model_config=model_config)


if __name__ == "__main__":
    main()
