{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Phase 1: Optimized Hybrid Analytic Gradient Test\n",
    "\n",
    "This notebook tests the optimized hybrid analytic gradient implementation:\n",
    "- GRAD_BLEND grid search\n",
    "- Fully analytic MoE backward pass\n",
    "- Mixed-precision gradient computation\n",
    "- Batched analytic gradient with vmap\n",
    "\n",
    "**Test Configuration:**\n",
    "- Model: d_model=64, n_layers=4, N=128\n",
    "- Training: 3 epochs on WikiText-2\n",
    "- Validation: Numerical stability (no NaN/Inf), Convergence (loss decreases)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo setup (clone if needed, add to sys.path)\n",
    "import os, sys, subprocess, pathlib\n",
    "REPO_URL = 'https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git'\n",
    "REPO_DIR = 'Project-ResNet-BK-An-O-N-Language-Model-Architecture'\n",
    "cwd = pathlib.Path.cwd()\n",
    "candidates = [cwd, cwd.parent, cwd / REPO_DIR, cwd.parent / REPO_DIR]\n",
    "root = next((p for p in candidates if (p / 'src').exists()), None)\n",
    "if root is None:\n    root = cwd / REPO_DIR\n    if not root.exists():\n        subprocess.run(['git', 'clone', REPO_URL, str(root)], check=True)\n",
    "if root != pathlib.Path.cwd():\n    os.chdir(root)\n",
    "root_str = str(pathlib.Path.cwd())\n",
    "if root_str not in sys.path:\n    sys.path.insert(0, root_str)\n",
    "print('PWD:', root_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models.configurable_resnet_bk import ConfigurableResNetBK, ResNetBKConfig\n",
    "from src.models.analytic_moe import AnalyticMoELayer, validate_analytic_gradients\n",
    "from src.models.mixed_precision_bk_core import MixedPrecisionBKCoreFunction, benchmark_mixed_precision\n",
    "from src.models.batched_gradient import BatchedAnalyticBKCoreFunction, profile_batched_gradient\n",
    "from src.training.grad_blend_optimizer import GradBlendOptimizer\n",
    "from src.utils.data_utils import get_wikitext2_dataloaders\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Analytic MoE Gradient Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing Analytic MoE Gradient Validation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create small MoE layer\n",
    "moe_layer = AnalyticMoELayer(d_model=64, num_experts=4, top_k=1)\n",
    "moe_layer.to(device)\n",
    "\n",
    "# Test input\n",
    "x_test = torch.randn(2, 16, 64, device=device)\n",
    "\n",
    "# Validate gradients\n",
    "validation_results = validate_analytic_gradients(moe_layer, x_test)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Input gradient error: {validation_results['input_gradient_error']:.6f}\")\n",
    "print(f\"  Max error: {validation_results['max_error']:.6f}\")\n",
    "print(f\"  Passed: {validation_results['passed']}\")\n",
    "\n",
    "if validation_results['passed']:\n",
    "    print(\"\\n✓ Analytic MoE gradients validated successfully!\")\n",
    "else:\n",
    "    print(\"\\n✗ Analytic MoE gradient validation failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Benchmarking Mixed Precision\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mp_results = benchmark_mixed_precision(\n",
    "    batch_size=8,\n",
    "    seq_len=128,\n",
    "    num_trials=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  FP32 time: {mp_results['fp32_time']*1000:.2f}ms\")\n",
    "print(f\"  Mixed precision time: {mp_results['mixed_time']*1000:.2f}ms\")\n",
    "print(f\"  Speedup: {mp_results['speedup']:.2f}x\")\n",
    "print(f\"  Max error: {mp_results['max_error']:.6e}\")\n",
    "print(f\"  Relative error: {mp_results['relative_error']:.6e}\")\n",
    "\n",
    "if mp_results['speedup'] > 1.0:\n",
    "    print(f\"\\n✓ Mixed precision achieved {mp_results['speedup']:.2f}x speedup!\")\n",
    "else:\n",
    "    print(\"\\n✗ Mixed precision did not improve performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profile Batched Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Profiling Batched Gradient Computation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "batch_results = profile_batched_gradient(\n",
    "    batch_sizes=[1, 4, 8, 16, 32],\n",
    "    seq_len=128,\n",
    "    num_trials=50,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Time vs batch size\n",
    "ax = axes[0]\n",
    "ax.plot(batch_results['batch_sizes'], batch_results['sequential_times'], \n",
    "        marker='o', label='Sequential', linewidth=2)\n",
    "ax.plot(batch_results['batch_sizes'], batch_results['batched_times'], \n",
    "        marker='s', label='Batched', linewidth=2)\n",
    "ax.plot(batch_results['batch_sizes'], batch_results['memory_optimized_times'], \n",
    "        marker='^', label='Memory-Optimized', linewidth=2)\n",
    "ax.set_xlabel('Batch Size')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Gradient Computation Time vs Batch Size')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot 2: Speedup vs batch size\n",
    "ax = axes[1]\n",
    "ax.plot(batch_results['batch_sizes'], batch_results['speedups'], \n",
    "        marker='o', linewidth=2, color='green')\n",
    "ax.axhline(y=1.0, color='r', linestyle='--', label='Baseline')\n",
    "ax.set_xlabel('Batch Size')\n",
    "ax.set_ylabel('Speedup')\n",
    "ax.set_title('Batched Gradient Speedup')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('step2_phase1_batched_gradient_profile.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "avg_speedup = np.mean(batch_results['speedups'])\n",
    "print(f\"\\nAverage speedup: {avg_speedup:.2f}x\")\n",
    "\n",
    "if avg_speedup > 1.5:\n",
    "    print(f\"✓ Batched gradient achieved {avg_speedup:.2f}x average speedup!\")\n",
    "else:\n",
    "    print(\"✗ Batched gradient did not achieve significant speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load WikiText-2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading WikiText-2 Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_loader, val_loader, vocab_size = get_wikitext2_dataloaders(\n",
    "    batch_size=32,\n",
    "    seq_len=128,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRAD_BLEND Grid Search (Quick Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GRAD_BLEND Grid Search (Quick Test)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create model\n",
    "config = ResNetBKConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    n_layers=4,\n",
    "    n_seq=128,\n",
    "    num_experts=4,\n",
    "    top_k=1,\n",
    "    use_analytic_gradient=True,\n",
    "    grad_blend=0.5\n",
    ")\n",
    "\n",
    "model = ConfigurableResNetBK(config)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {model.get_num_parameters()/1e6:.2f}M\")\n",
    "\n",
    "# Quick grid search (fewer alpha values and epochs for testing)\n",
    "optimizer = GradBlendOptimizer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    alpha_values=[0.0, 0.3, 0.5, 0.7, 1.0],  # Quick test\n",
    "    epochs_per_trial=2,  # Quick test\n",
    "    device=device,\n",
    "    save_dir='results/step2_phase1_grad_blend_quick'\n",
    ")\n",
    "\n",
    "summary = optimizer.run_grid_search()\n",
    "\n",
    "print(f\"\\n✓ Grid search complete!\")\n",
    "print(f\"  Best alpha: {summary['best_alpha']}\")\n",
    "print(f\"  Best perplexity: {summary['best_perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model with Optimized Settings (3 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training with Optimized Settings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use best alpha from grid search\n",
    "from src.models.bk_core import BKCoreFunction\n",
    "BKCoreFunction.GRAD_BLEND = summary['best_alpha']\n",
    "\n",
    "# Reset model\n",
    "model = ConfigurableResNetBK(config)\n",
    "model.to(device)\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_ppls = []\n",
    "val_ppls = []\n",
    "has_nan_inf = False\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    print(f\"\\nEpoch {epoch+1}/3\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1))\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"\\n✗ NaN/Inf detected in loss at batch {batch_idx}!\")\n",
    "            has_nan_inf = True\n",
    "            break\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradients for NaN/Inf\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and not torch.isfinite(param.grad).all():\n",
    "                print(f\"\\n✗ NaN/Inf detected in gradient of {name} at batch {batch_idx}!\")\n",
    "                has_nan_inf = True\n",
    "                break\n",
    "        \n",
    "        if has_nan_inf:\n",
    "            break\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * y_batch.numel()\n",
    "        total_tokens += y_batch.numel()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            avg_ppl = np.exp(avg_loss)\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: Loss={avg_loss:.4f}, PPL={avg_ppl:.2f}\")\n",
    "    \n",
    "    if has_nan_inf:\n",
    "        break\n",
    "    \n",
    "    avg_train_loss = total_loss / total_tokens\n",
    "    avg_train_ppl = np.exp(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_ppls.append(avg_train_ppl)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y_batch.view(-1))\n",
    "            \n",
    "            total_val_loss += loss.item() * y_batch.numel()\n",
    "            total_val_tokens += y_batch.numel()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    avg_val_ppl = np.exp(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_ppls.append(avg_val_ppl)\n",
    "    \n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train PPL: {avg_train_ppl:.2f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val PPL: {avg_val_ppl:.2f}\")\n",
    "\n",
    "# Validation checks\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Validation Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: No NaN/Inf\n",
    "if not has_nan_inf:\n",
    "    print(\"✓ No NaN/Inf detected during training\")\n",
    "else:\n",
    "    print(\"✗ NaN/Inf detected during training\")\n",
    "\n",
    "# Check 2: Loss decreases\n",
    "if len(train_losses) >= 2 and train_losses[-1] < train_losses[0]:\n",
    "    print(f\"✓ Training loss decreased: {train_losses[0]:.4f} → {train_losses[-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"✗ Training loss did not decrease\")\n",
    "\n",
    "# Check 3: Validation loss decreases\n",
    "if len(val_losses) >= 2 and val_losses[-1] < val_losses[0]:\n",
    "    print(f\"✓ Validation loss decreased: {val_losses[0]:.4f} → {val_losses[-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"✗ Validation loss did not decrease\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, marker='o', label='Train', linewidth=2)\n",
    "ax.plot(val_losses, marker='s', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(train_ppls, marker='o', label='Train', linewidth=2)\n",
    "ax.plot(val_ppls, marker='s', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Training and Validation Perplexity')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('step2_phase1_training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = 'checkpoints/step2_phase1_model.pt'\n",
    "Path('checkpoints').mkdir(exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_ppls': train_ppls,\n",
    "    'val_ppls': val_ppls,\n",
    "    'best_alpha': summary['best_alpha']\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"\\n✓ Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested Step 2 Phase 1 optimizations:\n",
    "1. ✓ Analytic MoE gradient validation\n",
    "2. ✓ Mixed-precision speedup measurement\n",
    "3. ✓ Batched gradient profiling\n",
    "4. ✓ GRAD_BLEND grid search\n",
    "5. ✓ 3-epoch training with numerical stability checks\n",
    "\n",
    "All components are ready for integration into the full training pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}