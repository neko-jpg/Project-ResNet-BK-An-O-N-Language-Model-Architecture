{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-BK vs Mamba: Colab Head-to-Head\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture/blob/main/notebooks/colab_resnetbk_vs_mamba_headtohead.ipynb)\n",
    "\n",
    "Runs a fair, reproducible head-to-head between the repo's ResNet-BK implementation and the bundled Mamba baseline. Both models share tokenizer, dataset (WikiText-2 raw), optimizer (AdamW), schedule (cosine), seeds, and logging.\n",
    "- Quick sanity: run 8k tokens for a few hundred steps.\n",
    "- Stress: bump `SEQ_LENGTHS` to 32k/128k with smaller batch sizes.\n",
    "- Outputs: JSON loss traces + matplotlib curves per sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check runtime (GPU/versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform, torch, os\n",
    "\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies (Colab)\n",
    "- If you already have the repo checked out, skip the clone step.\n",
    "- Installation keeps everything CPU-friendly except Mamba kernels (requires GPU runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q datasets transformers matplotlib seaborn tqdm\n",
    "# Optional: official mamba-ssm kernels (not required for the bundled baseline)\n",
    "!pip install -q mamba-ssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone repository & set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo setup (clone if needed, add to sys.path)\n",
    "import os, sys, subprocess, pathlib\n",
    "REPO_URL = 'https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git'\n",
    "REPO_DIR = 'Project-ResNet-BK-An-O-N-Language-Model-Architecture'\n",
    "cwd = pathlib.Path.cwd()\n",
    "candidates = [cwd, cwd.parent, cwd / REPO_DIR, cwd.parent / REPO_DIR]\n",
    "root = next((p for p in candidates if (p / 'src').exists()), None)\n",
    "if root is None:\n    root = cwd / REPO_DIR\n    if not root.exists():\n        subprocess.run(['git', 'clone', REPO_URL, str(root)], check=True)\n",
    "if root != pathlib.Path.cwd():\n    os.chdir(root)\n",
    "root_str = str(pathlib.Path.cwd())\n",
    "if root_str not in sys.path:\n    sys.path.insert(0, root_str)\n",
    "print('PWD:', root_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, json, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
    "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"dataset\": {\"name\": \"wikitext\", \"config\": \"wikitext-2-raw-v1\"},\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"min_lr\": 1e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"max_steps\": 200,\n",
    "        \"log_every\": 20,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"batch_size\": 2,\n",
    "        \"seed\": 42,\n",
    "        \"use_amp\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"d_model\": 256,\n",
    "        \"n_layers\": 6,\n",
    "        \"num_experts\": 4,\n",
    "        \"top_k\": 1,\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "SEQ_LENGTHS = [8192, 32768]\n",
    "PER_SEQ_STEPS = {8192: 200, 32768: 120, 131072: 40}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset + dataloader (shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "_tokenizer = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    global _tokenizer\n",
    "    if _tokenizer is None:\n",
    "        tok = AutoTokenizer.from_pretrained(BASE_CONFIG[\"tokenizer_name\"])\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        _tokenizer = tok\n",
    "    return _tokenizer\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def load_lm_dataset(seq_length: int):\n",
    "    tokenizer = get_tokenizer()\n",
    "    raw = load_dataset(BASE_CONFIG[\"dataset\"][\"name\"], BASE_CONFIG[\"dataset\"][\"config\"])\n",
    "\n",
    "    def tok_fn(examples):\n",
    "        return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
    "\n",
    "    tokenized = raw[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    seq_plus_one = seq_length + 1\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
    "        total_length = len(concatenated) // seq_plus_one * seq_plus_one\n",
    "        concatenated = concatenated[:total_length]\n",
    "        result = [concatenated[i : i + seq_plus_one] for i in range(0, total_length, seq_plus_one)]\n",
    "        return {\"input_ids\": result}\n",
    "\n",
    "    grouped = tokenized.map(group_texts, batched=True, remove_columns=tokenized[\"train\"].column_names)\n",
    "    grouped.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "    return grouped[\"train\"]\n",
    "\n",
    "\n",
    "def make_dataloader(seq_length: int, batch_size: int, seed: int):\n",
    "    dataset = load_lm_dataset(seq_length)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    def collate(batch):\n",
    "        inputs = torch.stack([b[\"input_ids\"][:-1] for b in batch])\n",
    "        targets = torch.stack([b[\"input_ids\"][1:] for b in batch])\n",
    "        return inputs, targets\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        generator=g,\n",
    "        collate_fn=collate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model builders (identical dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnetbk(seq_length: int):\n",
    "    cfg = BASE_CONFIG[\"model\"]\n",
    "    tok = get_tokenizer()\n",
    "    return ResNetBK(\n",
    "        vocab_size=tok.vocab_size,\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        n_seq=seq_length,\n",
    "        num_experts=cfg[\"num_experts\"],\n",
    "        top_k=cfg[\"top_k\"],\n",
    "        dropout_p=cfg[\"dropout\"],\n",
    "        use_scattering_router=False,\n",
    "        use_birman_schwinger=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_mamba(seq_length: int):\n",
    "    cfg = BASE_CONFIG[\"model\"]\n",
    "    tok = get_tokenizer()\n",
    "    resnet_cfg = SimpleNamespace(\n",
    "        vocab_size=tok.vocab_size,\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        n_seq=seq_length,\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        tie_weights=True,\n",
    "    )\n",
    "    mamba_cfg = create_mamba_from_resnetbk_config(resnet_cfg)\n",
    "    return MambaLM(mamba_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop (shared optimizer/schedule/logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model_name, builder, seq_length: int, max_steps: int):\n",
    "    cfg = BASE_CONFIG[\"training\"]\n",
    "    batch_size = cfg[\"batch_size\"]\n",
    "    # keep long-context runs memory-friendly\n",
    "    if seq_length >= 32768:\n",
    "        batch_size = max(1, batch_size // 2)\n",
    "\n",
    "    set_seed(cfg[\"seed\"])\n",
    "    dataloader = make_dataloader(seq_length, batch_size=batch_size, seed=cfg[\"seed\"])\n",
    "    model = builder(seq_length).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg[\"learning_rate\"],\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=cfg[\"weight_decay\"],\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt, T_max=max_steps, eta_min=cfg[\"min_lr\"]\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"use_amp\"] and DEVICE == \"cuda\")\n",
    "\n",
    "    losses = []\n",
    "    wall_start = time.time()\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(dataloader):\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=cfg[\"use_amp\"] and DEVICE == \"cuda\"):\n",
    "            out = model(inputs)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "            )\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"[{model_name}] divergence detected at step {step+1} (loss={loss.item():.4f})\")\n",
    "            break\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg[\"grad_clip\"] is not None:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (step + 1) % cfg[\"log_every\"] == 0:\n",
    "            print(\n",
    "                f\"[{model_name}] step {step+1}/{max_steps} loss={loss.item():.4f} lr={scheduler.get_last_lr()[0]:.2e} bs={batch_size} seq={seq_length}\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"losses\": losses,\n",
    "        \"steps\": len(losses),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"wall_clock_sec\": time.time() - wall_start,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        def run_headtohead(seq_lengths=None):\n",
    "            seq_lengths = seq_lengths or SEQ_LENGTHS\n",
    "            all_results = {}\n",
    "            for seq_len in seq_lengths:\n",
    "                max_steps = PER_SEQ_STEPS.get(seq_len, BASE_CONFIG[\"training\"][\"max_steps\"])\n",
    "                print(f\"\n",
    "=== Sequence length {seq_len} | steps {max_steps} ===\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                resnet_result = train_one(\"resnet_bk\", build_resnetbk, seq_len, max_steps)\n",
    "                torch.cuda.empty_cache()\n",
    "                mamba_result = train_one(\"mamba\", build_mamba, seq_len, max_steps)\n",
    "\n",
    "                all_results[seq_len] = {\"resnet_bk\": resnet_result, \"mamba\": mamba_result}\n",
    "\n",
    "                out_path = Path(f\"colab_headtohead_{seq_len}.json\")\n",
    "                out_path.write_text(json.dumps(all_results[seq_len], indent=2))\n",
    "                print(f\"Saved {out_path}\")\n",
    "\n",
    "            return all_results\n",
    "\n",
    "\n",
    "        # placeholder to hold results across cells\n",
    "        results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run (choose quick or full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity (8k)\n",
    "# results = run_headtohead([8192])\n",
    "\n",
    "# Full sweep (defaults to SEQ_LENGTHS)\n",
    "# results = run_headtohead()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results_dict, seq_length: int):\n",
    "    data = results_dict[seq_length]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, color in [(\"resnet_bk\", \"#1f77b4\"), (\"mamba\", \"#d62728\")]:\n",
    "        losses = data[name][\"losses\"]\n",
    "        steps = range(1, len(losses) + 1)\n",
    "        plt.plot(steps, losses, label=name, color=color)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Cross-entropy loss\")\n",
    "    plt.title(f\"Training loss vs steps (seq_len={seq_length})\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example after running:\n",
    "# plot_losses(results, 8192)\n",
    "# plot_losses(results, 32768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect raw numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After run_headtohead, run this to see summary\n",
    "# import pprint\n",
    "# pprint.pp(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}