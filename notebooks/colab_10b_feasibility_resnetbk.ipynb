{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semiseparable 10B Parameter Feasibility (Colab analysis)\n",
        "\n",
        "Estimates param counts and memory, plus meta-device shape check. Does NOT train 10B on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stable install: Torch 2.3.1 cu121 + deps pinned for transformers/sklearn/scipy\n",
        "# If you just upgraded pip/setuptools, restart runtime once before running below.\n",
        "!pip install --upgrade --no-cache-dir pip setuptools wheel ninja packaging cmake jedi\n",
        "# Pin numpy/scipy/sklearn to avoid ABI issues on Colab (Py3.12)\n",
        "!pip install --force-reinstall --no-cache-dir numpy==2.1.4 scipy==1.13.1 scikit-learn==1.5.2 transformers==4.43.4 datasets==2.20.0 matplotlib==3.8.4\n",
        "!pip install --force-reinstall --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "%env FORCE_CUDA=1\n",
        "%env MAX_JOBS=4\n",
        "%env TORCH_CUDA_ARCH_LIST=\"7.5;8.0;8.6\"\n",
        "!pip install --no-cache-dir --no-build-isolation mamba-ssm==2.2.2 --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repo setup (clone if needed, add to sys.path)\n",
        "import os, sys, subprocess, pathlib\n",
        "REPO_URL = 'https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git'\n",
        "REPO_DIR = 'Project-ResNet-BK-An-O-N-Language-Model-Architecture'\n",
        "cwd = pathlib.Path.cwd()\n",
        "candidates = [cwd, cwd.parent, cwd / REPO_DIR, cwd.parent / REPO_DIR]\n",
        "root = next((p for p in candidates if (p / 'src').exists()), None)\n",
        "if root is None:\n    root = cwd / REPO_DIR\n    if not root.exists():\n        subprocess.run(['git', 'clone', REPO_URL, str(root)], check=True)\n",
        "if root != pathlib.Path.cwd():\n    os.chdir(root)\n",
        "root_str = str(pathlib.Path.cwd())\n",
        "if root_str not in sys.path:\n    sys.path.insert(0, root_str)\n",
        "print('PWD:', root_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from types import SimpleNamespace\n",
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "\n",
        "def param_count_resnetbk(vocab_size, d_model, n_layers, n_seq):\n",
        "    model = ResNetBK(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_seq=n_seq, num_experts=4, top_k=1, dropout_p=0.1, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def estimate_activation_memory(batch_size, seq_length, d_model, bytes_per_elem=2):\n",
        "    tokens = batch_size * seq_length\n",
        "    return tokens * d_model * bytes_per_elem\n",
        "\n",
        "def estimate_total_memory(params, bytes_per_param=2, activation_bytes=0):\n",
        "    return params * bytes_per_param + activation_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute example sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 50000\n",
        "configs = [\n",
        "    {\"name\": \"baseline_small\", \"d_model\": 256, \"n_layers\": 6, \"n_seq\": 2048},\n",
        "    {\"name\": \"mid\", \"d_model\": 512, \"n_layers\": 16, \"n_seq\": 8192},\n",
        "    {\"name\": \"target_10b\", \"d_model\": 2048, \"n_layers\": 48, \"n_seq\": 32768},\n",
        "]\n",
        "estimates = []\n",
        "for cfg in configs:\n",
        "    params = param_count_resnetbk(vocab_size, cfg[\"d_model\"], cfg[\"n_layers\"], cfg[\"n_seq\"])\n",
        "    act_mem = estimate_activation_memory(batch_size=1, seq_length=cfg[\"n_seq\"], d_model=cfg[\"d_model\"], bytes_per_elem=2)\n",
        "    total_mem = estimate_total_memory(params, bytes_per_param=2, activation_bytes=act_mem)\n",
        "    estimates.append({\"config\": cfg, \"params\": params, \"activation_bytes\": act_mem, \"total_bytes_fp16\": total_mem})\n",
        "print(json.dumps(estimates, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meta-device shape check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "\n",
        "with torch.device(\"meta\"):\n",
        "    cfg = SimpleNamespace(vocab_size=50000, d_model=2048, n_layers=48, n_seq=32768, num_experts=4, top_k=1, dropout_p=0.1, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    model_meta = ResNetBK(vocab_size=cfg.vocab_size, d_model=cfg.d_model, n_layers=cfg.n_layers, n_seq=cfg.n_seq, num_experts=cfg.num_experts, top_k=cfg.top_k, dropout_p=cfg.dropout_p, use_scattering_router=cfg.use_scattering_router, use_birman_schwinger=cfg.use_birman_schwinger)\n",
        "    print(\"Meta model constructed (no real memory)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What would be needed on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "requirements = {\n",
        "    \"mixed_precision\": \"fp16 or bf16 mandatory\",\n",
        "    \"activation_checkpointing\": True,\n",
        "    \"gradient_accumulation\": True,\n",
        "    \"offload\": \"ZeRO/FSDP-style sharding not available on stock Colab\",\n",
        "    \"multi_gpu\": \"Colab free tier is single T4; 10B would need multi-GPU or heavy offload\",\n",
        "    \"long_context\": \"1M tokens would require chunking/state saving; not feasible on single T4\",\n",
        "}\n",
        "print(json.dumps(requirements, indent=2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}