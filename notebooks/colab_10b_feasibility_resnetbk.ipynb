{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semiseparable 10B Parameter Feasibility (Colab analysis)\n",
        "\n",
        "Estimates param counts and memory, plus meta-device shape check. Does NOT train 10B on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess, math, json, torch\n",
        "from types import SimpleNamespace\n",
        "\n",
        "REPO_URL = \"https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git\"\n",
        "REPO_DIR = \"Project-ResNet-BK-An-O-N-Language-Model-Architecture\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n",
        "os.chdir(REPO_DIR)\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.append(os.getcwd())\n",
        "\n",
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def param_count_resnetbk(vocab_size, d_model, n_layers, n_seq):\n",
        "    model = ResNetBK(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_seq=n_seq, num_experts=4, top_k=1, dropout_p=0.1, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def estimate_activation_memory(batch_size, seq_length, d_model, bytes_per_elem=2):\n",
        "    tokens = batch_size * seq_length\n",
        "    return tokens * d_model * bytes_per_elem\n",
        "\n",
        "def estimate_total_memory(params, bytes_per_param=2, activation_bytes=0):\n",
        "    return params * bytes_per_param + activation_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute example sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 50000\n",
        "configs = [\n",
        "    {\"name\": \"baseline_small\", \"d_model\": 256, \"n_layers\": 6, \"n_seq\": 2048},\n",
        "    {\"name\": \"mid\", \"d_model\": 512, \"n_layers\": 16, \"n_seq\": 8192},\n",
        "    {\"name\": \"target_10b\", \"d_model\": 2048, \"n_layers\": 48, \"n_seq\": 32768},\n",
        "]\n",
        "estimates = []\n",
        "for cfg in configs:\n",
        "    params = param_count_resnetbk(vocab_size, cfg[\"d_model\"], cfg[\"n_layers\"], cfg[\"n_seq\"])\n",
        "    act_mem = estimate_activation_memory(batch_size=1, seq_length=cfg[\"n_seq\"], d_model=cfg[\"d_model\"], bytes_per_elem=2)\n",
        "    total_mem = estimate_total_memory(params, bytes_per_param=2, activation_bytes=act_mem)\n",
        "    estimates.append({\"config\": cfg, \"params\": params, \"activation_bytes\": act_mem, \"total_bytes_fp16\": total_mem})\n",
        "print(json.dumps(estimates, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meta-device shape check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.device(\"meta\"):\n",
        "    cfg = SimpleNamespace(vocab_size=50000, d_model=2048, n_layers=48, n_seq=32768, num_experts=4, top_k=1, dropout_p=0.1, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    model_meta = ResNetBK(vocab_size=cfg.vocab_size, d_model=cfg.d_model, n_layers=cfg.n_layers, n_seq=cfg.n_seq, num_experts=cfg.num_experts, top_k=cfg.top_k, dropout_p=cfg.dropout_p, use_scattering_router=cfg.use_scattering_router, use_birman_schwinger=cfg.use_birman_schwinger)\n",
        "    print(\"Meta model constructed (no real memory)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What would be needed on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "requirements = {\n",
        "    \"mixed_precision\": \"fp16 or bf16 mandatory\",\n",
        "    \"activation_checkpointing\": True,\n",
        "    \"gradient_accumulation\": True,\n",
        "    \"offload\": \"ZeRO/FSDP-style sharding not available on stock Colab\",\n",
        "    \"multi_gpu\": \"Colab free tier is single T4; 10B would need multi-GPU or heavy offload\",\n",
        "    \"long_context\": \"1M tokens would require chunking/state saving; not feasible on single T4\",\n",
        "}\n",
        "print(json.dumps(requirements, indent=2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}