{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-BK vs Mamba: Long-Context Stability (Colab)\n",
        "\n",
        "Two-mode run: \n",
        "- Step1 (baseline): USE_THEORY=False \u2192 check if ResNet-BK is at least as stable as Mamba.\n",
        "- Step2 (theory ON): USE_THEORY=True \u2192 show the gap widens with scattering/Birman-Schwinger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prerequisites: GPU runtime. If using Colab, set Runtime -> Change runtime type -> GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stable install: Torch 2.3.1 cu121 + deps pinned for transformers/sklearn/scipy\n",
        "# If you just upgraded pip/setuptools, restart runtime once before running below.\n",
        "!pip install --upgrade --no-cache-dir pip setuptools wheel ninja packaging cmake jedi\n",
        "# Pin numpy/scipy/sklearn to avoid ABI issues on Colab (Py3.12)\n",
        "!pip install --force-reinstall --no-cache-dir numpy==2.1.4 scipy==1.13.1 scikit-learn==1.5.2 transformers==4.43.4 datasets==2.20.0 matplotlib==3.8.4\n",
        "!pip install --force-reinstall --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "%env FORCE_CUDA=1\n",
        "%env MAX_JOBS=4\n",
        "%env TORCH_CUDA_ARCH_LIST=\"7.5;8.0;8.6\"\n",
        "!pip install --no-cache-dir --no-build-isolation mamba-ssm==2.2.2 --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform, torch\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q datasets transformers matplotlib seaborn tqdm\n",
        "!pip install -q mamba-ssm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clone repo and add to path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repo setup (clone if needed, add to sys.path)\n",
        "import os, sys, subprocess, pathlib\n",
        "REPO_URL = 'https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git'\n",
        "REPO_DIR = 'Project-ResNet-BK-An-O-N-Language-Model-Architecture'\n",
        "cwd = pathlib.Path.cwd()\n",
        "candidates = [cwd, cwd.parent, cwd / REPO_DIR, cwd.parent / REPO_DIR]\n",
        "root = next((p for p in candidates if (p / 'src').exists()), None)\n",
        "if root is None:\n    root = cwd / REPO_DIR\n    if not root.exists():\n        subprocess.run(['git', 'clone', REPO_URL, str(root)], check=True)\n",
        "if root != pathlib.Path.cwd():\n    os.chdir(root)\n",
        "root_str = str(pathlib.Path.cwd())\n",
        "if root_str not in sys.path:\n    sys.path.insert(0, root_str)\n",
        "print('PWD:', root_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports and shared helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools, json, random, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuration (toggle theory ON/OFF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toggle for theory features (scattering router + Birman-Schwinger)\n",
        "USE_THEORY = False  # Step1: keep False. Step2: set True and re-run the notebook.\n",
        "RUN_TAG = \"theory_on\" if USE_THEORY else \"vanilla\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"dataset\": {\"name\": \"wikitext\", \"config\": \"wikitext-2-raw-v1\"},\n",
        "    \"tokenizer_name\": \"gpt2\",\n",
        "    \"training\": {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"min_lr\": 1e-5,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"max_steps\": 200,\n",
        "        \"log_every\": 20,\n",
        "        \"grad_clip\": 1.0,\n",
        "        \"batch_size\": 2,\n",
        "        \"seed\": 42,\n",
        "        \"use_amp\": True,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"d_model\": 256,\n",
        "        \"n_layers\": 6,\n",
        "        \"num_experts\": 4,\n",
        "        \"top_k\": 1,\n",
        "        \"dropout\": 0.1,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Default sweep includes 8k/32k/131k\n",
        "SEQ_LENGTHS = [8192, 32768, 131072]\n",
        "SEEDS = [42, 43, 44]\n",
        "PER_SEQ_STEPS = {8192: 200, 32768: 120, 131072: 60}\n",
        "SAVE_PLOTS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "_tokenizer = None\n",
        "\n",
        "def get_tokenizer():\n",
        "    global _tokenizer\n",
        "    if _tokenizer is None:\n",
        "        tok = AutoTokenizer.from_pretrained(BASE_CONFIG[\"tokenizer_name\"])\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        _tokenizer = tok\n",
        "    return _tokenizer\n",
        "\n",
        "@lru_cache()\n",
        "def load_lm_dataset(seq_length):\n",
        "    tokenizer = get_tokenizer()\n",
        "    raw = load_dataset(BASE_CONFIG[\"dataset\"][\"name\"], BASE_CONFIG[\"dataset\"][\"config\"])\n",
        "    def tok_fn(examples):\n",
        "        return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "    tokenized = raw[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
        "    seq_plus_one = seq_length + 1\n",
        "    def group_texts(examples):\n",
        "        concatenated = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
        "        total_length = len(concatenated) // seq_plus_one * seq_plus_one\n",
        "        concatenated = concatenated[:total_length]\n",
        "        result = [concatenated[i:i+seq_plus_one] for i in range(0, total_length, seq_plus_one)]\n",
        "        return {\"input_ids\": result}\n",
        "    grouped = tokenized.map(group_texts, batched=True, remove_columns=tokenized[\"train\"].column_names)\n",
        "    grouped.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "    return grouped[\"train\"]\n",
        "\n",
        "def make_dataloader(seq_length, batch_size, seed):\n",
        "    dataset = load_lm_dataset(seq_length)\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    def collate(batch):\n",
        "        inputs = torch.stack([b[\"input_ids\"][:-1] for b in batch])\n",
        "        targets = torch.stack([b[\"input_ids\"][1:] for b in batch])\n",
        "        return inputs, targets\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, generator=g, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model builders (respect USE_THEORY toggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config\n",
        "\n",
        "def build_resnetbk(seq_length):\n",
        "    cfg = BASE_CONFIG[\"model\"]\n",
        "    tok = get_tokenizer()\n",
        "    return ResNetBK(\n",
        "        vocab_size=tok.vocab_size,\n",
        "        d_model=cfg[\"d_model\"],\n",
        "        n_layers=cfg[\"n_layers\"],\n",
        "        n_seq=seq_length,\n",
        "        num_experts=cfg[\"num_experts\"],\n",
        "        top_k=cfg[\"top_k\"],\n",
        "        dropout_p=cfg[\"dropout\"],\n",
        "        use_scattering_router=USE_THEORY,\n",
        "        use_birman_schwinger=USE_THEORY,\n",
        "    )\n",
        "\n",
        "def build_mamba(seq_length):\n",
        "    cfg = BASE_CONFIG[\"model\"]\n",
        "    tok = get_tokenizer()\n",
        "    resnet_cfg = SimpleNamespace(\n",
        "        vocab_size=tok.vocab_size,\n",
        "        d_model=cfg[\"d_model\"],\n",
        "        n_layers=cfg[\"n_layers\"],\n",
        "        n_seq=seq_length,\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        tie_weights=True,\n",
        "    )\n",
        "    mamba_cfg = create_mamba_from_resnetbk_config(resnet_cfg)\n",
        "    return MambaLM(mamba_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one(model_name, builder, seq_length, max_steps, seed):\n",
        "    cfg = BASE_CONFIG[\"training\"]\n",
        "    batch_size = cfg[\"batch_size\"]\n",
        "    if seq_length >= 32768:\n",
        "        batch_size = max(1, batch_size // 2)\n",
        "    set_seed(seed)\n",
        "    dataloader = make_dataloader(seq_length, batch_size=batch_size, seed=seed)\n",
        "    model = builder(seq_length).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"learning_rate\"], betas=(0.9, 0.999), weight_decay=cfg[\"weight_decay\"])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps, eta_min=cfg[\"min_lr\"])\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"use_amp\"] and DEVICE == \"cuda\")\n",
        "    losses = []\n",
        "    wall_start = time.time()\n",
        "    for step, (inputs, targets) in enumerate(dataloader):\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=cfg[\"use_amp\"] and DEVICE == \"cuda\"):\n",
        "            logits = model(inputs)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"{model_name} divergence at step {step+1} loss={loss.item():.4f}\")\n",
        "            break\n",
        "        scaler.scale(loss).backward()\n",
        "        if cfg[\"grad_clip\"]:\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
        "        scaler.step(opt); scaler.update(); scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "        if (step + 1) % cfg[\"log_every\"] == 0:\n",
        "            print(f\"{model_name} step {step+1}/{max_steps} loss={loss.item():.4f} lr={scheduler.get_last_lr()[0]:.2e} bs={batch_size} seq={seq_length} seed={seed} tag={RUN_TAG}\")\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"seq_length\": seq_length,\n",
        "        \"losses\": losses,\n",
        "        \"steps\": len(losses),\n",
        "        \"batch_size\": batch_size,\n",
        "        \"seed\": seed,\n",
        "        \"run_tag\": RUN_TAG,\n",
        "        \"use_theory\": USE_THEORY,\n",
        "        \"wall_clock_sec\": time.time() - wall_start,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run experiments across seeds and sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        def run_headtohead(seq_lengths=None, seeds=None):\n",
        "            seq_lengths = seq_lengths or SEQ_LENGTHS\n",
        "            seeds = seeds or SEEDS\n",
        "            all_results = {}\n",
        "            for seq_len in seq_lengths:\n",
        "                max_steps = PER_SEQ_STEPS.get(seq_len, BASE_CONFIG[\"training\"][\"max_steps\"])\n",
        "                print(f\"\n",
        "=== Sequence length {seq_len} | steps {max_steps} | seeds {seeds} | tag {RUN_TAG} ===\")\n",
        "                seed_results = []\n",
        "                for seed in seeds:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"-- seed {seed} | ResNet-BK\")\n",
        "                    resnet_result = train_one(\"resnet_bk\", build_resnetbk, seq_len, max_steps, seed)\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"-- seed {seed} | Mamba\")\n",
        "                    mamba_result = train_one(\"mamba\", build_mamba, seq_len, max_steps, seed)\n",
        "                    seed_results.append({\"seed\": seed, \"resnet_bk\": resnet_result, \"mamba\": mamba_result})\n",
        "                all_results[seq_len] = seed_results\n",
        "                out_path = Path(f\"colab_long_context_{seq_len}_{RUN_TAG}_seeds.json\")\n",
        "                out_path.write_text(json.dumps(all_results[seq_len], indent=2))\n",
        "                print(\"Saved\", out_path)\n",
        "            return all_results\n",
        "\n",
        "        results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run (defaults include 8k/32k/131k and seeds 42,43,44)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step1 (baseline): USE_THEORY=False \u2192 run below\n",
        "# results = run_headtohead()\n",
        "\n",
        "# Step2 (theory ON): set USE_THEORY=True in the config cell above, re-run all cells, then run below\n",
        "# results = run_headtohead()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot losses (per-seed overlays, saved with tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_losses(results_dict, seq_length, save_fig=SAVE_PLOTS):\n",
        "    data = results_dict[seq_length]\n",
        "    plt.figure(figsize=(10,5))\n",
        "    for entry in data:\n",
        "        seed = entry[\"seed\"]\n",
        "        for name, color in [(\"resnet_bk\", \"blue\"), (\"mamba\", \"red\")]:\n",
        "            losses = entry[name][\"losses\"]\n",
        "            steps = range(1, len(losses)+1)\n",
        "            plt.plot(steps, losses, label=f\"{name}-seed{seed}-{RUN_TAG}\", color=color, alpha=0.4 if name==\"mamba\" else 0.7)\n",
        "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(f\"Seq {seq_length} ({RUN_TAG})\"); plt.legend(); plt.grid(alpha=0.3)\n",
        "    if save_fig:\n",
        "        fname = f\"loss_{seq_length}_{RUN_TAG}.png\"\n",
        "        plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n",
        "        print(\"Saved\", fname)\n",
        "    plt.show()\n",
        "\n",
        "# After running:\n",
        "# plot_losses(results, 8192)\n",
        "# plot_losses(results, 32768)\n",
        "# plot_losses(results, 131072)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zip artifacts (JSON/PNG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil, glob\n",
        "def zip_artifacts(prefix=None):\n",
        "    prefix = prefix or f\"artifacts_long_context_{RUN_TAG}\"\n",
        "    targets = glob.glob(f\"colab_long_context_*_{RUN_TAG}_seeds.json\") + glob.glob(f\"loss_*_{RUN_TAG}.png\")\n",
        "    if not targets:\n",
        "        print(\"No artifacts found yet.\")\n",
        "        return\n",
        "    shutil.make_archive(prefix, \"zip\", \".\")\n",
        "    print(\"Created\", f\"{prefix}.zip\", \"with\", targets)\n",
        "\n",
        "# After plots:\n",
        "# zip_artifacts()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}