{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-BK Efficiency: GradientCache (ACT proxy) vs Mamba\n",
        "\n",
        "Toy-scale FLOPs estimate with/without GradientCache as ACT proxy. Saves JSON + optional ZIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q datasets transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess\n",
        "REPO_URL = \"https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git\"\n",
        "REPO_DIR = \"Project-ResNet-BK-An-O-N-Language-Model-Architecture\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n",
        "os.chdir(REPO_DIR)\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.append(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools, json, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "\n",
        "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
        "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config\n",
        "from src.training.gradient_caching import GradientCache\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "RUN_TAG = \"efficiency\"\n",
        "BASE_CONFIG = {\"tokenizer_name\": \"gpt2\", \"seq_length\": 2048, \"train_steps\": 40, \"batch_size\": 2, \"lr\": 3e-4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_tokenizer():\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_CONFIG[\"tokenizer_name\"])\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "\n",
        "def make_loader(seq_length, batch_size, seed):\n",
        "    tok = get_tokenizer()\n",
        "    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "    def tok_fn(examples):\n",
        "        return tok(examples[\"text\"], add_special_tokens=False)\n",
        "    tokenized = raw[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
        "    seq_plus_one = seq_length + 1\n",
        "    def group(examples):\n",
        "        concat = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
        "        total = len(concat) // seq_plus_one * seq_plus_one\n",
        "        concat = concat[:total]\n",
        "        return {\"input_ids\": [concat[i:i+seq_plus_one] for i in range(0, total, seq_plus_one)]}\n",
        "    grouped = tokenized.map(group, batched=True, remove_columns=tokenized[\"train\"].column_names)\n",
        "    grouped.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    def collate(batch):\n",
        "        inputs = torch.stack([b[\"input_ids\"][:-1] for b in batch])\n",
        "        targets = torch.stack([b[\"input_ids\"][1:] for b in batch])\n",
        "        return inputs, targets\n",
        "    return DataLoader(grouped[\"train\"], batch_size=batch_size, shuffle=True, drop_last=True, generator=g, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_models(seq_length, vocab_size):\n",
        "    d_model = 256; n_layers = 6; dropout = 0.1\n",
        "    bk_base = ResNetBK(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_seq=seq_length, num_experts=4, top_k=1, dropout_p=dropout, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    bk_act = ResNetBK(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_seq=seq_length, num_experts=4, top_k=1, dropout_p=dropout, use_scattering_router=False, use_birman_schwinger=False)\n",
        "    res_cfg = SimpleNamespace(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_seq=seq_length, dropout=dropout, tie_weights=True)\n",
        "    mamba = MambaLM(create_mamba_from_resnetbk_config(res_cfg))\n",
        "    return bk_base, bk_act, mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_profile(model, loader, use_act=False):\n",
        "    steps = BASE_CONFIG[\"train_steps\"]\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_CONFIG[\"lr\"])\n",
        "    losses = []\n",
        "    flop_samples = []\n",
        "    model.train()\n",
        "    for step, (inp, tgt) in enumerate(loader):\n",
        "        if step >= steps:\n",
        "            break\n",
        "        inp = inp.to(DEVICE); tgt = tgt.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        def forward_pass(x):\n",
        "            return model(x)\n",
        "        if use_act:\n",
        "            cache = GradientCache(chunk_size=max(1, inp.shape[1]//2))\n",
        "            logits = cache(forward_pass, inp)\n",
        "        else:\n",
        "            logits = forward_pass(inp)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
        "        loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "        with torch.autograd.profiler.profile(enabled=True, use_cuda=DEVICE==\"cuda\") as prof:\n",
        "            _ = forward_pass(inp)\n",
        "        flops = sum(e.flops or 0 for e in prof.function_events)\n",
        "        flop_samples.append(flops)\n",
        "        if (step+1) % 10 == 0:\n",
        "            print(\"step\", step+1, \"loss\", loss.item(), \"flops\", flops)\n",
        "    avg_flops = float(sum(flop_samples)/max(1,len(flop_samples)))\n",
        "    return {\"losses\": losses, \"avg_flops\": avg_flops}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run efficiency exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = make_loader(BASE_CONFIG[\"seq_length\"], BASE_CONFIG[\"batch_size\"], seed=42)\n",
        "tok = get_tokenizer()\n",
        "bk_base, bk_act, mamba = build_models(BASE_CONFIG[\"seq_length\"], tok.vocab_size)\n",
        "\n",
        "res_base = train_and_profile(bk_base, loader, use_act=False)\n",
        "res_act = train_and_profile(bk_act, loader, use_act=True)\n",
        "res_mamba = train_and_profile(mamba, loader, use_act=False)\n",
        "\n",
        "results = {\"resnet_bk\": res_base, \"resnet_bk_act\": res_act, \"mamba\": res_mamba}\n",
        "Path(f\"colab_efficiency_results_{RUN_TAG}.json\").write_text(json.dumps(results, indent=2))\n",
        "print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zip artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil, glob\n",
        "\n",
        "def zip_artifacts(prefix=None):\n",
        "    prefix = prefix or f\"artifacts_efficiency_{RUN_TAG}\"\n",
        "    targets = glob.glob(f\"colab_efficiency_results_{RUN_TAG}.json\")\n",
        "    if not targets:\n",
        "        print(\"No artifacts found yet.\")\n",
        "        return\n",
        "    shutil.make_archive(prefix, \"zip\", \".\")\n",
        "    print(\"Created\", f\"{prefix}.zip\", \"with\", targets)\n",
        "\n",
        "# After running:\n",
        "# zip_artifacts()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}