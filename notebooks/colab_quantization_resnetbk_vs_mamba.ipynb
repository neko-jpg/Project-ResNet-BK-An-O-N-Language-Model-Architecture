{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-BK vs Mamba: Quantization (INT8 / Fake INT4)\n",
    "\n",
    "- Runs a small train, then evaluates PPL in FP32/INT8/fake-INT4.\n",
    "- Fake INT4 is per-tensor clipping on Linear weights (reference only).\n",
    "- Outputs JSON + optional ZIP of artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites: GPU runtime recommended for speed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable install: Torch 2.3.1 cu121 + mamba-ssm build-friendly\n",
    "# If you just upgraded pip/setuptools, restart runtime once before running below.\n",
    "!pip install --upgrade --no-cache-dir pip setuptools wheel ninja packaging cmake jedi\n",
    "!pip install --force-reinstall --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# Prepare a minimal build path for mamba-ssm (no heredoc)\n",
    "%env FORCE_CUDA=1\n",
    "%env MAX_JOBS=4\n",
    "%env TORCH_CUDA_ARCH_LIST=\"7.5;8.0;8.6\"\n",
    "!pip install --no-cache-dir --no-build-isolation mamba-ssm==2.2.2 --extra-index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform, torch\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q datasets transformers matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo setup (clone if needed, add to sys.path)\n",
    "import os, sys, subprocess, pathlib\n",
    "REPO_URL = 'https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture.git'\n",
    "REPO_DIR = 'Project-ResNet-BK-An-O-N-Language-Model-Architecture'\n",
    "cwd = pathlib.Path.cwd()\n",
    "candidates = [cwd, cwd.parent, cwd / REPO_DIR, cwd.parent / REPO_DIR]\n",
    "root = next((p for p in candidates if (p / 'src').exists()), None)\n",
    "if root is None:\n    root = cwd / REPO_DIR\n    if not root.exists():\n        subprocess.run(['git', 'clone', REPO_URL, str(root)], check=True)\n",
    "if root != pathlib.Path.cwd():\n    os.chdir(root)\n",
    "root_str = str(pathlib.Path.cwd())\n",
    "if root_str not in sys.path:\n    sys.path.insert(0, root_str)\n",
    "print('PWD:', root_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, json, random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.resnet_bk import LanguageModel as ResNetBK\n",
    "from src.models.mamba_baseline import MambaLM, create_mamba_from_resnetbk_config\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RUN_TAG = \"quant\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"dataset\": {\"name\": \"wikitext\", \"config\": \"wikitext-2-raw-v1\"},\n",
    "    \"model\": {\"d_model\": 256, \"n_layers\": 4, \"num_experts\": 2, \"top_k\": 1, \"dropout\": 0.1},\n",
    "    \"training\": {\"steps\": 80, \"lr\": 3e-4, \"weight_decay\": 0.01, \"batch_size\": 2, \"seed\": 42},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_CONFIG[\"tokenizer_name\"])\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_data(seq_length):\n",
    "    tok = get_tokenizer()\n",
    "    raw = load_dataset(BASE_CONFIG[\"dataset\"][\"name\"], BASE_CONFIG[\"dataset\"][\"config\"])\n",
    "    def tok_fn(examples):\n",
    "        return tok(examples[\"text\"], add_special_tokens=False)\n",
    "    tokenized = raw[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "    seq_plus_one = seq_length + 1\n",
    "    def group(examples):\n",
    "        concat = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n",
    "        total = len(concat) // seq_plus_one * seq_plus_one\n",
    "        concat = concat[:total]\n",
    "        return {\"input_ids\": [concat[i:i+seq_plus_one] for i in range(0, total, seq_plus_one)]}\n",
    "    grouped = tokenized.map(group, batched=True, remove_columns=tokenized[\"train\"].column_names)\n",
    "    grouped.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "    return grouped[\"train\"], raw[\"validation\"], tok\n",
    "\n",
    "def make_loader(dataset, batch_size, seed):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    def collate(batch):\n",
    "        inputs = torch.stack([b[\"input_ids\"][:-1] for b in batch])\n",
    "        targets = torch.stack([b[\"input_ids\"][1:] for b in batch])\n",
    "        return inputs, targets\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, generator=g, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(seq_length, vocab_size):\n",
    "    cfg = BASE_CONFIG[\"model\"]\n",
    "    bk = ResNetBK(vocab_size=vocab_size, d_model=cfg[\"d_model\"], n_layers=cfg[\"n_layers\"], n_seq=seq_length, num_experts=cfg[\"num_experts\"], top_k=cfg[\"top_k\"], dropout_p=cfg[\"dropout\"], use_scattering_router=False, use_birman_schwinger=False)\n",
    "    res_cfg = SimpleNamespace(vocab_size=vocab_size, d_model=cfg[\"d_model\"], n_layers=cfg[\"n_layers\"], n_seq=seq_length, dropout=cfg[\"dropout\"], tie_weights=True)\n",
    "    mb_cfg = create_mamba_from_resnetbk_config(res_cfg)\n",
    "    mb = MambaLM(mb_cfg)\n",
    "    return bk, mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_small(model, loader):\n",
    "    cfg = BASE_CONFIG[\"training\"]\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for step, (inp, tgt) in enumerate(loader):\n",
    "        if step >= cfg[\"steps\"]:\n",
    "            break\n",
    "        inp = inp.to(DEVICE); tgt = tgt.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "        loss.backward(); opt.step()\n",
    "        losses.append(loss.item())\n",
    "        if (step+1) % 20 == 0:\n",
    "            print(\"step\", step+1, \"loss\", loss.item())\n",
    "    return losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ppl(model, loader):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    total_loss = 0; total_tokens = 0\n",
    "    for inp, tgt in loader:\n",
    "        inp = inp.to(DEVICE); tgt = tgt.to(DEVICE)\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += tgt.numel()\n",
    "        if total_tokens > 200000:\n",
    "            break\n",
    "    return float(torch.exp(torch.tensor(total_loss / total_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8_linear(model):\n",
    "    import torch.ao.quantization as tq\n",
    "    qmodel = tq.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    return qmodel\n",
    "\n",
    "def quantize_int4_linear(model):\n",
    "    import copy\n",
    "    qmodel = copy.deepcopy(model).cpu()\n",
    "    for _, mod in qmodel.named_modules():\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            w = mod.weight.data\n",
    "            scale = w.abs().max() / 7.0 + 1e-8\n",
    "            q = torch.clamp(torch.round(w / scale), -8, 7)\n",
    "            mod.weight.data = (q * scale).to(mod.weight.dtype)\n",
    "    return qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 2048\n",
    "train_data, val_raw, tok = load_data(seq_length)\n",
    "train_loader = make_loader(train_data, BASE_CONFIG[\"training\"][\"batch_size\"], BASE_CONFIG[\"training\"][\"seed\"])\n",
    "val_tokenized = val_raw.map(lambda ex: tok(ex[\"text\"], add_special_tokens=False), batched=True, remove_columns=[\"text\"])\n",
    "val_tokenized = val_tokenized.map(lambda ex: {\"input_ids\": [ids[:seq_length+1] for ids in ex[\"input_ids\"] if len(ids) >= seq_length+1]}, batched=True)\n",
    "val_tokenized = val_tokenized.filter(lambda ex: len(ex[\"input_ids\"]) > 0)\n",
    "val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "val_loader = make_loader(val_tokenized[\"validation\"], batch_size=1, seed=0)\n",
    "\n",
    "bk, mb = build_models(seq_length, tok.vocab_size)\n",
    "print(\"Training ResNet-BK\")\n",
    "train_small(bk, train_loader)\n",
    "print(\"Training Mamba\")\n",
    "train_small(mb, train_loader)\n",
    "\n",
    "print(\"Eval FP32\")\n",
    "ppl_bk_fp32 = eval_ppl(bk, val_loader)\n",
    "ppl_mb_fp32 = eval_ppl(mb, val_loader)\n",
    "\n",
    "print(\"Quantize INT8\")\n",
    "bk_int8 = quantize_int8_linear(bk)\n",
    "mb_int8 = quantize_int8_linear(mb)\n",
    "ppl_bk_int8 = eval_ppl(bk_int8, val_loader)\n",
    "ppl_mb_int8 = eval_ppl(mb_int8, val_loader)\n",
    "\n",
    "print(\"Quantize INT4 (fake-quant)\")\n",
    "bk_int4 = quantize_int4_linear(bk)\n",
    "mb_int4 = quantize_int4_linear(mb)\n",
    "ppl_bk_int4 = eval_ppl(bk_int4, val_loader)\n",
    "ppl_mb_int4 = eval_ppl(mb_int4, val_loader)\n",
    "\n",
    "results = {\n",
    "    \"seq_length\": seq_length,\n",
    "    \"ppl\": {\n",
    "        \"resnet_bk\": {\"fp32\": ppl_bk_fp32, \"int8\": ppl_bk_int8, \"int4_fake\": ppl_bk_int4},\n",
    "        \"mamba\": {\"fp32\": ppl_mb_fp32, \"int8\": ppl_mb_int8, \"int4_fake\": ppl_mb_int4},\n",
    "    },\n",
    "}\n",
    "Path(f\"colab_quant_results_{RUN_TAG}.json\").write_text(json.dumps(results, indent=2))\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob\n",
    "def zip_artifacts(prefix=None):\n",
    "    prefix = prefix or f\"artifacts_quant_{RUN_TAG}\"\n",
    "    targets = glob.glob(f\"colab_quant_results_{RUN_TAG}.json\")\n",
    "    if not targets:\n",
    "        print(\"No artifacts found yet.\")\n",
    "        return\n",
    "    shutil.make_archive(prefix, \"zip\", \".\")\n",
    "    print(\"Created\", f\"{prefix}.zip\", \"with\", targets)\n",
    "\n",
    "# After running:\n",
    "# zip_artifacts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}