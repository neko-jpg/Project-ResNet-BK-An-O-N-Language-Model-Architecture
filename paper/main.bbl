\begin{thebibliography}{10}

\bibitem{banino2021pondernet}
Andrea Banino, Jan Balaguer, and Charles Blundell.
\newblock Pondernet: Learning to ponder.
\newblock {\em arXiv preprint arXiv:2107.05407}, 2021.

\bibitem{birman1962spectral}
M~Sh Birman and MZ~Solomjak.
\newblock {\em Spectral theory of self-adjoint operators in Hilbert space}.
\newblock Springer, 1987.

\bibitem{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
  Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock {\em International Conference on Machine Learning}, 2022.

\bibitem{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em Journal of Machine Learning Research}, 23(120):1--39, 2022.

\bibitem{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{fu2023hungry}
Daniel~Y Fu, Tri Dao, Khaled~K Saab, Armin~W Thomas, Atri Rudra, and
  Christopher R{\'e}.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock {\em International Conference on Learning Representations}, 2023.

\bibitem{graves2016adaptive}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1603.08983}, 2016.

\bibitem{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and
  acceleration.
\newblock {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{martin2018implicit}
Charles~H Martin and Michael~W Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock {\em arXiv preprint arXiv:1810.01075}, 2018.

\bibitem{mehta2004random}
Madan~Lal Mehta.
\newblock {\em Random matrices}, volume 142.
\newblock Elsevier, 2004.

\bibitem{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi
  Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock {\em arXiv preprint arXiv:2305.13048}, 2023.

\bibitem{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y Fu, Tri Dao, Stephen
  Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R{\'e}.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock {\em International Conference on Machine Learning}, 2023.

\bibitem{schwinger1961brownian}
Julian Schwinger.
\newblock On the bound states of a given potential.
\newblock {\em Proceedings of the National Academy of Sciences},
  47(1):122--129, 1961.

\bibitem{sun2023retentive}
Yutao Sun, Li~Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong
  Wang, and Furu Wei.
\newblock Retentive network: A successor to transformer for large language
  models.
\newblock {\em arXiv preprint arXiv:2307.08621}, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\end{thebibliography}
