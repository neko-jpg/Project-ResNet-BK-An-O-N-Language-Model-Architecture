% References for Mamba-Killer ResNet-BK Paper

% ========================================
% Main Baselines
% ========================================

@article{gu2023mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{gu2022efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={International Conference on Learning Representations},
  year={2022}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  journal={International Conference on Machine Learning},
  year={2023}
}

@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@article{fu2023hungry,
  title={Hungry hungry hippos: Towards language modeling with state space models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={International Conference on Learning Representations},
  year={2023}
}

% ========================================
% Mathematical Foundations
% ========================================

@book{birman1962spectral,
  title={Spectral theory of self-adjoint operators in Hilbert space},
  author={Birman, M Sh and Solomjak, MZ},
  year={1987},
  publisher={Springer}
}

@article{schwinger1961brownian,
  title={On the bound states of a given potential},
  author={Schwinger, Julian},
  journal={Proceedings of the National Academy of Sciences},
  volume={47},
  number={1},
  pages={122--129},
  year={1961}
}

@book{reed1979methods,
  title={Methods of modern mathematical physics: Analysis of operators},
  author={Reed, Michael and Simon, Barry},
  volume={4},
  year={1979},
  publisher={Elsevier}
}

@article{mourre1981absence,
  title={Absence of singular continuous spectrum for certain selfadjoint operators},
  author={Mourre, Eric},
  journal={Communications in Mathematical Physics},
  volume={78},
  number={3},
  pages={391--408},
  year={1981}
}

@article{newton1982scattering,
  title={Scattering theory of waves and particles},
  author={Newton, Roger G},
  journal={Springer-Verlag, New York},
  year={1982}
}

@book{mehta2004random,
  title={Random matrices},
  author={Mehta, Madan Lal},
  volume={142},
  year={2004},
  publisher={Elsevier}
}

@article{weil1952number,
  title={Sur les "formules explicites" de la th{\'e}orie des nombres premiers},
  author={Weil, Andr{\'e}},
  journal={Communications du s{\'e}minaire math{\'e}matique de l'Universit{\'e} de Lund},
  pages={252--265},
  year={1952}
}

@article{martin2018implicit,
  title={Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning},
  author={Martin, Charles H and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1810.01075},
  year={2018}
}

% ========================================
% Mixture of Experts
% ========================================

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{du2022glam,
  title={GLaM: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={International Conference on Machine Learning},
  year={2022}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={International Conference on Learning Representations},
  year={2017}
}

% ========================================
% Adaptive Computation
% ========================================

@article{graves2016adaptive,
  title={Adaptive computation time for recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1603.08983},
  year={2016}
}

@article{banino2021pondernet,
  title={Pondernet: Learning to ponder},
  author={Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  journal={arXiv preprint arXiv:2107.05407},
  year={2021}
}

% ========================================
% Quantization
% ========================================

@article{frantar2022gptq,
  title={GPTQ: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={AWQ: Activation-aware weight quantization for LLM compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{dettmers2022llm,
  title={LLM.int8(): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

% ========================================
% Datasets
% ========================================

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

% ========================================
% Memory Optimization
% ========================================

@article{rajbhandari2020zero,
  title={ZeRO: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  journal={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

% ========================================
% Evaluation
% ========================================

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{wang2019superglue,
  title={SuperGLUE: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{rajpurkar2016squad,
  title={SQuAD: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={International Conference on Learning Representations},
  year={2021}
}

% ========================================
% Related Architectures
% ========================================

@article{dao2022flashattention,
  title={FlashAttention: Fast and memory-efficient exact attention with IO-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  journal={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020}
}

% ========================================
% Theoretical Foundations
% ========================================

@book{simon2005trace,
  title={Trace ideals and their applications},
  author={Simon, Barry},
  volume={120},
  year={2005},
  publisher={American Mathematical Soc.}
}

@article{cycon1987schrodinger,
  title={Schr{\"o}dinger operators with application to quantum mechanics and global geometry},
  author={Cycon, Hans L and Froese, Richard G and Kirsch, Werner and Simon, Barry},
  journal={Springer Study Edition. Texts and Monographs in Physics},
  year={1987}
}

@book{yafaev2010mathematical,
  title={Mathematical scattering theory: Analytic theory},
  author={Yafaev, Dimitri R},
  volume={158},
  year={2010},
  publisher={American Mathematical Soc.}
}
