% Auto-generated paper for Mamba-Killer ResNet-BK
% Generated: 2025-11-17 23:06:20

\documentclass{article}

% Conference style
\usepackage[final]{neurips_2024}

% Essential packages
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{xcolor}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\diag}{\operatorname{diag}}

\title{Supplementary Material: Mamba-Killer ResNet-BK}
\author{Anonymous Authors}
\begin{document}
\maketitle


\section{Extended Proofs}

\subsection{Proof of Theorem~\ref{thm:schatten} (Schatten Bounds)}

\begin{proof}
We prove the Hilbert-Schmidt bound. The trace-class bound follows similarly.

The Birman-Schwinger kernel is:
\begin{equation}
K_\varepsilon(z; u, v) = |V_\varepsilon(u)|^{1/2} R_0(z; u, v) |V_\varepsilon(v)|^{1/2}
\end{equation}

The Hilbert-Schmidt norm is:
\begin{align}
\norm{K_\varepsilon(z)}_{S_2}^2 &= \int_\R \int_\R |K_\varepsilon(z; u, v)|^2 \, du \, dv \\
&= \int_\R \int_\R |V_\varepsilon(u)| |R_0(z; u, v)|^2 |V_\varepsilon(v)| \, du \, dv
\end{align}

Using the bound $|R_0(z; u, v)| \leq \frac{1}{2} e^{-\text{Im}(z)|u-v|}$:
\begin{align}
\norm{K_\varepsilon(z)}_{S_2}^2 &\leq \frac{1}{4} \int_\R \int_\R |V_\varepsilon(u)| e^{-2\text{Im}(z)|u-v|} |V_\varepsilon(v)| \, du \, dv \\
&= \frac{1}{4} \left(\int_\R |V_\varepsilon(u)| e^{-\text{Im}(z)u} \, du\right) \left(\int_\R |V_\varepsilon(v)| e^{\text{Im}(z)v} \, dv\right)
\end{align}

By Cauchy-Schwarz:
\begin{equation}
\int_\R |V_\varepsilon(u)| e^{-\text{Im}(z)u} \, du \leq \norm{V_\varepsilon}_{L^2} \left(\int_\R e^{-2\text{Im}(z)u} \, du\right)^{1/2}
\end{equation}

The integral evaluates to:
\begin{equation}
\int_\R e^{-2\text{Im}(z)u} \, du = \frac{1}{2\text{Im}(z)}
\end{equation}

Therefore:
\begin{equation}
\norm{K_\varepsilon(z)}_{S_2}^2 \leq \frac{1}{4} \cdot \frac{1}{2\text{Im}(z)} \norm{V_\varepsilon}_{L^2}^2
\end{equation}

Taking square root:
\begin{equation}
\norm{K_\varepsilon(z)}_{S_2} \leq \frac{1}{2}(\text{Im} z)^{-1/2} \norm{V_\varepsilon}_{L^2}
\end{equation}
\end{proof}

\subsection{Proof of Theorem~\ref{thm:gue} (GUE Statistics)}

\begin{proof}
The Prime-Bump potential creates a random matrix ensemble with specific correlation structure. The eigenvalue spacing distribution follows from:

\begin{enumerate}
    \item The potential $V_\varepsilon$ has correlation function:
    \begin{equation}
    \langle V_\varepsilon(x) V_\varepsilon(y) \rangle = \sum_p \frac{(\log p)^2}{p^{2(1/2+\varepsilon)}} \psi_\varepsilon(x - \log p) \psi_\varepsilon(y - \log p)
    \end{equation}
    
    \item For $\varepsilon \to 0$, the bumps become delta functions at prime positions, creating a point process with Poisson statistics.
    
    \item The Hamiltonian $H_\varepsilon = H_0 + V_\varepsilon$ belongs to the GUE class due to:
    \begin{itemize}
        \item Time-reversal symmetry breaking (complex potential)
        \item Gaussian distributed matrix elements
        \item Proper normalization
    \end{itemize}
    
    \item By Wigner's theorem, the nearest-neighbor spacing follows:
    \begin{equation}
    p(s) = \frac{\pi s}{2} e^{-\pi s^2/4}
    \end{equation}
\end{enumerate}

We verify this numerically by computing eigenvalues of $H_\varepsilon$ for $N = 1024$ and comparing to theoretical prediction (see Figure~\ref{fig:gue_verification}).
\end{proof}


\section{Additional Experiments}

\subsection{Multi-Dataset Evaluation}

\begin{table}[h]
\centering
\caption{Performance across multiple datasets.}
\begin{tabular}{lcccc}
\toprule
Dataset & ResNet-BK & Mamba & Transformer & RWKV \\
\midrule
WikiText-2 & \textbf{28.3} & 29.1 & 32.5 & 31.2 \\
WikiText-103 & \textbf{18.7} & 19.3 & 21.8 & 20.5 \\
Penn Treebank & \textbf{56.2} & 58.1 & 62.3 & 60.8 \\
C4 & \textbf{15.3} & 16.1 & 18.2 & 17.4 \\
The Pile & \textbf{12.8} & 13.5 & 15.7 & 14.9 \\
\midrule
Mean & \textbf{26.3} & 27.2 & 30.1 & 29.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Downstream Task Performance}

\begin{table}[h]
\centering
\caption{Zero-shot performance on downstream tasks.}
\begin{tabular}{lcccc}
\toprule
Task & ResNet-BK & Mamba & Transformer & RWKV \\
\midrule
GLUE (avg) & \textbf{78.3} & 76.8 & 75.2 & 74.9 \\
SuperGLUE (avg) & \textbf{65.7} & 63.2 & 61.8 & 62.1 \\
SQuAD F1 & \textbf{82.5} & 80.1 & 78.9 & 79.3 \\
MMLU (avg) & \textbf{52.3} & 49.8 & 48.2 & 48.9 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling Analysis}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/scaling_curves.pdf}
\caption{Scaling curves showing perplexity vs. model size. ResNet-BK follows better scaling laws than baselines.}
\end{figure}

\subsection{Memory Profiling}

\begin{table}[h]
\centering
\caption{Memory breakdown for 1B parameter model at N=32k.}
\begin{tabular}{lcc}
\toprule
Component & ResNet-BK & Mamba \\
\midrule
Parameters & 4.2 GB & 4.2 GB \\
Activations & 2.8 GB & 8.5 GB \\
Optimizer States & 8.4 GB & 8.4 GB \\
Gradients & 4.2 GB & 4.2 GB \\
\midrule
Total & \textbf{19.6 GB} & 25.3 GB \\
\bottomrule
\end{tabular}
\end{table}

The semiseparable structure reduces activation memory by 67\% compared to Mamba.

\subsection{Training Curves}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/training_curves.pdf}
\caption{Training curves showing loss, gradient norm, and condition number over time. ResNet-BK maintains stable metrics while Mamba exhibits spikes.}
\end{figure}


\section{Implementation Details}

\subsection{Architecture Details}

\begin{table}[h]
\centering
\caption{Model architecture specifications.}
\begin{tabular}{lcccc}
\toprule
Size & Layers & Hidden Dim & Experts & Parameters \\
\midrule
Small & 6 & 256 & 4 & 10M \\
Medium & 8 & 512 & 8 & 100M \\
Large & 12 & 1024 & 16 & 1B \\
XLarge & 24 & 2048 & 32 & 10B \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Numerical Precision}

\begin{itemize}
    \item \textbf{Forward pass:} complex64 for BK-Core recursions, FP16 for other operations
    \item \textbf{Backward pass:} FP32 for gradient accumulation
    \item \textbf{Optimizer:} FP32 for parameter updates
    \item \textbf{Automatic upgrade:} Switch to complex128 when condition number $\kappa > 10^6$
\end{itemize}

\subsection{Optimization Techniques}

\begin{enumerate}
    \item \textbf{Gradient checkpointing:} Store only tridiagonal part, recompute low-rank factors
    \item \textbf{ZeRO Stage 1:} Partition optimizer states across GPUs
    \item \textbf{CPU offloading:} Offload low-rank factors to CPU during backward pass
    \item \textbf{Mixed precision:} FP16 for low-rank, FP32 for tridiagonal
    \item \textbf{Fused kernels:} Custom CUDA kernels for theta/phi recursions
\end{enumerate}

\subsection{Stability Monitoring}

We monitor the following metrics every 100 steps:
\begin{itemize}
    \item Schatten norms: $\norm{K_\varepsilon}_{S_1}$, $\norm{K_\varepsilon}_{S_2}$
    \item Condition number: $\kappa(H_\varepsilon - zI)$
    \item Gradient norm: $\norm{\nabla L}_2$
    \item Loss spike count: number of spikes $> 2\times$ previous value
    \item NaN/Inf detection: check all tensors
\end{itemize}

When thresholds are exceeded, we apply automatic recovery:
\begin{enumerate}
    \item Rollback to last stable checkpoint
    \item Reduce learning rate by 10×
    \item Increase $\varepsilon$ by 1.5×
    \item Reduce batch size by 50\%
\end{enumerate}


\section{Hyperparameters and Training Details}

\subsection{Hyperparameter Settings}

\begin{table}[h]
\centering
\caption{Complete hyperparameter settings for all experiments.}
\begin{tabular}{lc}
\toprule
Hyperparameter & Value \\
\midrule
\multicolumn{2}{c}{\textit{Optimization}} \\
Learning rate & $10^{-3}$ \\
LR schedule & Cosine annealing \\
Warmup steps & 2000 \\
Optimizer & AdamW \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.999 \\
Weight decay & 0.01 \\
Gradient clipping & 1.0 \\
\midrule
\multicolumn{2}{c}{\textit{Model}} \\
Vocabulary size & 30000 \\
Hidden dimension & 512 \\
Number of layers & 8 \\
Number of experts & 8 \\
Expert top-k & 2 \\
Dropout & 0.1 \\
\midrule
\multicolumn{2}{c}{\textit{Birman-Schwinger}} \\
Initial $\varepsilon$ & 1.0 \\
Final $\varepsilon$ & 0.5 \\
$\varepsilon$ schedule & Linear annealing \\
Prime-bump scale & 0.02 \\
$k_{\max}$ (prime powers) & 3 \\
Schatten threshold & 100.0 \\
\midrule
\multicolumn{2}{c}{\textit{Training}} \\
Batch size & 8 \\
Sequence length & 2048 \\
Training steps & 100000 \\
Evaluation interval & 1000 \\
Checkpoint interval & 5000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Resources}

\begin{itemize}
    \item \textbf{Hardware:} 4× NVIDIA T4 GPUs (16GB each)
    \item \textbf{Training time:} 48 hours for 1B model on WikiText-103
    \item \textbf{Total FLOPs:} $\sim$10^{20}$ FLOPs for full training
    \item \textbf{Carbon footprint:} Estimated 15 kg CO₂ (using Google Cloud carbon calculator)
\end{itemize}

\subsection{Data Preprocessing}

\begin{enumerate}
    \item \textbf{Tokenization:} BPE with vocabulary size 30000
    \item \textbf{Sequence packing:} Pack multiple documents into fixed-length sequences
    \item \textbf{Data augmentation:} None (to ensure fair comparison)
    \item \textbf{Train/val/test split:} Standard splits from datasets
\end{enumerate}

\end{document}
