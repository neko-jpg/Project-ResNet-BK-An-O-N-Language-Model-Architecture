# Phase 8 Maximum Configuration
# Hyperbolic Transcendence - 8GB VRAM対応
# 目標: ~3B parameters (30M minimum after compression)

model:
  d_model: 3072      # 3B設定に増強 (2048→3072)
  n_layers: 40       # 深いネットワーク (32→40)
  num_heads: 24      # d_model/128
  n_seq: 512         # メモリ効率のため512に抑える
  dropout_p: 0.1
  vocab_size: 50257  # GPT-2 vocab size
  
  # Phase 8 specific
  use_bk_hyperbolic: true
  use_ar_ssm_fusion: true
  curvature_initial: 0.01    # Low curvature for O(N) linear attention

training:
  batch_size: 1      # 3Bモデルなので1に戻す
  gradient_accumulation_steps: 32  # Effective batch size: 32
  epochs: 10
  learning_rate: 3.0e-5  # 大きいモデルなので学習率を下げる
  warmup_steps: 5000     # より長いウォームアップ
  weight_decay: 0.01
  max_grad_norm: 0.5
  
  # Mixed Precision (必須)
  mixed_precision: true
  fp16: true
  
  # Gradient Checkpointing (3Bモデルには必須)
  gradient_checkpointing: true
  
  # 8-bit Optimizer (メモリ削減) - 3Bモデルには推奨
  use_8bit_adam: true  # bitsandbytes使用

optimization:
  # Low-Rank Compression (必須 - 3Bを8GBに収めるため)
  low_rank_embedding: true  # 75% compression (d_model/4)
  low_rank_ffn: true        # 87.5% compression (d_model/8)
  
  # Memory Optimization
  use_flash_attention: false  # Use O(N) linear attention
  use_triton_kernels: true
  empty_cache_every: 3  # 3Bモデルなのでより頻繁に
  
  # Adaptive Computation
  adaptive_computation: true
  early_exit_threshold: 0.5

checkpoint:
  save_dir: checkpoints/phase8_max_3b
  save_every: 1000
  keep_last_n: 2

logging:
  log_every: 50
  eval_every: 500
  wandb: false

# Expected Memory Usage (8GB VRAM対応)
# Model Parameters: ~3.2B (3,200M) before compression
# After Low-Rank Compression: ~30-40M effective parameters
# Model (FP16): ~6.4 GB
# Optimizer States (8-bit): ~3.2 GB (通常の半分)
# Activations (with gradient checkpointing): ~2 GB
# Peak: ~7.8-8 GB (8GB VRAM ギリギリ)
