# Long-Context Stability Configuration
# For training on 8k-1M token sequences

# Inherit from base config
_base_: base_config.yaml

# Model Architecture - Long Context
model:
  n_seq: 8192  # Start with 8k, scale up to 128k-1M
  d_model: 512
  n_layers: 12
  
  # Enable all stability features
  use_mourre: true
  use_lap: true
  use_semiseparable: true
  use_cpu_offload: true  # For ultra-long sequences
  
  # Hierarchical semiseparable for memory efficiency
  low_rank: 13  # log2(8192)

# Training Configuration
training:
  batch_size: 2  # Smaller batch for long sequences
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-4  # Lower LR for stability
  max_grad_norm: 0.5  # Stricter gradient clipping
  
  # More aggressive checkpointing
  gradient_checkpointing: true
  mixed_precision: true
  
  # Longer training
  max_steps: 200000
  warmup_steps: 2000
  
  # Frequent evaluation for stability monitoring
  eval_interval: 500
  save_interval: 2000

# Data Configuration
data:
  dataset: "c4"  # C4 has longer documents
  data_dir: "./data"

# Monitoring - Enhanced for long-context
monitoring:
  use_wandb: true
  experiment_name: "long_context_stability"
  
  # Aggressive stability monitoring
  check_nan_inf: true
  check_interval: 50
  condition_number_threshold: 1.0e5  # Stricter threshold
  gradient_explosion_threshold: 5.0  # More sensitive

# Checkpointing - Keep more checkpoints for recovery
checkpoint:
  keep_last_n: 10
  save_optimizer: true
  save_scheduler: true

# Sequence length schedule (progressive training)
sequence_schedule:
  enabled: true
  lengths: [512, 2048, 8192, 32768, 131072]
  steps_per_length: [10000, 20000, 40000, 60000, 80000]
