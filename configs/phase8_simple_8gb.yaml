# Phase 8 Simple Configuration - 8GB VRAM対応
# 双曲幾何学を無効化し、標準的なTransformerとして動作
# 安定性を最優先

model:
  d_model: 512           # 小さめで安定
  n_layers: 12           # 適度なレイヤー数
  num_heads: 8
  max_seq_len: 512
  curvature: 0.0         # 双曲幾何学を無効化（ユークリッド空間）
  dropout: 0.1
  use_hyperbolic_ssm: false

training:
  batch_size: 4          # 小さいモデルなので増やせる
  gradient_accumulation_steps: 8  # Effective batch size: 32
  epochs: 10
  learning_rate: 3.0e-4  # 標準的な学習率
  warmup_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Mixed Precision
  mixed_precision: true
  fp16: true
  
  # Gradient Checkpointing（安定版）
  gradient_checkpointing: false  # 小さいモデルなので不要

optimization:
  # Low-Rank Compression
  low_rank_embedding: true
  low_rank_ffn: true
  
  # Memory Optimization
  use_flash_attention: false
  use_triton_kernels: false  # Tritonを無効化
  empty_cache_every: 10

checkpoint:
  save_dir: checkpoints/phase8_simple
  save_every: 1000
  keep_last_n: 3

logging:
  log_every: 50
  eval_every: 500
  wandb: false

# Expected Memory Usage
# Model Parameters: ~50M
# Model (FP16): ~100 MB
# Optimizer States: ~200 MB
# Activations: ~500 MB
# Peak: ~1-2 GB (8GB以内に余裕で収まる)
