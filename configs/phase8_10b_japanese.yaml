# Phase 8: 10B Japanese LLM - RTX 3080 8GB Optimized (v2)
# =========================================================
# 日本語特化モデル設定 - 最適化版

# === Model Architecture ===
d_model: 4096
n_layers: 48
n_seq: 512
num_heads: 32
htt_rank: 16
hyperbolic_window_size: 64

# === Japanese Tokenizer ===
vocab_size: 32000
tokenizer_name: "rinna/japanese-gpt-neox-3.6b"

# === Riemannian Resonant Tunneling ===
use_resonant_htt: true
resonant_num_cores: 4
use_zeta_init: true

# === Performance Optimization ===
compile: true  # torch.compile() for 10-20% speedup

# === Ultra Compression ===
low_rank_ffn: true
low_rank_attention: true
low_rank_rank: 16
use_bitnet: true

# === Memory Optimization ===
use_gradient_checkpointing: true
gradient_checkpointing_segments: 12
use_mixed_precision: true
mixed_precision_dtype: bfloat16

# === Training ===
batch_size: 1
gradient_accumulation_steps: 64
effective_batch_size: 8

# === Optimizer (BK-HyperSGD - ResNet-BK専用) ===
optimizer_type: adamw
beta1: 0.9
beta2: 0.95
eps: 1.0e-8
weight_decay: 0.1

# === Learning Rate Scheduler ===
# 2025-12-16: Reduced LR from 1.0 to 0.05 for stable training
learning_rate: 0.0001  # 0.001 -> 0.0001 (1e-4)
min_lr: 1.0e-6      # 1e-6
warmup_steps: 200   # 1-5%

# === Bootstrap LR (Shock Therapy) ===
# Use very high LR until loss decreases significantly
# Automatically exits when loss drops by bootstrap_exit_threshold
bootstrap_lr: 0.0  # Disabled
bootstrap_steps: 0  # Disabled
bootstrap_exit_threshold: 0.5

# === Gradient Control ===
# 2025-12-16: Stronger clipping to prevent spikes after phase transition
grad_clip_warmup: 0.1  # Stricter warmup clipping
grad_clip_train: 1.0   # Expert recommendation: allow more gradient flow
grad_skip_threshold: 10000.0  # Failsafe: 異常な勾配爆発をスキップ (10.0->10000.0 for 10B model)

# === EMA (Exponential Moving Average) ===
use_ema: true
ema_decay: 0.999

# === Regularization ===
label_smoothing: 0.1
dropout: 0.1
attention_dropout: 0.1

# === Phase 8 Components ===
use_bk_hyperbolic: true
use_ar_ssm_fusion: true
enable_numerical_guards: true

# Disable heavy optional components
enable_entailment_cones: false
enable_persistent_homology: false
enable_sheaf_attention: false

# === Phase 8 Optimizations (NEW) ===
# Fused Möbius operations
use_fused_mobius: true

# Green function caching
use_green_function_cache: true
green_function_cache_size: 512

# Low-Rank SSM parallel scan
use_parallel_ssm_scan: true

# Fused scattering gate
use_fused_scattering_gate: true

# Hyperbolic distance batching
use_batched_hyperbolic_distance: true

# Resonance adaptive curvature
use_resonance_adaptive_curvature: false  # DISABLED - causes geometry instability
resonance_threshold: 0.5
curvature_adjustment_rate: 0.01

# Ternary Möbius matmul
use_ternary_mobius_matmul: true

# === Triton Kernels ===
use_triton_kernel: true
triton_kernel_version: fast
triton_use_safe_ops: true
use_fused_kernels: true

# === torch.compile ===
# Note: Disabled on Python 3.12+
use_torch_compile: false
compile_mode: max-autotune

# === Flash Attention 2 ===
use_flash_attention_2: true

# === DataLoader ===
num_workers: 8
pin_memory: true
prefetch_factor: 4
persistent_workers: true

# === Logging ===
log_interval: 10
save_interval: 500  # 1000 -> 500
eval_interval: 500
save_dir: checkpoints/phase8_10b_japanese

# === Numerical Stability ===
max_norm: 0.99
numerical_epsilon: 1.0e-8
scale_epsilon: 1.0e-3

# === Moonshot Optimizations (Phase 2 & 3) ===
# #3 Eigenvalue Precomputation (Green Function LUT)
use_green_function_lut: true
green_function_lut_size: 1024

# #6 Resonance-Locked Training (skip low SNR updates)
use_resonance_locked: false  # DISABLED - blocks updates when GNS high
resonance_gns_threshold: 5.0

# #7 Scattering-Aware Attention Pruning
use_scattering_pruning: true
scattering_threshold: 0.1

# #8 Hyperbolic MoE (when using MoE layers)
use_hyperbolic_moe: true
hmoe_num_experts: 8
hmoe_top_k: 2

# #9 Gradient Teleportation (non-local gradient propagation)
# Scheduler managed: OFF during warmup (0-10%), ON after
use_gradient_teleportation: false  # DISABLED - distorts gradient direction
teleport_strength: 0.1

# #10 Time-Reversed Training (bi-directional consistency)
use_time_reversed: false  # DISABLED - fights forward progress
time_reversed_weight: 0.5

# #11 Holographic Compression (KV Cache)
use_holographic_kv_cache: true
holographic_compression_ratio: 0.25

# #12 Superposition Training (quantum-inspired)
use_superposition_training: false  # Heavy, enable manually
superposition_particles: 5

# === Revolutionary Training (3 Safe Algorithms) ===
# 2025-12-16: DISABLED - 内部オプティマイザがBK-HyperSGDと競合し
# 学習が「引き戻される」問題の主因と判明
use_revolutionary_training: false  # true → false
revolutionary_auto_schedule: true
revolutionary_algorithms: "closed_form,zeta,holographic"

# === TSP Path Optimizer (巡回セールスマン的学習経路最適化) ===
# 学習の振動状態を検知し、最適なハイパーパラメータへ動的に遷移
# 注意: すべてoptimizer step基準（16バッチ=1 optimizer step）
use_tsp_optimizer: true  # ONで開始
tsp_window_size: 10  # 評価ウィンドウ（10 optimizer steps = 160バッチ）
tsp_eval_interval: 10  # 評価間隔（10 optimizer steps = 160バッチごと評価）
tsp_epsilon: 0.10  # ε-greedy探索率（10%でランダム遷移）
tsp_min_dwell_steps: 15  # 最低滞在ステップ（optimizer step基準、≈240バッチ）

# === Gradient Aligner (勾配方向整合器) ===
# 逆向き勾配成分を除去し、全勾配をLoss減少方向に揃える
# VRAM +0: optimizer state (exp_avg) を参照方向として再利用
use_gradient_aligner: true
gradient_aligner_strength: 0.3  # 補正強度 (0.0-1.0, 0.1から開始推奨)
gradient_aligner_min_alignment: 0.0  # cos類似度下限 (0=逆向きのみ除去)
gradient_aligner_warmup: 100  # 観測のみ期間（optimizer steps）
