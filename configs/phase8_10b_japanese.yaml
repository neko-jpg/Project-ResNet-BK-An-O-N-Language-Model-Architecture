# Phase 8: 10B Japanese LLM - RTX 3080 8GB Optimized
# ===================================================
# 日本語特化モデル設定

# === Model Architecture ===
d_model: 4096
n_layers: 48
n_seq: 512
num_heads: 32
htt_rank: 16
hyperbolic_window_size: 64

# === Japanese Tokenizer ===
# rinna/japanese-gpt-neox uses 32000 tokens (vs 50257 for GPT-2)
vocab_size: 32000
tokenizer_name: "rinna/japanese-gpt-neox-3.6b"

# === Ultra Compression ===
low_rank_ffn: true
low_rank_attention: true
low_rank_rank: 16
use_bitnet: true

# === Memory Optimization ===
use_gradient_checkpointing: true
gradient_checkpointing_segments: 12
use_mixed_precision: true
mixed_precision_dtype: bfloat16

# === Training ===
batch_size: 1
gradient_accumulation_steps: 32
effective_batch_size: 32

# === Optimizer (Muon) ===
optimizer_type: muon
learning_rate: 0.02
warmup_steps: 2000
max_grad_norm: 0.5

# === Phase 8 Components ===
use_bk_hyperbolic: true
use_ar_ssm_fusion: true
enable_numerical_guards: true

# Disable heavy optional components
enable_entailment_cones: false
enable_persistent_homology: false
enable_sheaf_attention: false

# === Triton Kernels ===
use_triton_kernel: true
triton_kernel_version: fast
triton_use_safe_ops: true
use_fused_kernels: true

# === torch.compile ===
use_torch_compile: true
compile_mode: max-autotune

# === Flash Attention 2 ===
use_flash_attention_2: true

# === DataLoader ===
num_workers: 8
pin_memory: true
prefetch_factor: 4
persistent_workers: true

# === Logging ===
log_interval: 10
save_interval: 1000
eval_interval: 500
save_dir: checkpoints/phase8_10b_japanese

# === Numerical Stability ===
max_norm: 0.99
numerical_epsilon: 1.0e-8
scale_epsilon: 1.0e-3
