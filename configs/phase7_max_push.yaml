# ========================================
# Phase 7: 1B Chat Model Config
# ========================================
# 8GB VRAM向け安定設定
# GPU: RTX 3080 Laptop (8GB VRAM)
# 目標: ~1B パラメータ
#
# 使用方法:
#   make train-chat          # 訓練開始
#   make train-chat-test     # ダミーデータでテスト
#
# ========================================

# Architecture (~1B params)
model_type: phase7_1b
d_model: 3072          # 1B狙いで3072に調整
n_layers: 24           # 24層（OOM回避）
n_seq: 512             # シーケンス長
vocab_size: 50257      # GPT-2互換
num_heads: 24          # d_model / 128 = 24

# Low Rank Compression (必須 - これがないと即OOM)
embed_rank: 384        # d_model/8
ffn_rank: 384          # 圧縮率を上げてVRAM節約
head_rank: 384         # 出力層も圧縮

# HTT Embedding (Phase 7特有)
htt_rank: 64           # Holographic TT Embedding rank

# Hybrid Attention Settings
use_hybrid_attention: true
hyperbolic_window_size: 64
ar_ssm_max_rank: 32
ar_ssm_min_rank: 4

# Triton Kernels (高速化)
use_triton_kernel: true
triton_kernel_version: fast

# ========================================
# Training Strategy (VRAM 6.89GB使用)
# ========================================
batch_size: 1          # VRAM制約でバッチサイズは1一択
gradient_accumulation_steps: 16  # 実質バッチサイズ16を稼ぐ
use_gradient_checkpointing: true # ★ これがないと即死します
use_mixed_precision: true        # ★ FP16必須

# Optimizer
learning_rate: 0.0001  # 大きいモデルなので控えめに
weight_decay: 0.01
grad_clip: 1.0
warmup_steps: 2000

# Training Duration
# 1エポック = 195,312 steps (100M tokens / 512 seq)
# 5エポック = 976,560 steps
min_epochs: 5          # ★ 最低5エポックは回す
max_steps: 1000000     # 5エポック分 (~976,560 steps)

# ========================================
# ログ・保存
# ========================================
log_interval: 50
save_interval: 2000
eval_interval: 500
save_dir: checkpoints/phase7_max_push

# ========================================
# 安全設定
# ========================================
# OOM時の自動リカバリ
auto_reduce_batch_on_oom: true
min_batch_size: 1

# 勾配爆発防止
max_grad_norm: 1.0
gradient_nan_check: true

# ========================================
# データセット設定
# ========================================
# dataset_mixing.yaml を使用 (make recipe で設定)
data_limit: 100000000  # 100M tokens
