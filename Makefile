.PHONY: help setup install data data-lite data-ja data-ja-lite test demo clean up down doctor import recipe train-user phase4 build-rust bench-optimization

# Default shell
SHELL := /bin/bash
VENV := venv_ubuntu
export PYTHONPATH := .
# Try to detect if we are in the venv or need to use the path
PYTHON := $(shell if [ -f $(VENV)/bin/python ]; then echo $(VENV)/bin/python; else echo python3; fi)
PIP := $(shell if [ -f $(VENV)/bin/pip ]; then echo $(VENV)/bin/pip; else echo pip; fi)
PYTEST := $(shell if [ -f $(VENV)/bin/pytest ]; then echo $(VENV)/bin/pytest; else echo pytest; fi)

# Optional CLI overrides for training (set via `make train-user N_SEQ=512 BATCH_SIZE=8 ...`)
TRAIN_OVERRIDES :=
ifdef N_SEQ
TRAIN_OVERRIDES += --n-seq $(N_SEQ)
endif
ifdef D_MODEL
TRAIN_OVERRIDES += --d-model $(D_MODEL)
endif
ifdef N_LAYERS
TRAIN_OVERRIDES += --n-layers $(N_LAYERS)
endif
ifdef BATCH_SIZE
TRAIN_OVERRIDES += --batch-size $(BATCH_SIZE)
endif
ifdef EPOCHS
TRAIN_OVERRIDES += --epochs $(EPOCHS)
endif

help:
	@bash -c 'source .muse_config 2>/dev/null || true; \
	if [ "$$MUSE_LANG" = "2" ]; then \
		echo "MUSE (ResNet-BK) ÈñãÁô∫„Ç≥„Éû„É≥„Éâ"; \
		echo "======================================"; \
		echo "make setup      - ÂÆåÂÖ®„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó (‰ªÆÊÉ≥Áí∞Â¢É, ‰æùÂ≠òÈñ¢‰øÇ, Lite„Éá„Éº„Çø, Rust„Éì„É´„Éâ)"; \
		echo "make build-rust - Rust„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆ„Éì„É´„Éâ"; \
		echo "make install    - ‰æùÂ≠òÈñ¢‰øÇ„ÅÆ„Åø„Ç§„É≥„Çπ„Éà„Éº„É´"; \
		echo "make doctor     - „Ç∑„Çπ„ÉÜ„É†Ë®∫Êñ≠„Å®„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞"; \
		echo "make import     - Áã¨Ëá™„Éá„Éº„Çø„ÅÆ„Ç§„É≥„Éù„Éº„Éà (data/import/ „Åã„Çâ)"; \
		echo "make recipe     - Â≠¶Áøí„Éá„Éº„Çø„ÅÆÈÖçÂêàË®≠ÂÆö (Phase 3/7 „É¢„Éá„É´ÈÅ∏Êäû)"; \
		echo "make phase4     - Phase 4ÊúÄÂº∑Ë®≠ÂÆö(BitNet/Symplectic)„ÇíÁèæÂú®„ÅÆË®≠ÂÆö„Å´ÈÅ©Áî®"; \
		echo "make train-user - Ë®≠ÂÆö„Åó„Åü„É¨„Ç∑„Éî„ÅßÂ≠¶ÁøíÈñãÂßã"; \
		echo "make train-resume - Â≠¶Áøí„ÅÆÂÜçÈñã (Usage: make train-resume CHECKPOINT=...)"; \
		echo "make reborn       - Reborn Ritual (Âº∑ÂåñÂ≠¶ÁøíÁöÑËª¢Áîü)"; \
		echo "make merge        - „É¢„Éá„É´„ÅÆ„Éû„Éº„Ç∏"; \
		echo "make data-lite  - „ÉÜ„Çπ„ÉàÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"; \
		echo "make data       - ÂÖ®„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"; \
		echo "make test       - „ÉÜ„Çπ„Éà„ÅÆÂÆüË°å"; \
		echo "make demo       - MUSE„Éá„É¢„ÅÆÂÆüË°å"; \
		echo "make clean      - ‰ªÆÊÉ≥Áí∞Â¢É„Å®„Ç≠„É£„ÉÉ„Ç∑„É•„ÅÆÂâäÈô§"; \
		echo "make scale-up   - „Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Å´Âêà„Çè„Åõ„ÅüÊúÄÈÅ©Ë®≠ÂÆö„ÅÆËá™ÂãïÁîüÊàê"; \
		echo "make chat       - MUSE Creative Studio (Chat & Merge)"; \
		echo "make dashboard  - Â≠¶ÁøíÁä∂Ê≥Å„ÅÆÂèØË¶ñÂåñ (Streamlit)"; \
		echo "make clean-safe - „Ç¥„Éü„Éï„Ç°„Ç§„É´„Å®Âè§„ÅÑ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆÊéÉÈô§"; \
		echo "make deploy     - Hugging Face„Å∏„Éá„Éó„É≠„Ç§"; \
		echo "make pack       - ÈÖçÂ∏ÉÁî®Zip„ÅÆ‰ΩúÊàê"; \
		echo "make restore    - ÁèæÂú®„ÅÆÁä∂ÊÖã„Çí„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó"; \
		echo "make up         - DockerÁí∞Â¢É„ÅÆËµ∑Âãï"; \
		echo "make down       - DockerÁí∞Â¢É„ÅÆÂÅúÊ≠¢"; \
		echo "make compress-10b - üöÄ 100ÂÑÑ(10B)„Éë„É©„É°„Éº„Çø„É¢„Éá„É´„ÅÆÂàùÊúüÂåñ„Å®ÂúßÁ∏Æ"; \
		echo "make train-10b    - üöÄ 10BÂúßÁ∏Æ„É¢„Éá„É´„Åß„ÅÆË®ìÁ∑¥ÈñãÂßã (RTX 3080Âãï‰Ωú)"; \
		echo ""; \
		echo "Phase 7 („Éè„Ç§„Éñ„É™„ÉÉ„ÉâÂèåÊõ≤„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥ - TritonÂøÖÈ†à):"; \
		echo "make check-phase7-env       - Phase 7Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ (CUDA+TritonÁ¢∫Ë™ç)"; \
		echo "make train-phase7-1.5b      - üöÄ 1.5B„Éë„É©„É°„Éº„ÇøË®ìÁ∑¥ (10GB+ VRAM)"; \
		echo "make train-phase7-1.5b-8gb  - üöÄ 1.2B„Éë„É©„É°„Éº„ÇøË®ìÁ∑¥ (8GB VRAMÊúÄÈÅ©Âåñ)"; \
		echo "make train-phase7-1.5b-test - üß™ 1.5B„É¢„Éá„É´Âãï‰ΩúÁ¢∫Ë™ç („ÉÄ„Éü„Éº„Éá„Éº„Çø)"; \
		echo "make train-phase7-1.5b-resume CHECKPOINT=... - üîÑ Ë®ìÁ∑¥ÂÜçÈñã"; \
		echo "make bench-phase7-1.5b      - üìä GPU„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"; \
		echo "make chat-phase7-1.5b CHECKPOINT=... - üí¨ „ÉÅ„É£„ÉÉ„ÉàÊé®Ë´ñ"; \
		echo "make train-phase7           - Phase 7„É¢„Éá„É´„ÅÆÂ≠¶Áøí („Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö)"; \
		echo "make train-phase7-small     - „ÉÜ„Çπ„ÉàÁî®Â∞èË¶èÊ®°Ë®≠ÂÆö„ÅßÂ≠¶Áøí"; \
		echo "make train-phase7-large     - Â§ßË¶èÊ®°Ë®≠ÂÆö„ÅßÂ≠¶Áøí (24GB+ VRAM)"; \
		echo "make train-phase7-config CONFIG=path/to/config.yaml - „Ç´„Çπ„Çø„É†Ë®≠ÂÆö„ÅßÂ≠¶Áøí"; \
		echo "make train-phase7-resume CHECKPOINT=path/to/model.pt - Ë®ìÁ∑¥ÂÜçÈñã"; \
		echo "make test-phase7            - Phase 7Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å"; \
		echo "make triton-attn            - Triton„Ç´„Éº„Éç„É´Âãï‰ΩúÁ¢∫Ë™ç"; \
		echo ""; \
		echo "Phase 8 (ÂèåÊõ≤Ë∂ÖË∂ä - O(N)Ë§áÈõëÂ∫¶):"; \
		echo "make train-phase8       - Phase 8„É¢„Éá„É´„ÅÆÂ≠¶Áøí (O(N)Á∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥)"; \
		echo "make train-phase8-small - „ÉÜ„Çπ„ÉàÁî®Â∞èË¶èÊ®°Ë®≠ÂÆö„ÅßÂ≠¶Áøí"; \
		echo "make train-phase8-max   - ÊúÄÂ§ßË®≠ÂÆö„ÅßÂ≠¶Áøí (3B params, 8GB VRAM)"; \
		echo "make train-phase8-test  - „ÉÄ„Éü„Éº„Éá„Éº„Çø„Åß„ÉÜ„Çπ„Éà"; \
		echo "make bench-phase8-vs-phase7 - Phase 7„Å®Phase 8„ÅÆÊÄßËÉΩÊØîËºÉ"; \
		echo "make bench-optimization     - ‰ªäÂõûÂÆüË£Ö„Åó„ÅüÊúÄÈÅ©Âåñ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"; \
	else \
		echo "MUSE (ResNet-BK) Development Commands"; \
		echo "======================================"; \
		echo "make setup      - Full setup (venv, deps, lite data, rust build)"; \
		echo "make build-rust - Build Rust data loader"; \
		echo "make install    - Install dependencies only"; \
		echo "make doctor     - Run system diagnostics"; \
		echo "make import     - Import user data from data/import/"; \
		echo "make recipe     - Configure dataset mixing recipe (select Phase 3/7 model)"; \
		echo "make phase4     - Merge Phase 4 Config into current recipe"; \
		echo "make train-user - Start training with user recipe"; \
		echo "make train-resume - Resume training (Usage: make train-resume CHECKPOINT=...)"; \
		echo "make reborn       - Reborn Ritual (Usage: make reborn CHECKPOINT=...)"; \
		echo "make merge        - Merge Models"; \
		echo "make data-lite  - Download small test dataset"; \
		echo "make data       - Download ALL datasets"; \
		echo "make test       - Run tests"; \
		echo "make demo       - Run MUSE capabilities demo"; \
		echo "make clean      - Remove venv and artifacts"; \
		echo "make scale-up   - Auto-configure for hardware"; \
		echo "make chat       - MUSE Creative Studio (Chat & Merge)"; \
		echo "make dashboard  - Visualize Training (Streamlit)"; \
		echo "make clean-safe - Clean garbage and old checkpoints"; \
		echo "make deploy     - Deploy to Hugging Face"; \
		echo "make pack       - Create distribution Zip"; \
		echo "make restore    - Backup current state"; \
		echo "make up         - Start Docker environment"; \
		echo "make down       - Stop Docker environment"; \
		echo "make compress-10b - üöÄ Initialize and Compress 10B Parameter Model"; \
		echo "make train-10b    - üöÄ Train 10B Compressed Model (RTX 3080 Ready)"; \
		echo ""; \
		echo "Phase 7 (Hybrid Hyperbolic Attention - Triton Required):"; \
		echo "make check-phase7-env       - Check Phase 7 environment (CUDA+Triton)"; \
		echo "make train-phase7-1.5b      - üöÄ Train 1.5B model (10GB+ VRAM)"; \
		echo "make train-phase7-1.5b-8gb  - üöÄ Train 1.2B model (8GB VRAM optimized)"; \
		echo "make train-phase7-1.5b-test - üß™ Test 1.5B model (dummy data)"; \
		echo "make train-phase7-1.5b-resume CHECKPOINT=... - üîÑ Resume training"; \
		echo "make bench-phase7-1.5b      - üìä Benchmark GPU"; \
		echo "make chat-phase7-1.5b CHECKPOINT=... - üí¨ Chat inference"; \
		echo "make train-phase7           - Train Phase 7 model (default config)"; \
		echo "make train-phase7-small     - Train with small config for testing"; \
		echo "make train-phase7-large     - Train with large config (24GB+ VRAM)"; \
		echo "make train-phase7-config CONFIG=path/to/config.yaml - Train with custom config"; \
		echo "make train-phase7-resume CHECKPOINT=path/to/model.pt - Resume training"; \
		echo "make test-phase7            - Run Phase 7 integration tests"; \
		echo "make triton-attn            - Verify Triton kernel"; \
		echo ""; \
		echo "Phase 8 (Hyperbolic Transcendence - O(N) Complexity):"; \
		echo "make train-phase8       - Train Phase 8 model (O(N) linear attention)"; \
		echo "make train-phase8-small - Train with small config for testing"; \
		echo "make train-phase8-max   - Train with maximum config (3B params, 8GB VRAM)"; \
		echo "make train-phase8-test  - Test with dummy data"; \
		echo "make bench-phase8-vs-phase7 - Benchmark Phase 7 vs Phase 8"; \
		echo "make bench-optimization     - Benchmark new optimizations"; \
	fi'

setup:
	@if [ -f scripts/easy_setup.sh ]; then \
		chmod +x scripts/easy_setup.sh && ./scripts/easy_setup.sh; \
	else \
		$(MAKE) install; \
	fi
	$(MAKE) build-rust

install:
	test -d $(VENV) || python3 -m venv $(VENV)
	$(PIP) install --upgrade pip setuptools wheel
	$(PIP) install -r requirements.txt
	$(PIP) install -e .

build-rust:
	cd rust_loader && maturin develop --release

bench-optimization:
	$(PYTHON) src/benchmarks/optimization_benchmark.py

data-lite:
	$(PYTHON) scripts/prepare_datasets.py --datasets cosmopedia --max_samples 1000

data-ja-lite:
	$(PYTHON) scripts/prepare_datasets.py --datasets mc4_ja --max_samples 1000

data-ja:
	$(PYTHON) scripts/prepare_datasets.py --datasets mc4_ja

data:
	$(PYTHON) scripts/prepare_datasets.py

test:
	$(PYTEST) tests/

ci:
	$(PYTHON) scripts/run_ci.py

demo:
	$(PYTHON) scripts/demo_muse_full.py

clean:
	rm -rf $(VENV)
	rm -rf build dist *.egg-info
	find . -name "__pycache__" -type d -exec rm -rf {} +
	find . -name "*.pyc" -delete
	cd rust_loader && cargo clean

up:
	docker-compose up -d

down:
	docker-compose down

doctor:
	$(PYTHON) scripts/doctor.py

import:
	$(PYTHON) scripts/import_user_data.py

recipe:
	$(PYTHON) scripts/configure_recipe.py

phase4:
	$(PYTHON) scripts/apply_phase4_config.py

train-user:
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Error: Recipe not found. Please run 'make recipe' first."; \
		exit 1; \
	fi
	@if [ -f configs/auto_optimized.yaml ]; then \
		echo "Using auto-optimized config..."; \
		cmd="$(PYTHON) scripts/train.py --dataset configs/dataset_mixing.yaml --config configs/auto_optimized.yaml $(TRAIN_OVERRIDES)"; \
		echo "$$cmd"; \
		$$cmd; \
	elif [ -f configs/user_train_config.yaml ]; then \
		echo "Using user_train_config.yaml (Phase 4 / Manual)..."; \
		cmd="$(PYTHON) scripts/train.py --dataset configs/dataset_mixing.yaml --config configs/user_train_config.yaml $(TRAIN_OVERRIDES)"; \
		echo "$$cmd"; \
		$$cmd; \
	else \
		echo "User config not found. Running with default preset 'small'."; \
		cmd="$(PYTHON) scripts/train.py --dataset configs/dataset_mixing.yaml --config-preset small $(TRAIN_OVERRIDES)"; \
		echo "$$cmd"; \
		$$cmd; \
	fi

train-resume:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		exit 1; \
	fi
	$(PYTHON) scripts/train.py --dataset configs/dataset_mixing.yaml --resume-from $(CHECKPOINT)

reborn:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/elder.pt"; \
		exit 1; \
	fi
	$(PYTHON) scripts/reborn.py --checkpoint $(CHECKPOINT)

merge:
	$(PYTHON) scripts/merge_models.py --help

scale-up:
	$(PYTHON) scripts/auto_scale.py

chat:
	PYTHONPATH=. $(VENV)/bin/streamlit run app.py

dashboard:
	PYTHONPATH=. $(VENV)/bin/streamlit run app.py

clean-safe:
	$(PYTHON) scripts/muse_utils.py clean-safe

deploy:
	$(PYTHON) scripts/deploy_interactive.py

restore:
	$(PYTHON) scripts/muse_utils.py restore-point

pack:
	$(PYTHON) scripts/muse_utils.py pack

check-update:
	$(PYTHON) scripts/muse_utils.py version-guardian

notify:
	$(PYTHON) scripts/muse_utils.py notify

# Phase 7 Hyperbolic Attention Triton smoke test
triton-attn:
	$(PYTHON) scripts/check_hyperbolic_triton.py --use-triton --use-mask --kernel fast --json results/triton_attention_check.json

# Phase 7 Hyperbolic Attention Triton benchmark (compare all kernels)
triton-bench:
	$(PYTHON) scripts/benchmark_hyperbolic_triton.py --batch 4 --seq-len 512 --d-model 256 --heads 8 --json results/benchmarks/hyperbolic_triton_benchmark.json

# Phase 7 Hyperbolic Attention - fast kernel only
triton-fast:
	$(PYTHON) scripts/check_hyperbolic_triton.py --use-triton --use-mask --kernel fast --seq-len 512 --d-model 256 --heads 8 --json results/triton_attention_check.json

# ============================================================================
# Phase 7 Training Commands (TritonÂøÖÈ†à - CUDA+Triton Required)
# ============================================================================

# Phase 7Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ (TritonÂøÖÈ†àÁ¢∫Ë™ç)
check-phase7-env:
	@echo "=========================================="
	@echo "üîç Phase 7 Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ (TritonÂøÖÈ†à)"
	@echo "=========================================="
	@$(PYTHON) -c "import torch; print('‚úì PyTorch:', torch.__version__)" || (echo "‚ùå PyTorch not found"; exit 1)
	@$(PYTHON) -c "import torch; assert torch.cuda.is_available(), 'CUDA not available'; print('‚úì CUDA:', torch.version.cuda)" || (echo "‚ùå CUDA not available"; exit 1)
	@$(PYTHON) -c "import triton; print('‚úì Triton:', triton.__version__)" || (echo "‚ùå Triton not found. Install: pip install triton"; exit 1)
	@$(PYTHON) -c "from src.kernels.hyperbolic_attention_fast import fast_hyperbolic_attention; print('‚úì Hyperbolic Triton kernel loaded')" || (echo "‚ùå Triton kernel load failed"; exit 1)
	@echo "=========================================="
	@echo "‚úÖ Phase 7Áí∞Â¢ÉOK - Ë®ìÁ∑¥ÂèØËÉΩ„Åß„Åô"
	@echo "=========================================="

# Phase 7 Training - Default configuration (RTX 3080 optimized, TritonÂøÖÈ†à)
train-phase7: check-phase7-env
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Error: Recipe not found. Please run 'make recipe' first."; \
		exit 1; \
	fi
	@echo "üöÄ Phase 7Ë®ìÁ∑¥ÈñãÂßã (Triton„Ç´„Éº„Éç„É´‰ΩøÁî®)"
	$(PYTHON) scripts/train_phase7.py --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)

# Phase 7 Training - Small configuration for testing (TritonÂøÖÈ†à)
train-phase7-small: check-phase7-env
	@echo "üß™ Phase 7Â∞èË¶èÊ®°„ÉÜ„Çπ„ÉàË®ìÁ∑¥ (Triton„Ç´„Éº„Éç„É´‰ΩøÁî®)"
	$(PYTHON) scripts/train_phase7.py --d-model 256 --n-layers 4 --n-seq 256 --batch-size 8 --epochs 1 $(TRAIN_OVERRIDES)

# Phase 7 Training - Large configuration (requires 24GB+ VRAM, TritonÂøÖÈ†à)
train-phase7-large: check-phase7-env
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Error: Recipe not found. Please run 'make recipe' first."; \
		exit 1; \
	fi
	@echo "üî• Phase 7Â§ßË¶èÊ®°Ë®ìÁ∑¥ (Triton„Ç´„Éº„Éç„É´‰ΩøÁî®)"
	$(PYTHON) scripts/train_phase7.py --d-model 768 --n-layers 12 --n-seq 1024 --batch-size 2 --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)

# Phase 7 Training - Resume from checkpoint (TritonÂøÖÈ†à)
train-phase7-resume: check-phase7-env
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		exit 1; \
	fi
	@echo "üîÑ Phase 7Ë®ìÁ∑¥ÂÜçÈñã (Triton„Ç´„Éº„Éç„É´‰ΩøÁî®)"
	$(PYTHON) scripts/train_phase7.py --resume-from $(CHECKPOINT) --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)

# Phase 7 Training - Custom config file (TritonÂøÖÈ†à)
train-phase7-config: check-phase7-env
	@if [ -z "$(CONFIG)" ]; then \
		echo "Error: Please specify CONFIG=path/to/config.yaml"; \
		echo "Example: make train-phase7-config CONFIG=configs/phase7_optimized.yaml"; \
		exit 1; \
	fi
	@if [ ! -f $(CONFIG) ]; then \
		echo "Error: Config file not found: $(CONFIG)"; \
		exit 1; \
	fi
	@echo "‚öôÔ∏è  Phase 7Ë®ìÁ∑¥ („Ç´„Çπ„Çø„É†Ë®≠ÂÆö: $(CONFIG))"
	$(PYTHON) scripts/train_phase7.py --config $(CONFIG) $(TRAIN_OVERRIDES)

# Phase 7 Validation - Run integration tests
test-phase7:
	$(PYTEST) tests/test_phase7_integration.py -v

# Phase 7 Benchmark - Full validation suite
bench-phase7:
	$(PYTHON) benchmarks/phase7_validation.py

# ============================================================================
# Phase 8 Training Commands (Hyperbolic Transcendence)
# ============================================================================

# Phase 8 Training - Default configuration (RTX 3080 optimized)
train-phase8:
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Error: Recipe not found. Please run 'make recipe' first."; \
		exit 1; \
	fi
	$(PYTHON) scripts/train_phase8.py --config configs/phase8_optimized.yaml --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)

# Phase 8 Training - Small configuration for testing
train-phase8-small:
	$(PYTHON) scripts/train_phase8.py --d-model 256 --n-layers 4 --n-seq 256 --batch-size 8 --epochs 1 --dry-run $(TRAIN_OVERRIDES)

# Phase 8 Training - Maximum configuration (3B params, 8GB VRAM)
train-phase8-max:
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Warning: Recipe not found. Using dry-run mode."; \
		$(PYTHON) scripts/train_phase8.py --config configs/phase8_max_push.yaml --dry-run; \
	else \
		$(PYTHON) scripts/train_phase8.py --config configs/phase8_max_push.yaml --dataset configs/dataset_mixing.yaml; \
	fi

# Phase 8 Training - Maximum with SSM (heavier, experimental)
train-phase8-max-ssm:
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Warning: Recipe not found. Using dry-run mode."; \
		$(PYTHON) scripts/train_phase8.py --config configs/phase8_max_push.yaml --use-ssm --dry-run; \
	else \
		$(PYTHON) scripts/train_phase8.py --config configs/phase8_max_push.yaml --use-ssm --dataset configs/dataset_mixing.yaml; \
	fi

# Phase 8 Training - Dry run test
train-phase8-test:
	$(PYTHON) scripts/train_phase8.py --config configs/phase8_max_push.yaml --dry-run

# Phase 8 Training - Resume from checkpoint
train-phase8-resume:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		exit 1; \
	fi
	$(PYTHON) scripts/train_phase8.py --config configs/phase8_optimized.yaml --dataset configs/dataset_mixing.yaml --resume-from $(CHECKPOINT) $(TRAIN_OVERRIDES)

# Phase 8 vs Phase 7 Benchmark
bench-phase8-vs-phase7:
	$(PYTHON) scripts/benchmark_phase7_vs_phase8.py

# ============================================================================
# Phase 7 Maximum Parameters (1.8B Monster)
# ============================================================================

# Phase 7 Max - 1.8B parameters training (d=4096, L=32)
train-phase7-max:
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "Warning: Recipe not found. Using dry-run mode."; \
		$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dry-run; \
	else \
		$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dataset configs/dataset_mixing.yaml; \
	fi

# Phase 7 Max - Dry run (test with dummy data)
train-phase7-max-test:
	$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dry-run

# Phase 7 Max - Resume training
train-phase7-max-resume:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		exit 1; \
	fi
	$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dataset configs/dataset_mixing.yaml --resume-from $(CHECKPOINT)

# GPU Benchmark - Find maximum parameters for your GPU
gpu-benchmark:
	$(PYTHON) scripts/gpu_benchmark_standalone.py

# ============================================================================
# Phase 7 - 1.5B Parameters Training (TritonÂøÖÈ†à - ÂÖ®ÊúÄÈÅ©ÂåñON)
# ============================================================================

# üöÄ Phase 7 - 1.5B„Éë„É©„É°„Éº„ÇøË®ìÁ∑¥ÈñãÂßã (TritonÂøÖÈ†à)
train-phase7-1.5b: check-phase7-env
	@echo "=========================================="
	@echo "üöÄ Phase 7 - 1.5B Parameters Training"
	@echo "=========================================="
	@echo "Config: d_model=2048, n_layers=24, seq=1024"
	@echo "Parameters: ~1.5B (1,500,000,000)"
	@echo "VRAM: ~8-10GB (batch=1, gradient_accum=16)"
	@echo "Triton: ÂøÖÈ†à (ÂÖ®ÊúÄÈÅ©ÂåñON)"
	@echo ""
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "‚ö†Ô∏è  Warning: Recipe not found. Please run 'make recipe' first."; \
		echo "Using dry-run mode for testing..."; \
		$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton.yaml --dry-run; \
	else \
		$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton.yaml --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES); \
	fi

# üöÄ Phase 7 - 1.5B„Éë„É©„É°„Éº„ÇøË®ìÁ∑¥ (8GB VRAMÁâà)
train-phase7-1.5b-8gb: check-phase7-env
	@echo "=========================================="
	@echo "üöÄ Phase 7 - 1.2B Parameters (8GB VRAM)"
	@echo "=========================================="
	@echo "Config: d_model=1792, n_layers=24, seq=512"
	@echo "Parameters: ~1.2B (optimized for 8GB GPU)"
	@echo "VRAM: ~7-8GB (batch=1, gradient_accum=16)"
	@echo "Optimizer: AdamW 8bit (memory efficient)"
	@echo ""
	@if [ ! -f configs/dataset_mixing.yaml ]; then \
		echo "‚ö†Ô∏è  Warning: Recipe not found. Using dry-run mode..."; \
		$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton_8gb.yaml --dry-run; \
	else \
		$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton_8gb.yaml --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES); \
	fi

# üß™ 1.5B„É¢„Éá„É´ - „ÉÄ„Éü„Éº„Éá„Éº„Çø„Åß„ÉÜ„Çπ„Éà
train-phase7-1.5b-test: check-phase7-env
	@echo "=========================================="
	@echo "üß™ Phase 7 - 1.5B Dry Run Test"
	@echo "=========================================="
	$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton.yaml --dry-run

# üîÑ 1.5B„É¢„Éá„É´ - Ë®ìÁ∑¥ÂÜçÈñã
train-phase7-1.5b-resume: check-phase7-env
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		echo "Example: make train-phase7-1.5b-resume CHECKPOINT=checkpoints/phase7_1.5b_triton/step_2000.pt"; \
		exit 1; \
	fi
	@echo "üîÑ Phase 7 - 1.5B Training Resume"
	$(PYTHON) scripts/train_phase7.py --config configs/phase7_1.5b_triton.yaml --dataset configs/dataset_mixing.yaml --resume-from $(CHECKPOINT) $(TRAIN_OVERRIDES)

# üìä 1.5B„É¢„Éá„É´ - GPUÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ
bench-phase7-1.5b:
	@echo "=========================================="
	@echo "üìä Phase 7 - 1.5B GPU Benchmark"
	@echo "=========================================="
	$(PYTHON) scripts/gpu_benchmark_phase7.py --config configs/phase7_1.5b_triton.yaml

# üí¨ 1.5B„É¢„Éá„É´ - „ÉÅ„É£„ÉÉ„ÉàÊé®Ë´ñ
chat-phase7-1.5b:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "========================================"; \
		echo "üí¨ Phase 7 - 1.5B Chat (Auto-detect)"; \
		echo "========================================"; \
		$(PYTHON) scripts/chat_inference.py --config configs/phase7_1.5b_triton.yaml; \
	else \
		echo "========================================"; \
		echo "üí¨ Phase 7 - 1.5B Chat"; \
		echo "========================================"; \
		$(PYTHON) scripts/chat_inference.py --config configs/phase7_1.5b_triton.yaml --checkpoint $(CHECKPOINT); \
	fi

# ============================================================================
# Phase 7 Chat AI Training (1.8B Monster - Quick Start)
# ============================================================================

# üöÄ „ÉÅ„É£„ÉÉ„ÉàAIË®ìÁ∑¥ÈñãÂßã (ÊúÄÂ§ßË®≠ÂÆö: d=4096, L=32, ~1.8B params)
train-chat:
	@echo "=========================================="
	@echo "üöÄ Phase 7 Chat AI Training (1.8B Monster)"
	@echo "=========================================="
	@echo "Config: d_model=4096, n_layers=32, seq=512"
	@echo "VRAM: ~6.89GB (batch=1, gradient_accum=16)"
	@echo ""
	$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dataset configs/dataset_mixing.yaml

# üß™ „ÉÄ„Éü„Éº„Éá„Éº„Çø„Åß„ÉÜ„Çπ„Éà („Éá„Éº„Çø„Çª„ÉÉ„Éà„Å™„Åó„ÅßÂãï‰ΩúÁ¢∫Ë™ç)
train-chat-test:
	@echo "=========================================="
	@echo "üß™ Phase 7 Chat AI - Dry Run Test"
	@echo "=========================================="
	$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dry-run

# üìä GPUÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ (ÊúÄÂ§ß„Éë„É©„É°„Éº„ÇøÊï∞„ÇíÊ∏¨ÂÆö)
bench-chat:
	@echo "=========================================="
	@echo "üìä GPU Maximum Parameters Benchmark"
	@echo "=========================================="
	$(PYTHON) scripts/gpu_benchmark_standalone.py

# üîÑ Ë®ìÁ∑¥ÂÜçÈñã
train-chat-resume:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "Error: Please specify CHECKPOINT=path/to/model.pt"; \
		echo "Example: make train-chat-resume CHECKPOINT=checkpoints/phase7_max_push/step_2000.pt"; \
		exit 1; \
	fi
	$(PYTHON) scripts/train_phase7_max.py --config configs/phase7_max_push.yaml --dataset configs/dataset_mixing.yaml --resume-from $(CHECKPOINT)

# ‚úÖ Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ (Ë®ìÁ∑¥Ââç„Å´ÂÆüË°åÊé®Â•®)
verify-phase7:
	@echo "=========================================="
	@echo "‚úÖ Phase 7 Environment Verification"
	@echo "=========================================="
	$(PYTHON) scripts/verify_phase7_ready.py

# üîß Triton„Ç´„Éº„Éç„É´Âãï‰ΩúÁ¢∫Ë™ç
verify-triton:
	$(PYTHON) scripts/check_hyperbolic_triton.py --use-triton --kernel fast

# üí¨ Ë®ìÁ∑¥Ê∏à„Åø„É¢„Éá„É´„Åß„ÉÅ„É£„ÉÉ„Éà
chat-ai:
	@if [ -z "$(CHECKPOINT)" ]; then \
		echo "========================================"; \
		echo "üí¨ MUSE Chat AI (Auto-detect checkpoint)"; \
		echo "========================================"; \
		$(PYTHON) scripts/chat_inference.py; \
	else \
		echo "========================================"; \
		echo "üí¨ MUSE Chat AI"; \
		echo "========================================"; \
		$(PYTHON) scripts/chat_inference.py --checkpoint $(CHECKPOINT); \
	fi

# ============================================================================
# Phase 8 Extreme Compression (1B -> 10B)
# ============================================================================

compress-10b:
	@echo "=========================================="
	@echo "üóúÔ∏è  Compressing 10B (100.1 Billion) Parameter Model"
	@echo "=========================================="
	$(PYTHON) scripts/compress_model.py --output_dir checkpoints/compressed_10b_start --d_model 5120 --n_layers 31

train-10b:
	@if [ ! -f checkpoints/compressed_10b_start/compressed_model.pt ]; then \
		echo "Error: Compressed model not found. Please run 'make compress-10b' first."; \
		exit 1; \
	fi
	@echo "=========================================="
	@echo "üöÄ Starting Training on 10B Compressed Model (RTX 3080 Ready)"
	@echo "=========================================="
	$(PYTHON) scripts/train_phase8.py --config configs/phase8_10b.yaml --resume-from checkpoints/compressed_10b_start/compressed_model.pt --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)
<<<<<<< HEAD

train-10b-8gb:
	@echo "=========================================="
	@echo "üöÄ Starting Extreme Optimization Training (RTX 3080 8GB)"
	@echo "=========================================="
	$(PYTHON) scripts/train_phase8.py --d-model 4096 --n-layers 48 --extreme-compression --dataset configs/dataset_mixing.yaml $(TRAIN_OVERRIDES)
=======
>>>>>>> 99f3f4c6dcba04bfb1d5e20a9f802278fe6d055a
