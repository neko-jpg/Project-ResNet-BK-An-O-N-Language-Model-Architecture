# Phase 8 Speed Optimization - Quick Guide

## ğŸš€ é€Ÿåº¦æœ€é©åŒ–ã®ä½¿ã„æ–¹

### 1. Flash Attention 2ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã ãŒæ¨å¥¨)
```bash
# CUDA 11.8/12.1ã®å ´åˆ
pip install flash-attn --no-build-isolation

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ããªã„å ´åˆã¯è‡ªå‹•çš„ã«fallbackã•ã‚Œã¾ã™
```

### 2. torch.compileä½¿ç”¨æ–¹æ³•

#### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰(ãƒãƒ©ãƒ³ã‚¹å‹)
```python
config = Phase8Config(
    use_torch_compile=True,
    compile_mode="default",  # ãƒãƒ©ãƒ³ã‚¹
)
```

#### æœ€å¤§é€Ÿåº¦ãƒ¢ãƒ¼ãƒ‰
```python
config = Phase8Config(
    use_torch_compile=True,
    compile_mode="max-autotune",  # æœ€é€Ÿ
)
```

### 3. 10B Ultraè¨­å®šã§è¨“ç·´
```bash
# WSL Ubuntuç’°å¢ƒ
cd /mnt/c/dev/Project-ResNet-BK-An-O-N-Language-Model-Architecture
source venv_ubuntu/bin/activate

# Dry run test
python scripts/train_phase8.py --config configs/phase8_10b_ultra.yaml --dry-run

# Full training (config already has all optimizations)
python scripts/train_phase8.py --config configs/phase8_10b_ultra.yaml --dataset configs/dataset_mixing.yaml
```

### 4. é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
```bash
# Baseline (æœ€é©åŒ–ãªã—)
python scripts/benchmark_phase8_speed.py --d-model 512 --n-layers 8 --n-seq 512 --low-rank-rank 16

# With torch.compile
python scripts/benchmark_phase8_speed.py --d-model 512 --n-layers 8 --n-seq 512 --low-rank-rank 16 --use-compile

# With torch.compile + Flash Attention 2
python scripts/benchmark_phase8_speed.py --d-model 512 --n-layers 8 --n-seq 512 --low-rank-rank 16 --use-compile --use-flash-attn2
```

## âš¡ æœŸå¾…ã•ã‚Œã‚‹é€Ÿåº¦å‘ä¸Š

| æœ€é©åŒ– | é€Ÿåº¦å‘ä¸Š | ç´¯ç©å‘ä¸Š |
|--------|----------|----------|
| Baseline | 1x | 1x |
| Data loadingæœ€é©åŒ– | +20% | 1.2x |
| bfloat16 mixed precision | +50% | 1.8x|
| torch.compile (default) | +100% | 3.6x |
| torch.compile (max-autotune) | +150% | 4.5x |
| Flash Attention 2 | +50% | 6.7x |
| Fused kernels (è¨ˆç”»ä¸­) | +50% | 10x |

**ç›®æ¨™: >1000 tokens/ç§’**
- BaselineãŒ~100-150 tokens/ç§’ã®å ´åˆ
- ç¾åœ¨ã®æœ€é©åŒ–ã§600-800 tokens/ç§’é”æˆå¯èƒ½
- Fused kernelså®Ÿè£…å¾Œã«1000+ tokens/ç§’é”æˆ

## ğŸ—œï¸ ãƒ¡ãƒ¢ãƒªã¨ã®å…¼ã­åˆã„

### RTX 3080 8GBæ¨å¥¨è¨­å®š

**é«˜é€Ÿå„ªå…ˆ (Medium model)**:
```yaml
d_model: 1024
n_layers: 16
low_rank_rank: 32
use_torch_compile: true
use_flash_attention_2: true
gradient_checkpointing: false  # é€Ÿåº¦å„ªå…ˆ
```

**ãƒ¡ãƒ¢ãƒªå„ªå…ˆ (10B ultra)**:
```yaml
d_model: 4096
n_layers: 48
low_rank_rank: 16
use_torch_compile: true  # compileè‡ªä½“ã¯ãƒ¡ãƒ¢ãƒªç¯€ç´„
use_flash_attention_2: true  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚‚è‰¯ã„
gradient_checkpointing: true  # å¿…é ˆ
gradient_accumulation_steps: 32
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### torch.compileã‚¨ãƒ©ãƒ¼
```
# ã‚¨ãƒ©ãƒ¼: "Triton kernel failed"
â†’ compile_fullgraph: false ã«è¨­å®š (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
â†’ ã¾ãŸã¯ compile_mode: "default" ã«å¤‰æ›´
```

### Flash Attention 2ãŒinstallã§ããªã„
```
# å•é¡Œãªã„ã€è‡ªå‹•çš„ã«fallbackã—ã¾ã™
# wrapper ãŒæ¨™æº–attention ã‚’ä½¿ç”¨
```

### OOM (Out of Memory)
```
# gradient_checkpointing ã‚’æœ‰åŠ¹åŒ–
use_gradient_checkpointing: true

# ã¾ãŸã¯ gradient accumulation ã‚’å¢—ã‚„ã™
gradient_accumulation_steps: 64
```

## ğŸ“Š è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

ã™ã¹ã¦ã®æœ€é©åŒ–ã¯`configs/phase8_10b_ultra.yaml`ã«å«ã¾ã‚Œã¦ã„ã¾ã™:
- âœ… torch.compile (max-autotune)
- âœ… Flash Attention 2
- âœ… Data loading optimizations
- âœ… Gradient accumulation (32 steps)
- âœ… Mixed precision (bfloat16)
- âœ… Gradient checkpointing

ãã®ã¾ã¾ä½¿ç”¨å¯èƒ½ã§ã™ï¼
