% BibTeX citation for ResNet-BK

@inproceedings{resnetbk2025,
  title={ResNet-BK: Birman-Schwinger Operator Theory for Ultra-Stable O(N) Language Models},
  author={Your Name},
  booktitle={Advances in Neural Information Processing Systems},
  volume={38},
  pages={1--12},
  year={2025},
  url={https://arxiv.org/abs/XXXX.XXXXX},
  doi={10.XXXX/XXXXX},
  abstract={We present ResNet-BK, a novel O(N) language model architecture based on Birman-Schwinger operator theory that surpasses Mamba across three critical dimensions: long-context stability (up to 1M tokens), quantization robustness (4× lower perplexity at INT4), and dynamic compute efficiency (2× fewer FLOPs at equal perplexity). Our approach leverages rigorous mathematical foundations including trace-class operators, Mourre estimates, and Limiting Absorption Principle to guarantee numerical stability.}
}

% Software citation
@software{resnetbk_software,
  title={ResNet-BK: Implementation of Birman-Schwinger Based Language Model},
  author={Your Name},
  year={2025},
  version={0.9.0},
  url={https://github.com/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture},
  license={MIT}
}

% arXiv preprint (use before conference publication)
@article{resnetbk2024arxiv,
  title={ResNet-BK: Birman-Schwinger Operator Theory for Ultra-Stable O(N) Language Models},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024},
  url={https://arxiv.org/abs/XXXX.XXXXX}
}

% Theoretical foundations

@misc{arai2025riemann,
  title={Riemann Hypothesis and AI: Emergent Theory},
  author={Arai, Teppei},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.17600573},
  url={https://doi.org/10.5281/zenodo.17600573},
  note={License: CC BY-NC-ND 4.0}
}

% Related work citations

@article{mamba2023,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023},
  url={https://arxiv.org/abs/2312.00752}
}

@article{s4_2022,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={International Conference on Learning Representations (ICLR)},
  year={2022},
  url={https://arxiv.org/abs/2111.00396}
}

@book{newton1982scattering,
  title={Scattering Theory of Waves and Particles},
  author={Newton, Roger G.},
  year={1982},
  publisher={Springer},
  isbn={978-0-486-42535-8}
}

@article{mourre1981limiting,
  title={Limiting Absorption Principle and Resonances for the Dirac Operator},
  author={Mourre, Eric},
  journal={Communications in Mathematical Physics},
  volume={78},
  number={3},
  pages={391--408},
  year={1981},
  publisher={Springer},
  doi={10.1007/BF01942331}
}

@article{birman1962spectral,
  title={On the Spectral Theory of Elliptic Operators},
  author={Birman, M. Sh. and Schwinger, Julian},
  journal={Soviet Mathematics Doklady},
  volume={3},
  pages={740--744},
  year={1962}
}

@book{reed1979methods,
  title={Methods of Modern Mathematical Physics: IV. Analysis of Operators},
  author={Reed, Michael and Simon, Barry},
  year={1979},
  publisher={Academic Press},
  isbn={978-0125850049}
}

@article{weil1952explicit,
  title={Sur les "formules explicites" de la th{\'e}orie des nombres premiers},
  author={Weil, Andr{\'e}},
  journal={Communications du S{\'e}minaire Math{\'e}matique de l'Universit{\'e} de Lund},
  pages={252--265},
  year={1952}
}

@article{transformer2017,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}

@article{rwkv2023,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023},
  url={https://arxiv.org/abs/2305.13048}
}

@article{hyena2023,
  title={Hyena Hierarchy: Towards Larger Convolutional Language Models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  journal={International Conference on Machine Learning (ICML)},
  year={2023},
  url={https://arxiv.org/abs/2302.10866}
}

% Usage examples:
% 
% For the main paper:
% \cite{resnetbk2025}
%
% For the software:
% \cite{resnetbk_software}
%
% For arXiv preprint (before publication):
% \cite{resnetbk2024arxiv}
%
% For comparison with Mamba:
% \cite{mamba2023}
%
% For theoretical foundations:
% \cite{newton1982scattering,mourre1981limiting,birman1962spectral}
