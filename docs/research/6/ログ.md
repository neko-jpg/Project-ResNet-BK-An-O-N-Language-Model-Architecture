1) まずログが示してる“事実”

添付の Step Timing Log だと、

平常時はだいたい 5.1〜5.6秒/step

でも Step 3968 が 36,995.9ms（約37秒） になってる
これ、ラベルが NORMAL なのがポイントで、「そのstep自体が“チェックポイント保存を含んだstep”」 です（POST-CKPTは“保存した次のstepから”付く仕様）。

さらにその直後、Step 3970〜3991 が 15〜22秒/step まで落ちてる（長く続く重さ）

つまり「保存処理そのものが重い」＋「保存後もしばらく重い」の両方が起きています。

2) コード上の“犯人候補”トップ3（濃厚順）
A. MakefileがRustのcheckpoint-saverを“裏で起動”してる（超濃厚）

make train-japanese / resume-japanese が、学習開始前に checkpoint-saver daemon をバックグラウンド起動しています。
そして checkpoint-saver/config.toml が compression enabled = true。

このdaemon、実装を見ると「新しい step_*.pt を検知した瞬間に」：

.pt を 丸ごとメモリに読み込み

zstd圧縮して .zst を同じディレクトリに書く

さらに古いチェックポイント削除（cleanup）

をやります。

これが何を意味するかというと：

チェックポイント保存直後に、CPUとディスクIOをガッツリ奪う別プロセスが走る

GPU計算自体は速くても、カーネル起動やCPU側処理が詰まって step が 3〜4倍遅くなる（ログの 15〜22秒/step はまさにこの形）

B. チェックポイントが /mnt/c（Windows側のファイルシステム）に書かれている可能性（超濃厚）

ログに ...:/mnt/c/dev/... が出ていたので、レポジトリごとWindows側に置いてWSLから回している可能性が高いです。
WSLの /mnt/c は 巨大ファイルの連続書き込みが遅くなりやすい ので、torch.save() が一気に重くなりがちです。

Step 3968 が約37秒になってるのは、まさに「巨大ptを /mnt/c に保存」した時の典型症状です。
その後にRustが同じ巨大ptを“読み直して圧縮”するので、さらに地獄が延長します。

C. train_phase8.py 側の後処理が強い（中〜濃厚）

チェックポイント後に：

gc.collect() を3回

torch.cuda.empty_cache()（毎回）

torch.cuda.synchronize() 多発
をやっています。

これは「保存直後の一発目が少し遅くなる」原因になります（Step 3473の 8.1秒みたいな “軽いスパイク” はこれっぽい）。
ただ、15〜22秒/stepが何十stepも続くのは、これ単体では説明しづらく、やはりA/Bが主因です。

3) 最短で効く“対処”ランキング（ここからやると勝ちやすい）
① Rust checkpoint-saver の圧縮を止める（or 優先度を落とす）

いったん結論：学習中は圧縮しない。 圧縮は学習止めてからでOKです。

すぐ試す（学習中に止める）
pkill -f "checkpoint-saver" || true

“起動するけど弱くする”（1行）
(source ~/.cargo/env 2>/dev/null; cd checkpoint-saver && RUST_LOG=warn nice -n 19 ionice -c3 cargo run --release -q -- --config config.toml --no-compress &)

設定で止める（config.toml）
[compression] enabled = false

さらに重要：Python側も古いckpt削除してるので、daemonのcleanupと二重になります。二重削除はレースや余計なIOになります。
→ どっちか片方に統一してください（おすすめは「学習中はPythonだけ」）。

② checkpoints を /home（WSLのext4）側に逃がす（効果デカい）

configs/phase8_10b_japanese.yaml の

save_dir: checkpoints/phase8_10b_japanese

を例えば

save_dir: /home/arat2/checkpoints/phase8_10b_japanese

に変えるだけで、torch.save の体感が激変することが多いです。
（E組で例えるなら「黒板に書く」→「紙に書く」くらい速度が違うやつです。）

③ Pythonの保存を“原子的（tmp→rename）”にして、daemonの誤爆も潰す

Rust daemon は .tmp を無視する設計なので、Python側をこうすると安全＆余計な二度処理が減ります。

tmp_path = path + ".tmp"
torch.save(checkpoint, tmp_path)
os.replace(tmp_path, path)  # atomic rename


これで「書きかけの .pt を見つけて圧縮開始」みたいな事故も避けられます。

4) “徹底調査”用：切り分けコマンド（全部1行で）

どれが犯人かを即断する用です。

daemon が暴れてるか確認
ps aux | grep -E "checkpoint-saver|zstd" | grep -v grep

CPU/IOが保存直後に跳ねてるか（要sudoなこと多い）
sudo iotop -oPa

ある瞬間だけstepが伸びる原因がGPU待ちかを見る
nvidia-smi dmon -s pucm -d 1

「/mnt/c が原因か」確認（チェックポイントの実体パスを見る）
python - <<'PY'\nimport os; print(os.path.abspath('checkpoints/phase8_10b_japanese'))\nPY

5) 追加のコードレビュー指摘（地味に効く）

config.use_torch_compile: false と書いてあるのに、実際のコードは config.compile を見て torch.compile を掛けてます。
→ 「compile無効のつもりなのに動いてる」状態になり得ます。これ、再コンパイル絡みの不可解な重さの温床です。
対策：if config.use_torch_compile and hasattr(torch,'compile'): に寄せるのが安全。

torch.cuda.empty_cache() を毎回やるのは、次stepの再確保コストを増やしがち。
→ “メモリ逼迫した時だけ”に条件化する方が、速度安定に効きます。