Hyperbolic Data Compression (アルゴリズム 4) の現状と改善提案
実装の概要と課題

アルゴリズム 4「Hyperbolic Data Compression」では、入力データをローレンツモデル上の高次元ハイパーボリック空間に埋め込み、階層的な代表点選択によりデータセットを圧縮しています。主な特徴は以下の通りです。

ローレンツモデルによる埋め込み：ユークリッド空間のベクトルをローレンツ双曲線へ指数写像で変換し
github.com
、内積や距離計算もローレンツ計量で実装されています
github.com
。ポアンカレ球モデルに比べて境界付近の不安定性がなく、低次元でも歪みが小さいと報告されています
ai.meta.com
。

Isotropic Gaussian Loss：埋め込みがハイパーボリック空間の境界に集中する「次元崩壊」を防ぐために、接空間（ユークリッド空間）上で等方的なシェル分布を課す損失を導入しています
github.com
。ハイパーボリック表現の有効ランクを高めることで表現の多様性を維持する狙いがあります
arxiv.org
。

階層的圧縮：フレシェ平均に基づき、データを近い点群に分割し代表点を選ぶ再帰的な手法が実装されています
github.com
。

情報保持率の計算：実装では圧縮前後の分散比を利用し情報保持率を算出しています
github.com
。しかし、分散だけでは高次元データの構造やクラス境界を十分に評価できません。

ベンチマークでは 圧縮率≥10^6x、情報保持率≥95% が求められますが、現在は目標を満たしていません。target_compression が既定で100であり、アルゴリズムが 1e6 倍圧縮を目指す設定になっていないことも原因です。また、情報保持率の評価指標が粗く、高次元構造が失われている可能性があります。

最新研究に基づく改善策
1. 圧縮率の実現に向けた構造変更

ターゲット圧縮率の増加：target_compression を 10^6 に近い値に設定し、max_depth の計算を圧縮比に合わせて調整する。デフォルトでは int(log2(target_compression)) + 3 であり
github.com
、大きな圧縮比では深い階層が必要になります。

再帰的クラスター数の制御：現在のアルゴリズムは中央値で分割し代表点を選んでいますが
github.com
、クラスタ数を動的に決定する K-means 風の手法や、ハイパーボリック k‑medoids アルゴリズムを導入すれば圧縮効果が高まります。

量子化によるさらなる圧縮：圧縮後のローレンツ埋め込みを イソトロピック反復量子化で2値または多値符号に変換する手法が提案されています
researchgate.net
。この方法は30倍以上の圧縮率を達成しながら性能を維持しているため、ハイパーボリック表現にも応用できます。

2. 情報保持率と歪み評価の改善

有効ランクや分散保存率以外の指標：ハイパーボリック表現の情報量を評価するには、最近傍分類の精度、クロスエントロピー損失、ペアワイズ距離の歪み（distortion）など複数の指標を用いるべきです。現行コードでは一部で歪み平均を算出していますが
github.com
、get_kpi_results() に反映されていません。distortion もKPIに含め、閾値を設定して監視します。

ハイパーボリック階層的コントラスト学習 (HHCL)：最近提案された Hyperbolic Hierarchical Contrastive Loss は、ハイパーボリック空間内で多階層の語彙・意味階層を一貫性を持って学習するための損失です。HHCLでは、接空間での等方的ガウス損失と等方性ペナルティを組み合わせることで表現崩壊を防ぎます
arxiv.org
。この損失を階層的圧縮パイプラインに取り入れることで、情報保持率を高められます。

3. Isotropic Gaussian Loss の強化

重みの調整：現在 isotropic_weight は0.1に設定されています
github.com
。研究では等方的ガウス損失の重みを適切に増やすことで有効ランクが上昇し、表現の多様性が大きくなることが示されています
arxiv.org
。ハイパーパラメータ探索を行い、例えば 0.5〜1.0 の範囲で最適な値を決定します。

可変スケールの採用：データセットサイズや次元によって表現の広がりが異なるため、ミニバッチごとにガウス半径をスケールさせるなど適応的な正則化が効果的です。

4. その他の提案

ローレンツモデルの数値安定性の改善：指数写像や対数写像の前後で入力ノルムをクリッピングしています
github.com
github.com
。より厳密には、torch.clamp ではなく torch.where で条件分岐することで勾配消失を避けられます。

異なるハイパーボリックモデルの併用：ローレンツモデルの他に、ポアンカレ半空間やジオデシック偏角モデルを用いることで異なる歪み特性を活用できます。各モデル間でエンコーダ・デコーダを学習してアンサンブルを作ると保情報率が上がる場合があります。

まとめ

Hyperbolic Data Compression の現在の実装はローレンツモデルと Isotropic Gaussian Loss により先進的な基盤を整えていますが、ターゲット圧縮率と評価指標が目標に対して不十分です。圧縮目標値や階層深さの調整、量子化を含む追加の圧縮ステージ、HHCL など最新の表現学習技術の導入により、圧縮率≥10^6x と 情報保持率≥95% を達成できる可能性があります。また、情報保持率の評価には有効ランク・歪み・分類性能といった複数の指標を採用し、Isotropic Gaussian Loss の重みを適応的に最適化することが重要です。