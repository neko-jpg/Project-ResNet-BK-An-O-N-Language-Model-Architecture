---

## **0\. 全体構成イメージ**

1ファイルにまとめるなら、だいたいこんな順番がおすすめです。

1. 共通ユーティリティ（BF16 用の safe atanh / クリッピング / 勾配フック）

2. 双曲線形層 `ReliableMobiusLinear`

3. Lorentz モデルラッパ `LorentzWrapper`

4. BitNet 版線形層 `BitLinearHyperbolic`

5. 接空間 Attention `HyperbolicTangentAttention`

6. Symplectic Green’s Initialization `symplectic_greens_init_`

以下は、そのまま `hyperbolic_modules.py` みたいなファイルに入れられるコードとして書いています。

---

## **1\. 共通ユーティリティ**

from \_\_future\_\_ import annotations

import math  
from typing import Optional

import torch  
from torch import nn  
from torch.nn import functional as F  
from geoopt.manifolds import PoincareBall

\# \============================================================  
\# bfloat16 向けの安全な atanh / Poincaré ball プロジェクション  
\# \============================================================

def safe\_atanh\_bf16(x: torch.Tensor, eps: float \= 1e-2) \-\> torch.Tensor:  
    """  
    bfloat16 を前提にした安全版 atanh.

    eps:  
        境界 1.0 からどれだけ離してクランプするか。  
        bfloat16 の machine epsilon (\~0.0078) より少し大きめを取る。:contentReference\[oaicite:1\]{index=1}  
    """  
    x\_clamped \= torch.clamp(x, min=-1.0 \+ eps, max=1.0 \- eps)  
    return torch.atanh(x\_clamped)

def safe\_project\_disk\_bf16(  
    x: torch.Tensor,  
    c: float \= 1.0,  
    eps: float \= 1e-2,  
) \-\> torch.Tensor:  
    """  
    Poincaré ball の境界より確実に内側に投影する。

    x: \[..., dim\]  
    c: 負曲率パラメータ（PoincaréBall(c) と整合させる）  
    eps: bfloat16 向けの安全マージン  
    """  
    \# ノルムを計算  
    norm \= x.norm(dim=-1, keepdim=True)

    \# Poincaré ball の半径は 1/sqrt(c)  
    maxnorm \= (1.0 \- eps) / math.sqrt(c)

    \# ゼロ除算防止  
    norm\_safe \= norm.clamp(min=1e-12)

    \# 境界を超えそうな場合のみスケーリング  
    projected \= x / norm\_safe \* maxnorm  
    cond \= norm \> maxnorm  
    return torch.where(cond, projected, x)

def register\_grad\_clamp\_hook(  
    tensor: torch.Tensor,  
    clip\_value: float \= 1000.0,  
) \-\> None:  
    """  
    勾配爆発を防ぐための簡易クリッピング Hook. :contentReference\[oaicite:2\]{index=2}

    例:  
        register\_grad\_clamp\_hook(self.weight)  
    """  
    if tensor.requires\_grad:  
        tensor.register\_hook(  
            lambda grad: grad.clamp\_(min=-clip\_value, max=clip\_value)  
        )

---

## **2\. 双曲線形層 ReliableMobiusLinear**

レポートの `ReliableMobiusLinear` を、実運用＆bf16 対応を意識した形に整えたものです。

* 重み `weight` は **接空間 (Euclidean)** パラメータ

* `logmap0 / expmap0` だけ FP32 で実行

* それ以外は元の dtype（bf16 など）で計算

class ReliableMobiusLinear(nn.Module):  
    """  
    Topic 1 Solution:  
    \- 重みは接空間 (Euclidean) パラメータ  
    \- Poincaré ball 上の点を logmap0 \-\> 線形 \-\> expmap0 で変換

    期待する入出力:  
        x : \[..., in\_features\] on Poincaré ball  
        y : \[..., out\_features\] on Poincaré ball  
    """

    def \_\_init\_\_(  
        self,  
        in\_features: int,  
        out\_features: int,  
        c: float \= 1.0,  
        precision: str \= "fp32",  \# "fp32" or "bf16"  
        bias: bool \= True,  
        use\_safe\_projection: bool \= True,  
    ) \-\> None:  
        super().\_\_init\_\_()

        self.in\_features \= in\_features  
        self.out\_features \= out\_features  
        self.c \= float(c)  
        self.precision \= precision  
        self.use\_safe\_projection \= use\_safe\_projection

        \# geoopt の PoincareBall  
        self.ball \= PoincareBall(c=self.c)

        \# 重みはあくまで Euclidean パラメータ  
        self.weight \= nn.Parameter(torch.empty(out\_features, in\_features))

        if bias:  
            self.bias \= nn.Parameter(torch.empty(out\_features))  
        else:  
            self.register\_parameter("bias", None)

        self.reset\_parameters()

    def reset\_parameters(self) \-\> None:  
        \# 接空間上の通常の初期化  
        nn.init.kaiming\_uniform\_(self.weight, a=math.sqrt(5.0))

        if self.bias is not None:  
            fan\_in, \_ \= nn.init.\_calculate\_fan\_in\_and\_fan\_out(self.weight)  
            bound \= 1.0 / math.sqrt(fan\_in)  
            nn.init.uniform\_(self.bias, \-bound, bound)

    def \_to\_tangent(self, x: torch.Tensor) \-\> torch.Tensor:  
        """  
        Poincaré \-\> 接空間  
        precision=="bf16" の場合は一時的に FP32 で logmap0 を評価。  
        """  
        if self.use\_safe\_projection and x.dtype \== torch.bfloat16:  
            \# 境界付近の点を安全な範囲に投影  
            x \= safe\_project\_disk\_bf16(x, c=self.c)

        if self.precision \== "bf16" and x.dtype \== torch.bfloat16:  
            x\_fp32 \= x.float()  
            v \= self.ball.logmap0(x\_fp32)  \# FP32 で logmap0  
            return v.to(torch.bfloat16)  
        else:  
            return self.ball.logmap0(x)

    def \_from\_tangent(self, v: torch.Tensor, dtype: torch.dtype) \-\> torch.Tensor:  
        """  
        接空間 \-\> Poincaré  
        precision=="bf16" の場合は FP32 で expmap0 を評価。  
        """  
        if self.precision \== "bf16" and v.dtype \== torch.bfloat16:  
            v\_fp32 \= v.float()  
            y \= self.ball.expmap0(v\_fp32)  
            return y.to(dtype)  
        else:  
            y \= self.ball.expmap0(v)  
            return y.to(dtype)

    def forward(self, x: torch.Tensor) \-\> torch.Tensor:  
        """  
        x: \[..., in\_features\] on Poincaré ball  
        """  
        orig\_dtype \= x.dtype

        \# 1\. Manifold \-\> Tangent  
        x\_tan \= self.\_to\_tangent(x)

        \# 2\. Linear (接空間のユークリッド演算)  
        out\_tan \= F.linear(x\_tan, self.weight, self.bias)

        \# 3\. Tangent \-\> Manifold  
        out \= self.\_from\_tangent(out\_tan, dtype=orig\_dtype)  
        return out

---

## **3\. Lorentz モデルラッパ（戦略B）**

レポートの Strategy B「Lorentz Wrapper」を、任意のモジュールを包める形で実装しています。

* `poincare_to_lorentz`: `[..., n] -> [..., n+1]`

* `lorentz_to_poincare`: `[..., n+1] -> [..., n]`

* 中身の `module` は「Lorentz 座標のまま」演算する層（Minkowski 内積や Lorentzian Attention など）を想定

class LorentzWrapper(nn.Module):  
    """  
    Poincaré 座標の入力をいったん Lorentz (Hyperboloid) モデルに写像し、  
    内側の module で演算してから Poincaré に戻すためのラッパー。

    例:  
        core \= SomeLorentzModule(...)  
        layer \= LorentzWrapper(core, c=1.0)  
        y \= layer(x\_poincare)  
    """

    def \_\_init\_\_(  
        self,  
        module: nn.Module,  
        c: float \= 1.0,  
        eps: float \= 1e-3,  
    ) \-\> None:  
        super().\_\_init\_\_()  
        self.module \= module  
        self.c \= float(c)  
        self.eps \= eps

    def poincare\_to\_lorentz(  
        self,  
        x: torch.Tensor,  
    ) \-\> torch.Tensor:  
        """  
        x: \[..., n\] on Poincaré ball  
        returns z: \[..., n+1\] on Lorentz model  
        """  
        \# ノルムを境界より少し内側にクランプ  
        xn \= x.norm(dim=-1, keepdim=True).clamp(max=1.0 \- self.eps)  
        xn2 \= xn \*\* 2

        \# Poincaré \-\> Lorentz の標準写像（c=1 ケース）に相当  
        factor \= 1.0 / (1.0 \- xn2)  
        z0 \= (1.0 \+ xn2) \* factor  
        z\_spatial \= 2.0 \* x \* factor  
        return torch.cat((z0, z\_spatial), dim=-1)

    def lorentz\_to\_poincare(  
        self,  
        z: torch.Tensor,  
    ) \-\> torch.Tensor:  
        """  
        z: \[..., n+1\] on Lorentz model  
        returns x: \[..., n\] on Poincaré ball  
        """  
        z0 \= z\[..., 0:1\]  
        z\_spatial \= z\[..., 1:\]  
        \# x \= z\_spatial / (z0 \+ 1\)  
        denom \= z0 \+ 1.0  
        return z\_spatial / denom

    def forward(self, x\_poincare: torch.Tensor, \*args, \*\*kwargs) \-\> torch.Tensor:  
        \# 1\. Poincaré \-\> Lorentz  
        z \= self.poincare\_to\_lorentz(x\_poincare)

        \# 2\. Lorentz 空間での演算  
        z\_out \= self.module(z, \*args, \*\*kwargs)

        \# 3\. Lorentz \-\> Poincaré  
        x\_out \= self.lorentz\_to\_poincare(z\_out)  
        return x\_out

---

## **4\. BitNet b1.58 on Tangent Space: BitLinearHyperbolic**

Topic 3 の BitNet b1.58 対応線形層を、

* 接空間での量子化

* 任意 bit の活性量子化（デフォルト 8bit）

* バイアスあり / なし  
   に対応するよう整理したものです。

class BitLinearHyperbolic(nn.Module):  
    """  
    BitNet b1.58 風の量子化を接空間で行う双曲線形層。

    流れ:  
        x\_h : Poincaré (or other hyperbolic) coords  
        \-\> logmap0 : tangent  
        \-\> activation quantization (8bit)  
        \-\> weight quantization (1.58bit ≒ {-1,0,1})  
        \-\> F.linear  
        \-\> expmap0 : hyperbolic  
    """

    def \_\_init\_\_(  
        self,  
        in\_features: int,  
        out\_features: int,  
        c: float \= 1.0,  
        bias: bool \= True,  
        activation\_bits: int \= 8,  
    ) \-\> None:  
        super().\_\_init\_\_()  
        self.in\_features \= in\_features  
        self.out\_features \= out\_features  
        self.c \= float(c)  
        self.activation\_bits \= activation\_bits

        self.ball \= PoincareBall(c=self.c)

        \# 重みは接空間の Euclidean パラメータ  
        self.weight \= nn.Parameter(torch.empty(out\_features, in\_features))

        if bias:  
            self.bias \= nn.Parameter(torch.empty(out\_features))  
        else:  
            self.register\_parameter("bias", None)

        self.reset\_parameters()

    def reset\_parameters(self) \-\> None:  
        nn.init.kaiming\_uniform\_(self.weight, a=math.sqrt(5.0))  
        if self.bias is not None:  
            fan\_in, \_ \= nn.init.\_calculate\_fan\_in\_and\_fan\_out(self.weight)  
            bound \= 1.0 / math.sqrt(fan\_in)  
            nn.init.uniform\_(self.bias, \-bound, bound)

    \# \---------- Weight quantization: AbsMean \+ {-1,0,1} \----------

    def weight\_quantization(self, w: torch.Tensor) \-\> torch.Tensor:  
        """  
        BitNet b1.58: AbsMean-based scaling \+ {-1, 0, 1} 量子化.  
        逆伝播は Straight-Through Estimator (STE).  
        """  
        gamma \= w.abs().mean().clamp(min=1e-5)  
        w\_scaled \= w / gamma  
        w\_ternary \= w\_scaled.round().clamp(-1, 1\)  
        w\_quant \= w\_ternary \* gamma

        \# STE : forward は w\_quant, backward は w と同じ勾配  
        return (w\_quant \- w).detach() \+ w

    \# \---------- Activation quantization: per-token symmetric int \----------

    def activation\_quantization(self, x: torch.Tensor) \-\> torch.Tensor:  
        """  
        接空間上の活性を per-token（最後の次元）ごとに 8bit 量子化。  
        """  
        qmax \= float(2 \*\* (self.activation\_bits \- 1\) \- 1\)  \# 8bit \-\> 127  
        \# max along feature dim  
        max\_abs \= x.abs().max(dim=-1, keepdim=True).values.clamp(min=1e-5)  
        scale \= qmax / max\_abs

        x\_int \= (x \* scale).round().clamp(-qmax, qmax)  
        x\_quant \= x\_int / scale

        \# STE  
        return (x\_quant \- x).detach() \+ x

    def forward(self, x\_h: torch.Tensor) \-\> torch.Tensor:  
        """  
        x\_h: \[..., in\_features\] on hyperbolic manifold  
        戻り値: \[..., out\_features\] on hyperbolic manifold  
        """  
        orig\_dtype \= x\_h.dtype

        \# 1\. Manifold \-\> Tangent (FP32 で logmap0 を計算)  
        x\_fp32 \= x\_h.float()  
        x\_tangent \= self.ball.logmap0(x\_fp32)

        \# 2\. Activation Quantization  
        x\_quant \= self.activation\_quantization(x\_tangent)

        \# 3\. Weight Quantization  
        w\_quant \= self.weight\_quantization(self.weight)

        \# 4\. BitLinear (接空間の線形演算)  
        y\_tangent \= F.linear(x\_quant, w\_quant, self.bias)

        \# 5\. Tangent \-\> Manifold  
        y\_h \= self.ball.expmap0(y\_tangent)  
        return y\_h.to(orig\_dtype)

---

## **5\. Tangent Space Attention: HyperbolicTangentAttention**

Topic 4 の「Tangent Space Attention」を、PyTorch 標準の `nn.MultiheadAttention` ベースで実装したものです。

* 入力 `query_h, key_h, value_h` は **双曲空間上の埋め込み**

* `logmap0` で接空間へ

* 通常の Scaled Dot-Product Attention（FlashAttention カーネルも利用可能）

* 接空間で Residual を足してから `expmap0` で戻す

class HyperbolicTangentAttention(nn.Module):  
    """  
    Tangent Space Attention:  
    \- 入力: Poincaré (or hyperbolic) 埋め込み  
    \- Attention 本体: 接空間 (Euclidean) で MultiheadAttention  
    \- 出力: 再び双曲空間へ expmap0 で戻す  
    """

    def \_\_init\_\_(  
        self,  
        embed\_dim: int,  
        num\_heads: int,  
        c: float \= 1.0,  
        dropout: float \= 0.0,  
        batch\_first: bool \= True,  
    ) \-\> None:  
        super().\_\_init\_\_()  
        self.c \= float(c)  
        self.ball \= PoincareBall(c=self.c)  
        self.embed\_dim \= embed\_dim

        self.mha \= nn.MultiheadAttention(  
            embed\_dim=embed\_dim,  
            num\_heads=num\_heads,  
            dropout=dropout,  
            batch\_first=batch\_first,  
        )

    def forward(  
        self,  
        query\_h: torch.Tensor,  
        key\_h: torch.Tensor,  
        value\_h: torch.Tensor,  
        attn\_mask: Optional\[torch.Tensor\] \= None,  
        key\_padding\_mask: Optional\[torch.Tensor\] \= None,  
    ) \-\> torch.Tensor:  
        """  
        query\_h, key\_h, value\_h:  
            shape: (batch, seq, embed\_dim) if batch\_first=True

        戻り値:  
            output\_h: 同じ形状で双曲空間の点。  
        """

        orig\_dtype \= query\_h.dtype

        \# 1\. Poincaré \-\> Tangent (FP32 で logmap0)  
        q\_t \= self.ball.logmap0(query\_h.float())  
        k\_t \= self.ball.logmap0(key\_h.float())  
        v\_t \= self.ball.logmap0(value\_h.float())

        \# 2\. Scaled Dot-Product Attention (Euclidean)  
        attn\_output\_t, \_ \= self.mha(  
            q\_t.to(orig\_dtype),  
            k\_t.to(orig\_dtype),  
            v\_t.to(orig\_dtype),  
            attn\_mask=attn\_mask,  
            key\_padding\_mask=key\_padding\_mask,  
            need\_weights=False,  
        )

        \# 3\. ResNet-style Residual in Tangent Space  
        output\_t \= q\_t \+ attn\_output\_t.float()

        \# 4\. Tangent \-\> Manifold  
        output\_h \= self.ball.expmap0(output\_t)  
        return output\_h.to(orig\_dtype)

---

## **6\. Symplectic Green’s Initialization (SGI)**

Topic 5 の初期化を、直接 `nn.Linear` などにも適用しやすい形にしたものです。

def symplectic\_greens\_init\_(  
    tensor: torch.Tensor,  
    dt: float \= 0.01,  
    min\_decay: float \= 1e-4,  
    max\_decay: float \= 1e-2,  
) \-\> None:  
    """  
    Symplectic Green's Initialization (SGI).

    W\_init \= Q \* D  
        Q: orthogonal (近似的にエネルギー保存的な回転/混合)  
        D: diag(exp(-gamma \* dt)), |eigenvalues| \< 1 だが 1 に非常に近い

    Hidden x Hidden の再帰行列 / 状態遷移に使うことを想定。  
    """

    if tensor.ndim \!= 2:  
        \# 2次元でない場合は通常の orthogonal init にフォールバック  
        nn.init.orthogonal\_(tensor)  
        return

    rows, cols \= tensor.shape  
    if rows \!= cols:  
        \# 正方でない場合も orthogonal のみにする（例えば projection など）  
        nn.init.orthogonal\_(tensor)  
        return

    with torch.no\_grad():  
        \# 1\. Random orthogonal Q  
        Q \= torch.empty(rows, cols, device=tensor.device, dtype=tensor.dtype)  
        nn.init.orthogonal\_(Q)

        \# 2\. Green's-style decay factors  
        gamma \= torch.rand(rows, device=tensor.device, dtype=tensor.dtype)  
        gamma \= gamma \* (max\_decay \- min\_decay) \+ min\_decay  
        decay\_factors \= torch.exp(-gamma \* dt)

        D \= torch.diag(decay\_factors)

        \# 3\. Combine W \= Q @ D  
        W\_init \= Q @ D  
        tensor.copy\_(W\_init)

使用例:

class BKCoreCell(nn.Module):  
    def \_\_init\_\_(self, hidden\_dim: int):  
        super().\_\_init\_\_()  
        self.A \= nn.Parameter(torch.empty(hidden\_dim, hidden\_dim))  
        symplectic\_greens\_init\_(self.A)  
        \# 他のパラメータもここで定義

---

## **7\. 次のステップ案**

もし「レポートに載せるためのコード断片」としても読みやすくしたい場合は：

* 上の各クラス／関数を**Topic ごとに分割した小さめのコードブロック**にして載せる

* 本文では

  * 「重みは接空間パラメータとし、ManifoldParameter は使わない」

  * 「logmap / expmap は FP32 で評価し、bf16 は線形演算とストレージのみに使う」

  * 「量子化は必ず接空間上で行う」

  * 「Attention は Tangent Space Attention を採用」

  * 「BK-Core の再帰行列には SGI を適用」  
     といった**実装原則**を箇条書きでまとめる

ような構成にすると、「理論 → 原則 → 具体コード」が綺麗に接続されると思います。

---

もし「このモジュールとこのモジュールを組み合わせて、ResNet-BK の 1 ブロック分の `nn.Module` を書いてほしい」など、もう一段具体的なレベルが必要でしたら、その前提でブロック定義まで展開します。

[Catch the Quantum Wave... Password: spinor](https://pulsr.co.uk/spinor.html)

