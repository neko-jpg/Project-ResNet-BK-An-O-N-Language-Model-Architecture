ResNet-BK (10B Params) 技術監査報告書：数値的不安定性の幾何学的・物理学的解明と実装指針プロジェクト: Project ResNet-BK対象アーキテクチャ: BK-Core (Green's Function), Hyperbolic Attention, BitNet b1.58報告者: Lead Researcher, Geometric Deep Learning & Mathematical Physics Laboratory日付: 2025年12月9日1. 序論：物理、幾何、量子化の統合における特異点現在開発中の「ResNet-BK (10B params)」は、深層学習の歴史において極めて野心的な試みである。$O(N)$ の計算複雑性で長距離依存性を捉える物理ベースの BK-Core、データの階層性を自然に埋め込む Hyperbolic Attention、そして推論効率を極限まで高める BitNet b1.58 という、本来異なる数学的基盤を持つ3つのパラダイムを統合しようとしているからである。しかし、Dry Run において観測された勾配ノルム $134,923.5$ という異常値と全パラメータの NaN 化は、この統合モデルが「数値的な事象の地平面（Numerical Event Horizon）」に突入したことを示唆している。本レポートは、発生した「Mathematical Explosion（数学的爆発）」の原因を、リーマン幾何学、数値解析、および量子化理論の観点から徹底的に解剖し、実装レベルでの解決策を提示するものである。我々の分析によれば、この問題は単なるハイパーパラメータの調整ミスではなく、ユークリッド空間（接空間）とリーマン多様体の定義域の混同、および bfloat16 演算における共形係数（Conformal Factor）の特異点挙動 に起因する構造的な欠陥である。以下、5つの核心的問いに対し、数学的証明と実装コードを交えた詳細な分析を行う。これは単なるバグ修正ではなく、幾何学的深層学習を10Bスケールで成功させるための新たな理論的枠組みの提示である。Topic 1: Riemannian Optimization for Linear Layers1.1 The Verdict（結論）結論: 双曲ニューラルネットワーク（HNN）における nn.Linear 層の重み行列 $W$ は、双曲多様体上の点（Manifold Parameter）として定義してはならない。$W$ はあくまで、多様体の原点における 接空間（Tangent Space $T_0\mathbb{D} \cong \mathbb{R}^n$）上で作用する線形変換行列 として定義し、ユークリッド空間のパラメータとして最適化すべきである。現在発生している勾配爆発の主因の一つは、オプティマイザ（BK-HyperSGD）が線形層の重み行列 $W$ を「Poincaré Ball 内部の点」として扱おうとし、強制的にノルム制約（$\|W\| < 1$）を課したことにあると断定できる。行列のフロベニウスノルムやスペクトルノルムは容易に1を超えるため、これを無理やり単位球内に押し込める操作（Retraction/Projection）が、重みの値を極小化、あるいは境界付近へ張り付かせ、その結果として双曲計量テンソルによる勾配のスケーリング係数が発散したと考えられる。1.2 Mathematical Proof（根拠）Möbius Gyrovector Space における線形変換の幾何学双曲ニューラルネットワークの基礎理論において、線形層はしばしば「Möbius Matrix-Vector Multiplication」として記述される。Poincaré Ball モデル $(\mathbb{D}^n_c, g_x)$ 上の点 $x$ に対する行列 $M \in \mathbb{R}^{m \times n}$ の作用 $M \otimes_c x$ は、以下のように定義される 1。$$M \otimes_c x = \tanh \left( \frac{\|Mx\|}{\|x\|} \tanh^{-1}(\|x\|) \right) \frac{Mx}{\|Mx\|}$$この式を詳細に解析すると、$M$ は入力ベクトル $x$ に対して直接作用しているのではなく、暗黙的に 対数写像（Log Map）によって接空間に射影されたベクトル に対して作用していることがわかる。より形式的には、この操作は以下の3ステップの合成関数と等価である。Logarithmic Map: 入力 $x \in \mathbb{D}^n_c$ を原点の接空間 $T_0\mathbb{D}^n_c$ へ写像する。$$v = \log_0^c(x) \in \mathbb{R}^n$$Euclidean Linear Transformation: 接空間（ユークリッド空間）上で、行列 $M$ による通常の線形変換を行う。$$v' = Mv \in \mathbb{R}^m$$Exponential Map: 変換後のベクトル $v'$ を双曲空間 $\mathbb{D}^m_c$ へ戻す。$$y = \exp_0^c(v') \in \mathbb{D}^m_c$$このプロセスにおいて、行列 $M$ は接空間から接空間への写像 $T_0\mathbb{D}^n_c \to T_0\mathbb{D}^m_c$ を担う作用素である。接空間はベクトル空間（ユークリッド空間）と同型であるため、$M$ は通常の $\mathbb{R}^{m \times n}$ の元であり、曲率負の空間に拘束される理由はない 4。勾配爆発のメカニズムもし $M$ を双曲多様体上のパラメータとして定義した場合、リーマン勾配 $\text{grad} f(M)$ はユークリッド勾配 $\nabla f(M)$ に逆計量テンソル $g_M^{-1}$ を掛けたものになる 5。$$\text{grad} f(M) = \frac{(1 - \|M\|^2)^2}{4} \nabla f(M)$$しかし、これは $M$ がベクトル（点）の場合の話である。行列 $M$ 全体を多様体上の点として扱うと、次元の呪いにより $\|M\|_F$ は初期化直後から大きくなりやすく、正規化によって要素が極端に小さくなるか、あるいは最適化の過程で境界 $\|M\| \to 1$ に近づく。境界付近では計量 $\lambda_M = \frac{2}{1-\|M\|^2}$ が無限大に発散するため、わずかなユークリッド勾配がリーマン勾配変換時に爆発的な値を持つことになる。ResNet-BK で観測された勾配ノルム $134,923.5$ は、まさにこの計量テンソルの発散（$1/(1-\|W\|^2)$ 項の爆発）を示唆している。正しいアプローチ：接空間パラメータとしての $W$重み $W$ は接空間上のパラメータ（Euclidean Parameter）として定義し、最適化には通常の AdamW を使用すべきである。一方、バイアス項 $b$ については、双曲空間上の平行移動（Möbius Addition）として定義する場合、多様体上の点として扱う余地があるが、数値的安定性の観点からは、バイアスも接空間上で加算してから Exponential Map を適用する手法（HNN++等で採用）が推奨される 7。1.3 Pseudo-Code（実装案）以下の実装は、geoopt ライブラリの思想を継承しつつ、10Bモデル向けに数値的安定性を強化したものである。Pythonimport torch
import torch.nn as nn
import torch.nn.functional as F
from geoopt.manifolds import PoincareBall

class ReliableMobiusLinear(nn.Module):
    """
    Topic 1 Solution:
    Linear layer where weights are defined in the tangent space (Euclidean),
    avoiding Riemannian optimization instability for the weight matrix.
    """
    def __init__(self, in_features, out_features, c=1.0, precision='float32'):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.c = c
        # geooptのPoincareBallを使用するが、パラメータ登録には注意する
        self.ball = PoincareBall(c=c)
        self.precision = precision

        # 【重要】重みは nn.Parameter (Euclidean) として定義
        # ManifoldParameter ではないため、通常のAdamWで最適化可能
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features))
        
        self.reset_parameters()

    def reset_parameters(self):
        # 接空間（ユークリッド）上での初期化
        # Kaiming init 等がそのまま適用可能
        nn.init.kaiming_uniform_(self.weight, a=5**0.5)
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / (fan_in**0.5)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x):
        # x: on Poincare Ball
        
        # 1. Log Map: Manifold -> Tangent Space
        # Topic 2で詳述するが、境界付近の安定性のため float32 キャスト推奨
        if self.precision == 'bfloat16' and x.dtype == torch.bfloat16:
            x_fp32 = x.float()
            x_tan = self.ball.logmap0(x_fp32)
            x_tan = x_tan.bfloat16()
        else:
            x_tan = self.ball.logmap0(x)

        # 2. Linear Transform in Tangent Space (Euclidean operation)
        # y = xW^T + b (bias is added in tangent space for stability)
        output_tan = F.linear(x_tan, self.weight, self.bias)

        # 3. Exp Map: Tangent Space -> Manifold
        if self.precision == 'bfloat16' and output_tan.dtype == torch.bfloat16:
            output_tan_fp32 = output_tan.float()
            output = self.ball.expmap0(output_tan_fp32)
            output = output.bfloat16()
        else:
            output = self.ball.expmap0(output_tan)

        return output
考察: この実装により、重み行列 $W$ に対する勾配更新則は標準的な $\theta_{t+1} = \theta_t - \eta \nabla L$ となり、双曲計量に起因する勾配爆発を回避できる。幾何学的な非線形性は logmap0 と expmap0 に集約され、学習の安定性と表現力が両立される。Topic 2: Numerical Stability of Inverse Exponential Maps2.1 The Verdict（結論）結論: bfloat16 環境における Poincaré Ball モデルは、境界付近（$\|x\| \approx 1$）で構造的な数値崩壊を起こす。これを回避するためには、Lorentz モデル（Hyperboloid モデル）への移行、もしくは bfloat16 のイプシロン特性に基づいた厳格な "Safe Clipping" が必須である。観測されたパラメータの NaN 化は、hybrid_attn や log_c の計算中に、ノルムが $1.0$ に近いベクトルに対し逆正接関数（atanh）が適用された際に発生している。bfloat16 の有効桁数（仮数部7ビット）は非常に低く、$1 - \epsilon$ の微小な差を表現できないため、数学的にはボール内部にあるはずの点が、浮動小数点演算上では「境界上」または「ボール外」と判定され、無限大または複素数領域への逸脱を引き起こしている。2.2 Mathematical Proof（根拠）bfloat16 における「事象の地平面」の解析Poincaré Ball モデルにおける距離や Exp/Log 写像には、共形係数 $\lambda_x = \frac{2}{1 - \|x\|^2}$ や atanh(\|x\|) が頻出する。bfloat16 のマシンイプシロン $\epsilon_{\text{bf16}}$ は $2^{-7} \approx 0.00781$ である 8。これは float32 の $\epsilon_{\text{fp32}} \approx 1.19 \times 10^{-7}$ と比較して極めて粗い。数値表現上、$1.0$ 未満の最大の値は $1 - \epsilon_{\text{bf16}}/2$ 付近となる。もしベクトルのノルム $\|x\|$ が $1 - 0.0039$ を超えると、bfloat16 では $1.0$ に丸められるリスクがある。$$x = 0.997 \xrightarrow{\text{bfloat16}} 1.0 \implies \text{atanh}(x) \to \infty$$さらに、勾配計算においては $(1 - \|x\|^2)$ で除算する項が現れるため、分母がマシンイプシロン以下になると勾配値は $1/\epsilon \approx 128$ 倍どころか、アンダーフローによる $1/0$ で無限大に発散する。これが、勾配ノルム $134,923.5$ の正体である。Lorentz モデルによる幾何学的安定化Lorentz モデル（$\mathcal{L}^n$）は、ミンコフスキー空間 $\mathbb{R}^{n+1}$ 内の双曲面 $ -z_0^2 + z_1^2 + \dots + z_n^2 = -1, z_0 > 0 $ として定義される 9。Lorentz モデル上の点 $z$ の距離関数は $d_{\mathcal{L}}(x, y) = \text{arccosh}(- \langle x, y \rangle_{\mathcal{L}})$ であり、座標値 $z_i$ が大きくなっても特異点（Singularity）を持たない。Poincaré モデルのような「有限領域への押し込め」がないため、大きな値は大きな値として素直に表現され、bfloat16 のダイナミックレンジ（指数部は8ビットあり、広い範囲を扱える）と非常に相性が良い。文献 9 によれば、Lorentz モデルは Poincaré モデルに比べて最適化の観点（Gradient flow）で優れており、特に低精度演算においてその堅牢性が際立つとされる。2.3 Pseudo-Code（実装案）10Bモデルの既存実装への影響を考慮し、Poincaré モデルを維持する場合の「Safe Clipping」と、推奨される「Lorentz 変換」の2つの戦略を提示する。戦略A: bfloat16-Aware Safe Clipping (Minimal Change)bfloat16 の精度限界を明示的に考慮したクリッピング関数を導入する。eps は通常より遥かに大きく取る必要がある。Pythondef safe_atanh_bf16(x):
    """
    bfloat16 safe atanh.
    Machine epsilon for bfloat16 is ~0.0078.
    We need a margin larger than eps/2 to avoid rounding to 1.0.
    """
    # 0.01 (1e-2) は bfloat16 の eps (0.0078) より大きい安全マージン
    EPS = 1e-2 
    x_clamped = torch.clamp(x, min=-1.0 + EPS, max=1.0 - EPS)
    return torch.atanh(x_clamped)

def safe_project_disk_bf16(x, c=1.0):
    """
    Project points to be strictly inside the Poincare ball.
    """
    EPS = 1e-2
    norm = x.norm(dim=-1, keepdim=True)
    maxnorm = (1.0 - EPS) / (c ** 0.5)
    
    # ノルムが境界を超えそうな場合のみスケーリング
    cond = norm > maxnorm
    projected = x / norm * maxnorm
    return torch.where(cond, projected, x)

# 勾配爆発を防ぐためのHook
def register_grad_hook(tensor):
    if tensor.requires_grad:
        tensor.register_hook(lambda grad: torch.clamp(grad, -1000, 1000))
戦略B: Lorentz Space Wrapper (Recommended)Poincaré 上の点 $x$ を Lorentz 座標 $z$ に変換して演算し、最後に Poincaré に戻す。これにより、中間の加算やアテンション操作が境界問題の影響を受けなくなる。Pythonclass LorentzWrapper(nn.Module):
    """
    Wraps operations in Lorentz model for numerical stability.
    """
    def poincare_to_lorentz(self, x, c=1.0, eps=1e-3):
        # x: [..., n] -> z: [..., n+1]
        xn = x.norm(dim=-1, keepdim=True).clamp(max=1.0 - eps)
        xn2 = xn ** 2
        factor = 1.0 / (1.0 - xn2)
        
        z0 = (1.0 + xn2) * factor
        z_spatial = 2.0 * x * factor
        return torch.cat((z0, z_spatial), dim=-1)

    def lorentz_to_poincare(self, z, c=1.0, eps=1e-3):
        # z: [..., n+1] -> x: [..., n]
        z0 = z[..., 0:1]
        z_spatial = z[..., 1:]
        
        # Poincaré射影: x = z_spatial / (z0 + 1)
        # z0 >= 1 なので分母は常に >= 2、数値的に非常に安定
        denom = z0 + 1.0
        return z_spatial / denom

    def forward(self, x):
        z = self.poincare_to_lorentz(x)
        #... Lorentz空間での演算 (Minkowski dot product attention etc.)...
        return self.lorentz_to_poincare(z_out)
推奨: ResNet-BK のような深層モデルでは、各層の出力が徐々に境界へ近づく傾向があるため、各ブロックの入出力で 戦略B (Lorentz Wrapper) を採用し、内部演算を安定化させることを強く推奨する。Topic 3: BitNet b1.58 on Riemannian Manifolds3.1 The Verdict（結論）結論: BitNet の量子化アルゴリズム（重みの平均絶対値によるスケーリングと $\{-1, 0, 1\}$ 丸め）は、リーマン多様体上の点に対して直接適用することは数学的に誤りであり、最適化と共存できない。正しいアプローチは、「接空間量子化 (Tangent Space Quantization)」 である。すなわち、Topic 1 で定義した「接空間上の重み行列 $W$」に対して BitNet の量子化（AbsMean Quantization）を適用し、その量子化された線形変換の結果を Exponential Map で双曲空間へ射影する、という順序を厳守する必要がある。多様体上の点を直接量子化しようとすると、曲率による空間の歪み（体積要素の非一様性）のため、等間隔の量子化グリッドが意味をなさず、情報量が著しく劣化する。3.2 Mathematical Proof（根拠）BitNet b1.58 のアルゴリズムと多様体の不整合BitNet b1.58 12 では、重み $W$ をスケーリング係数 $\gamma$ と量子化重み $W_q \in \{-1, 0, 1\}$ に分解する。$$W \approx \gamma W_q, \quad \gamma = \frac{1}{NM} \sum_{i,j} |W_{ij}|$$$$W_q = \text{Clamp}(\text{Round}(W / \gamma), -1, 1)$$この操作は、重み空間が「平坦かつ均質（Flat and Homogeneous）」であることを前提としている。ユークリッド空間（接空間）では、ベクトル $v$ を $\alpha$ 倍することは、原点からの距離を $\alpha$ 倍することと等価であり、加算も可換である。しかし、双曲空間（Poincaré Ball）においては：距離の非線形性: 原点付近の距離 $0.1$ と、境界付近の距離 $0.1$ は、ユークリッド的な見た目の距離（座標値の差）としては全く異なる。境界付近では座標値のわずかな差が巨大な測地線距離に対応する。量子化グリッドの歪み: もし多様体上の座標を直接 $\{-1, 0, 1\}$ に量子化しようとすると、Poincaré Ball 内では $0$ (原点) と $\pm 1$ (境界、無限遠点) しか表現できなくなる。これは情報の完全な喪失を意味する。QAT on Tangent Spaceしたがって、量子化は線形性（Linearity）が保たれている接空間上で行わなければならない。計算グラフは以下のようになる。Input Mapping: $x_{\text{hyp}} \xrightarrow{\log_0} x_{\text{tan}}$Activation Quantization: $x_{\text{tan}} \to \hat{x}_{\text{tan}}$ (8-bit per-token quantization 12)Weight Quantization: $W_{\text{tan}} \to \hat{W}_{\text{tan}}$ (1.58-bit quantization)$W_{\text{tan}}$ は接空間 $T_0\mathbb{D}$ から $T_0\mathbb{D}$ への写像行列。BitLinear Computation: $y_{\text{tan}} = \hat{W}_{\text{tan}} \hat{x}_{\text{tan}}$Output Mapping: $y_{\text{tan}} \xrightarrow{\exp_0} y_{\text{hyp}}$この構成により、BitNet の計算効率（整数演算、省メモリ）を享受しつつ、双曲空間の階層的埋め込み能力を損なうことなく学習が可能になる。このとき、量子化誤差は接空間上のユークリッド距離として評価されるが、接空間の計量は原点においてリーマン計量と一致するため、局所的には幾何学的整合性が保たれる。3.3 Pseudo-Code（実装案）BitNet b1.58 の実装詳細（STE: Straight-Through Estimator）を組み込んだ、双曲対応版 BitLinear 層を定義する。Pythonclass BitLinearHyperbolic(nn.Module):
    def __init__(self, in_features, out_features, c=1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.c = c
        self.ball = PoincareBall(c=c)
        
        # 重みは接空間（ユークリッド）パラメータ
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        
    def weight_quantization(self, w):
        # BitNet b1.58: AbsMean Quantization
        # スケール gamma の計算
        gamma = w.abs().mean().clamp(min=1e-5)
        
        # スケーリングと丸め (-1, 0, 1)
        w_scaled = w / gamma
        w_quant = w_scaled.round().clamp(-1, 1) * gamma
        
        # STE (Straight-Through Estimator): 
        # 順伝播は w_quant、逆伝播は w の勾配を流す
        return (w_quant - w).detach() + w

    def activation_quantization(self, x):
        # BitNet: AbsMax Quantization (8-bit)
        # x は接空間上のベクトル
        scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp(min=1e-5)
        x_quant = (x * scale).round().clamp(-128, 127) / scale
        
        # STE
        return (x_quant - x).detach() + x

    def forward(self, x_hyperbolic):
        # 1. Log Map (Manifold -> Tangent)
        # bfloat16の安定性のため、射影前後はfloat32推奨
        x_fp32 = x_hyperbolic.float()
        x_tangent = self.ball.logmap0(x_fp32)
        
        # 2. Activation Quantization (Tangent Space)
        x_quant = self.activation_quantization(x_tangent)
        
        # 3. Weight Quantization (Tangent Space)
        w_quant = self.weight_quantization(self.weight)
        
        # 4. BitLinear Matmul (Euclidean Operation)
        # 実際にはカスタムCUDAカーネルで高速化される部分
        y_tangent = F.linear(x_quant, w_quant)
        
        # 5. Exp Map (Tangent -> Manifold)
        y_hyperbolic = self.ball.expmap0(y_tangent)
        
        return y_hyperbolic.type_as(x_hyperbolic)
インサイト: BitNet の重み行列が $\{-1, 0, 1\}$ に離散化されることは、接空間上の変換が「軸に沿った回転」「鏡映」「成分の無効化」の組み合わせに限定されることを意味する。これは双曲空間上では、測地線を特定の方向に強く曲げる、あるいは分岐させる操作に対応し、木構造データの離散的な分岐決定（Routing）と非常に相性が良い可能性がある。Topic 4: Hybrid Attention - Tangent vs. Manifold4.1 The Verdict（結論）結論: 10Bパラメータ規模かつ1.58bitモデルにおいて、Attention機構は "Tangent Space Attention" を採用すべきである。Fully Hyperbolic Attention（双曲空間上での直接的な距離計算とEinstein Midpointによる加重平均）は、計算コストが $O(N^2)$ のAttention操作内で複雑な超越関数（acosh, sqrt）を大量に必要とする上、数値安定性が極めて低い 11。一方、Tangent Space Attention（入力を接空間に写像し、通常の Scaled Dot-Product Attention を行う）は、FlashAttention などの既存の高速化カーネルを利用可能であり、10Bモデルの実用的な学習速度と安定性を確保できる唯一の解である。HNN++ 7 や最新の双曲Transformer研究においても、この「ハイブリッドアプローチ」がデファクトスタンダードとなっている。4.2 Mathematical Proof（根拠）Tangent Space Attention の幾何学的正当性Query $Q^H$, Key $K^H$, Value $V^H$ が双曲空間 $\mathbb{D}$ 上にあるとする。Tangent Space Attention は以下のように定式化される。Tangent Projection:$$Q = \log_0(Q^H), \quad K = \log_0(K^H), \quad V = \log_0(V^H)$$ここで、原点 $0$ における対数写像 $\log_0(x)$ は、点 $x$ の原点からの双曲距離をユークリッドノルム $\|v\|$ として保存する（$d_{\mathbb{D}}(0, x) = 2 \tanh^{-1}(\|x\|) \approx 2\|v\|$）。つまり、接空間ベクトルのノルムは「階層の深さ（一般性 vs 特殊性）」をエンコードしている。Attention Score (Euclidean):$$\text{Score}(Q, K) = \frac{QK^T}{\sqrt{d}}$$この内積操作は、双曲幾何における「類似度」を近似する。原点に近い（ノルムが小さい＝上位概念）ベクトル同士の内積は小さくなり、原点から遠い（ノルムが大きい＝具体的概念）ベクトル同士の内積は大きくなる。これは、具体的概念同士の強い結びつきや、上位概念の広範なマッチングという、双曲空間の特性を接空間上でも維持できることを意味する。Aggregation & Update:$$A_{\text{tan}} = \text{Softmax}(\text{Score}) V$$$$\text{Output}^H = \exp_0(A_{\text{tan}})$$接空間上での加重平均（Linear combination）は、双曲空間上での厳密な重心（Fréchet Mean / Einstein Midpoint）の一次近似である。10Bモデルのような高次元空間では、この近似誤差よりも、数値安定性と勾配の伝播効率の方が学習全体への寄与が大きい。Fully Hyperbolic Attention のリスクFully Hyperbolic Attention では、スコアとして $-d_{\mathbb{D}}(Q^H, K^H)^2$ 等を用いるが、この距離計算には acosh が含まれ、Topic 2 で述べた勾配爆発のリスクを常に抱える。また、Aggregation に Einstein Midpoint を用いる場合、反復計算や複雑な閉形式が必要となり、FlashAttention のような IO-Aware な最適化が適用できず、学習速度が数倍〜数十倍低下する恐れがある。4.3 Pseudo-Code（実装案）FlashAttention (Scaled Dot-Product Attention) をラップした Tangent Space Attention の実装。Pythonclass HyperbolicTangentAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, c=1.0):
        super().__init__()
        self.c = c
        self.ball = PoincareBall(c=c)
        self.embed_dim = embed_dim
        
        # 標準的な Multihead Attention (FlashAttention compatible)
        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

    def forward(self, query_h, key_h, value_h, mask=None):
        # query_h, key_h, value_h: on Manifold

        # 1. Map to Tangent Space (float32 cast for stability)
        # bfloat16 のまま logmap0 を通すと危険なため、一時的に FP32 へ
        q_t = self.ball.logmap0(query_h.float()).to(query_h.dtype)
        k_t = self.ball.logmap0(key_h.float()).to(key_h.dtype)
        v_t = self.ball.logmap0(value_h.float()).to(value_h.dtype)

        # 2. Standard Scaled Dot-Product Attention (Euclidean)
        # ここで GPU 最適化されたカーネルが走る
        attn_output_t, _ = self.mha(q_t, k_t, v_t, attn_mask=mask)

        # 3. Residual Connection in Tangent Space (Recommended)
        # 双曲空間での加算 (Mobius Add) は高コストなので、接空間で残差加算
        # ResNet-BK の "ResNet" 部分の正当性
        output_t = q_t + attn_output_t

        # 4. Map back to Manifold
        output_h = self.ball.expmap0(output_t.float()).to(query_h.dtype)
        
        return output_h
Topic 5: Physics-Informed Initialization5.1 The Verdict（結論）結論: BK-Core の学習初期における勾配消失・爆発を防ぐ最適な初期化戦略は、「Symplectic Green's Initialization (SGI)」 である。従来の BK Isometry Init（厳密な等長初期化）が学習を停滞させた理由は、リカレント行列を完全な直交行列（ユニタリ行列）に拘束したことで、状態空間のダイナミクスが「保存系」に固定され、過去の情報を捨てる（忘却する）ことも、新しいパターンを強調することもできない「硬直した」状態になったためである。対して SGI は、ハミルトニアン系のシンプレクティック構造（エネルギー保存）をベースにしつつ、Green 関数に由来する物理的に妥当な「散逸（Dissipation）」項を微小な摂動として導入する。これにより、長距離依存性を維持しつつ、学習可能な勾配の流れを作り出す。5.2 Mathematical Proof（根拠）Green 関数と長距離依存性のスペクトル解析線形微分作用素 $\mathcal{L} u = f$ に対する Green 関数 $G(t, s)$ は、系のインパルス応答記述である 15。$$u(t) = \int_{-\infty}^t G(t, s) f(s) ds$$リカレントニューラルネットワーク（RNN/SSM）において、この積分核 $G(t, s)$ は状態遷移行列 $A$ の累乗 $A^{t-s}$ に離散化される。長距離依存性（Long-term Dependency）を持つためには、$A$ の固有値 $\lambda_i$ が単位円周付近 $|\lambda_i| \approx 1$ に分布する必要がある（Vanishing Gradient の回避）16。しかし、厳密に $|\lambda_i| = 1$ (Unitary) であると、信号は永遠に減衰せず、ノイズも蓄積し続ける。物理的な波動伝播や拡散現象において、Green 関数は通常、距離や時間と共にある程度の減衰（Decay）を持つ。この減衰こそが、直近の情報を重視しつつ遠くの情報を参照するための「重み付け」として機能する。Symplectic Green's Initialization (SGI) の定式化SGI では、行列 $W$ を以下の形式で初期化する。$$W_{\text{init}} = Q \cdot \exp(-\Gamma \Delta t)$$$Q$ (Orthogonal Component): ランダムな直交行列。情報の回転と混合を担い、ノルムを保存する（Symplectic-like dynamics）。これにより勾配消失を防ぐベースラインを作る。$\exp(-\Gamma \Delta t)$ (Green's Dissipative Factor): 対角行列 $\Gamma = \text{diag}(\gamma_1, \dots, \gamma_n)$ による減衰項。$\gamma_i > 0$ は微小な値とし、固有値分布を単位円の「わずかに内側」へ配置する。これは物理的には散逸項（摩擦や抵抗）に相当し、無限の過去ではなく「有効な相関長」を定義する。この初期化により、ネットワークは「学習可能なハミルトニアン系（保存系）」からスタートし、必要な散逸（忘却）を学習していくことができる。5.3 Pseudo-Code（実装案）Pythonimport torch
import torch.nn.init as init

def symplectic_greens_init_(tensor, dt=0.01, min_decay=1e-4, max_decay=1e-2):
    """
    Symplectic Green's Initialization (SGI).
    Initializes weights as an orthogonal matrix composed with a 
    Green's function-like dissipative term.
    
    Args:
        tensor: Weight tensor (Hidden x Hidden)
        dt: Time step size approximation
        min_decay, max_decay: Range of decay rates (gamma)
    """
    rows, cols = tensor.shape
    if rows!= cols:
        # 正方でない場合は直交初期化のみ（射影行列など）
        init.orthogonal_(tensor)
        return

    with torch.no_grad():
        # 1. Random Orthogonal Matrix Q (Energy Conserving)
        # SVDベースの直交化により、全周波数成分を均等に扱う
        Q = torch.empty(rows, cols)
        init.orthogonal_(Q)
        
        # 2. Green's Decay Factors (Dissipative)
        # 固有値を単位円のわずかに内側に分布させる
        # gamma ~ Uniform(min_decay, max_decay)
        # Decay D = diag(exp(-gamma * dt))
        gamma = torch.rand(rows) * (max_decay - min_decay) + min_decay
        decay_factors = torch.exp(-gamma * dt)
        D = torch.diag(decay_factors)
        
        # 3. Combine: W = Q * D
        # これにより、|eigenvalues| < 1 だが、非常に 1 に近い状態を作る
        W_init = torch.mm(Q, D)
        
        tensor.copy_(W_init)

# 使用例: BK-CoreのRecurrent Weight
# self.A = nn.Parameter(torch.empty(hidden_dim, hidden_dim))
# symplectic_greens_init_(self.A)
総合実装ロードマップ (Roadmap to Stability)以上のリサーチに基づき、ResNet-BK プロジェクトにおける数値爆発を収束させ、10Bモデルの学習を成功させるための包括的な修正手順を以下にまとめる。Weight Definition:すべての nn.Linear (BitLinear) の重み $W$ を、ManifoldParameter から通常の nn.Parameter (Euclidean) に変更する。これにより、オプティマイザの誤ったリーマン更新を停止する。Precision Management:bfloat16 で学習する場合、Lorentz Wrapper を実装し、Poincaré Ball の境界問題を回避する。もしくは、atanh や expmap の直前で必ず float32 にキャストし、安全マージン（eps=1e-2）を持ったクリッピングを適用する。Quantization Strategy:BitNet の量子化は Tangent Space 上で行う。計算グラフは Log -> Quantize -> Linear -> Exp の順序を厳守する。Attention Mechanism:Fully Hyperbolic Attention を廃止し、Tangent Space Attention に統一する。これにより、FlashAttention の利用が可能となり、計算効率と数値安定性が飛躍的に向上する。Initialization:BK-Core 部には Symplectic Green's Initialization を適用し、直交性と適切な減衰を両立させることで、初期学習の安定化を図る。この是正措置により、勾配ノルムは理論的な正常値（1.0〜10.0）に回帰し、物理・幾何・量子化が融合した次世代アーキテクチャの真価が発揮されるであろう。以上