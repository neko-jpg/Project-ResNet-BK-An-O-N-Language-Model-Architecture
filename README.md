# Project-ResNet-BK-An-O-N-Language-Model-Architecture
(AI Learning Cost "One Millionth" Plan - Step 1/4 Achieved)
?? Repository Summary (README.md)

Project ResNet-BK: An O(N) Language Model Architecture
(1,000,000x AI Training Cost Reduction Plan ? Step 1/4 Achieved)

?? Overview (Elevator Pitch)

This repository documents the research and development of ResNet-BK, a new O(N) language model architecture designed to overcome the dominant bottleneck in modern AI: the O(N?) computational cost of Transformers.

This work represents a successful proof-of-concept for Step 1 (Architectural Overhaul) and Step 3 (Sparsification) of the long-term 1,000,000x Cost Reduction Plan.

?? Latest Status (Dec 2025, research prototype)
- Colab small-scale benchmark (WikiText-2, seq_len=512, batch=4, 2000 steps) shows ResNet-BK beating a Transformer baseline: val ppl ~590 vs ~1288 (no OOM, CUDA).
- Fairness note: Transformer uses autocast; ResNet-BK currently not. Speed numbers are therefore not comparable yet (throughput ~3.4k tok/s vs ~71k tok/s). Accuracy advantage remains.
- Long-context bench script builds models per sequence length to avoid n_seq mismatch; use `notebooks/long_context_benchmark_colab.py`.
- Target audience: research users. Large-scale recipes/CI are not production-ready yet.

?? Earlier Results: Faster & Trainable
ğŸš€ Final Results: 6.7Ã— Faster & Demonstrated Learning Ability

1. Speed: 6.7Ã— Faster than Attention at N=2048 (CPU)

The final integrated architecture â€” combining:

the O(N) core algorithm

analytic gradient (manual backward pass)

sparse MoE

surpasses standard Attention as sequence length increases.

At N = 2048, it achieves ~6.7Ã— speedup over Autograd-based Attention.
(From TeppeiArai_ONResNetBK_MoE_FinalScaling_Report.pdf)

2. Intelligence: Fully Trainable as a Language Model (GPU)

ResNet-BK is not only fast â€” it can learn.

Using BK-MoE_Language_Model.py, stable learning was observed on GPU:

Parameters: 10.16M

Task: WikiText-2

Result: Perplexity 428.84 after 3 epochs

Notes (Transformer baseline clarity):
- Uses pre-norm blocks and learned absolute positional embeddings (swap to sinusoidal / RoPE for ablations if desired).
- Small benchmark settings (vocabâ‰ˆ20k, seq_lenâ‰ˆ256, d_model=256, L=6) can show higher initial loss; this is expected, not a bug.
- LayerNorm is applied before each sublayer for stability in both baselines.

This confirms that the architecture is viable as a language model.

ğŸ”¬ Technical Milestones

Each result was achieved through the following PoCs:

1. O(N) Core vs O(NÂ²) Attention

Benchmarking pure compute throughput

Finding: Around N â‰ˆ 1000, O(N) computation becomes superior.

2. Analytic Gradient Implementation

Manual backward pass without Autograd

Finding: ~1.6Ã— faster in PoC; integrated version yields 2.5Ã— speedup at N=2048.

3. Sparse MoE Integration

Replaced dense MLP with sparse Mixture of Experts

Finding: Faster than dense FFN while maintaining accuracy.

ğŸ—‚ï¸ Repository Structure
/1_BK_Language_Model_PoC/

Contains the final integrated model (BK-MoE_Language_Model.py) and training results
(including PPL 428).

/2_Scaling_Benchmarks/

Time-ordered benchmarks, reports, and source code demonstrating:

O(N) vs O(NÂ²)

Analytic Gradient speedups

Sparse MoE

Final 6.7Ã— speed benchmark

ğŸ”® Future Work (What Comes Next)

This project completes Step 1 + Step 3 of the plan.

The next frontier is Step 2: Replacing Backpropagation.

Future research will explore:

operator-based learning (e.g., Koopman theory)

physics-informed optimization

gradient-free or hybrid training mechanisms

ResNet-BK now provides the O(N) â€œvesselâ€ needed to host these new learning paradigms.


æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¼ãƒ‰ï¼šBK-MoE_Ultra_v2_Stable.py
Running on CUDA
Vocabulary Size: 30000
Train tokens: 500000 (after batchify)
--- ResNet-BK Ultra v2: O(N) + Hybrid Analytic Grad + Sparse MoE ---
Model Parameters: 4.15M
Total Steps (approx): 585
BKCore GRAD_BLEND = 0.5
  [Step 50] Epoch 1 | Loss: 7.4817 | LR: 0.000984
  [Step 100] Epoch 1 | Loss: 7.1682 | LR: 0.000937
  [Step 150] Epoch 1 | Loss: 7.2618 | LR: 0.000862
============================================================
Epoch 1/3 | Time: 28.82s | Avg Loss: 7.6057 | Perplexity: 2009.60
============================================================
  [Step 200] Epoch 2 | Loss: 7.0199 | LR: 0.000764
  [Step 250] Epoch 2 | Loss: 7.0463 | LR: 0.000652
  [Step 300] Epoch 2 | Loss: 7.0798 | LR: 0.000532
  [Step 350] Epoch 2 | Loss: 7.1368 | LR: 0.000413
============================================================
Epoch 2/3 | Time: 24.11s | Avg Loss: 7.0517 | Perplexity: 1154.78
============================================================
  [Step 400] Epoch 3 | Loss: 7.0109 | LR: 0.000304
  [Step 450] Epoch 3 | Loss: 6.9486 | LR: 0.000213
  [Step 500] Epoch 3 | Loss: 7.0623 | LR: 0.000146
  [Step 550] Epoch 3 | Loss: 6.9950 | LR: 0.000108
============================================================
Epoch 3/3 | Time: 24.25s | Avg Loss: 7.0229 | Perplexity: 1122.06
============================================================


---

## ğŸ¯ Google Colab ã§ä»Šã™ãè©¦ã™ï¼

Step 2 Phase 1ã®å®Ÿè£…ã‚’Google Colabã§ç°¡å˜ã«å®Ÿè¡Œã§ãã¾ã™ï¼š

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ5åˆ†ï¼‰

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture/blob/main/notebooks/step2_phase1_colab.ipynb)

**å®Ÿè¡Œæ‰‹é †:**
1. ä¸Šã®ãƒãƒƒã‚¸ã‚’ã‚¯ãƒªãƒƒã‚¯
2. GPUè¨­å®š: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ T4 GPU ã‚’é¸æŠ
3. ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ
4. 20-30åˆ†ã§å®Œäº†ï¼

**å®Ÿè£…å†…å®¹:**
- âœ… Mixed-precision gradient computation (2Ã— speedup)
- âœ… Batched analytic gradient with vmap (2.5Ã— speedup)
- âœ… GRAD_BLEND grid search (æœ€é©ãªÎ±å€¤ã®ç™ºè¦‹)
- âœ… 3-epoch training with numerical stability

è©³ç´°ã¯ [COLAB_QUICK_START.md](COLAB_QUICK_START.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---


---

## ğŸŠ Step 4: Advanced Model Compression å®Œäº†ï¼ï¼ˆNEWï¼‰

**å®Ÿè£…å®Œäº†:**

Step 4ã®å®Œå…¨ãªåœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼

### ä¸»ãªæˆæœ

- âœ… **Quantization-Aware Training (QAT)** - INT8é‡å­åŒ–ã§4Ã—åœ§ç¸®
- âœ… **Complex Number Quantization** - å®Ÿéƒ¨ãƒ»è™šéƒ¨ã®å€‹åˆ¥é‡å­åŒ–
- âœ… **INT4 MoE Quantization** - ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¯ã‚¤ã‚ºé‡å­åŒ–ã§8Ã—åœ§ç¸®
- âœ… **Structured Pruning** - ä½¿ç”¨ç‡5%æœªæº€ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’è‡ªå‹•å‰Šé™¤
- âœ… **Knowledge Distillation** - æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å°å‹å­¦ç”Ÿãƒ¢ãƒ‡ãƒ«ã¸çŸ¥è­˜è»¢ç§»
- âœ… **Compression Pipeline** - è‡ªå‹•åŒ–ã•ã‚ŒãŸ3æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- âœ… **Target: 100Ã— compression** with <15% perplexity degradation

### åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```
Original Model (4.15M params)
    â†“
[Stage 1: QAT] â†’ 4Ã— compression
    â†“
[Stage 2: Pruning] â†’ 4Ã— compression
    â†“
[Stage 3: Distillation] â†’ 6Ã— compression
    â†“
Final Model (~42K params) = 96Ã— â‰ˆ 100Ã— compression
```

### Google Colabã§è©¦ã™

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neko-jpg/Project-ResNet-BK-An-O-N-Language-Model-Architecture/blob/main/notebooks/step4_compression.ipynb)

è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§:
- [STEP4_COMPRESSION_IMPLEMENTATION.md](STEP4_COMPRESSION_IMPLEMENTATION.md) - è©³ç´°ãªå®Ÿè£…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [STEP4_QUICK_REFERENCE.md](STEP4_QUICK_REFERENCE.md) - ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

---

## ğŸŠ Step 2 Phase 1 å®Œäº†ï¼

**Google Colabå®Ÿè¡Œçµæœ:**

Step 2 Phase 1ã®å®Ÿè£…ãŒGoogle Colabï¼ˆT4 GPUï¼‰ã§æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼

### ä¸»ãªæˆæœ

- âœ… **GRAD_BLENDæœ€é©åŒ–å®Œäº†** - Î± = 0.0ï¼ˆç´”ç²‹ãªç†è«–çš„å‹¾é…ï¼‰ãŒæœ€é©
- âœ… **Mixed-precisionå®Ÿè£…** - 1.5-2.0Ã— speedupé”æˆ
- âœ… **Batched gradientå®Ÿè£…** - 2.0-2.5Ã— speedupé”æˆ
- âœ… **æ•°å€¤å®‰å®šæ€§ç¢ºèª** - NaN/Infãªã—ã§å­¦ç¿’å®Œäº†
- âœ… **Best Perplexity: 309.90** on WikiText-2

### Grid Searchçµæœ

| GRAD_BLEND (Î±) | Perplexity | Gradient Variance | Status |
|----------------|------------|-------------------|--------|
| **0.0** | **309.90** | 0.0216 | âœ… Best |
| 0.3 | 341.95 | 0.1778 | âš ï¸ |
| 0.5 | 322.15 | 0.0742 | âš ï¸ |
| 0.7 | 495.04 | 427.32 | âŒ Unstable |
| 1.0 | 494.01 | 437.88 | âŒ Unstable |

**é‡è¦ãªç™ºè¦‹:** ç†è«–çš„å‹¾é…ï¼ˆÎ±=0.0ï¼‰ãŒHypothesis-7å‹¾é…ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚

è©³ç´°ã¯ [STEP2_PHASE1_COLAB_RESULTS.md](STEP2_PHASE1_COLAB_RESULTS.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---
