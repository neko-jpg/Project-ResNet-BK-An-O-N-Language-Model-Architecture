# Project-ResNet-BK-An-O-N-Language-Model-Architecture
(AI Learning Cost â€œOne Millionthâ€ Plan - Step 1/4 Achieved)
ğŸ“ Repository Summary (README.md)

Project ResNet-BK: An O(N) Language Model Architecture
(â€œ1,000,000Ã— AI Training Cost Reductionâ€ Plan â€“ Step 1/4 Achieved)

ğŸš€ Overview (Elevator Pitch)

This repository documents the research and development of ResNet-BK, a new O(N) language model architecture designed to overcome the dominant bottleneck in modern AI: the O(NÂ²) computational cost of Transformers.

This work represents a successful proof-of-concept for Step 1 (Architectural Overhaul) and Step 3 (Sparsification) of the long-term â€œ1,000,000Ã— Cost Reduction Plan.â€

ğŸš€ Final Results: 6.7Ã— Faster & Demonstrated Learning Ability
1. Speed: 6.7Ã— Faster than Attention at N=2048 (CPU)

The final integrated architecture â€” combining:

the O(N) core algorithm

analytic gradient (manual backward pass)

sparse MoE

surpasses standard Attention as sequence length increases.

At N = 2048, it achieves ~6.7Ã— speedup over Autograd-based Attention.
(From TeppeiArai_ONResNetBK_MoE_FinalScaling_Report.pdf)

2. Intelligence: Fully Trainable as a Language Model (GPU)

ResNet-BK is not only fast â€” it can learn.

Using BK-MoE_Language_Model.py, stable learning was observed on GPU:

Parameters: 10.16M

Task: WikiText-2

Result: Perplexity 428.84 after 3 epochs

This confirms that the architecture is viable as a language model.

ğŸ”¬ Technical Milestones

Each result was achieved through the following PoCs:

1. O(N) Core vs O(NÂ²) Attention

Benchmarking pure compute throughput

Finding: Around N â‰ˆ 1000, O(N) computation becomes superior.

2. Analytic Gradient Implementation

Manual backward pass without Autograd

Finding: ~1.6Ã— faster in PoC; integrated version yields 2.5Ã— speedup at N=2048.

3. Sparse MoE Integration

Replaced dense MLP with sparse Mixture of Experts

Finding: Faster than dense FFN while maintaining accuracy.

ğŸ—‚ï¸ Repository Structure
/1_BK_Language_Model_PoC/

Contains the final integrated model (BK-MoE_Language_Model.py) and training results
(including PPL 428).

/2_Scaling_Benchmarks/

Time-ordered benchmarks, reports, and source code demonstrating:

O(N) vs O(NÂ²)

Analytic Gradient speedups

Sparse MoE

Final 6.7Ã— speed benchmark

ğŸ”® Future Work (What Comes Next)

This project completes Step 1 + Step 3 of the plan.

The next frontier is Step 2: Replacing Backpropagation.

Future research will explore:

operator-based learning (e.g., Koopman theory)

physics-informed optimization

gradient-free or hybrid training mechanisms

ResNet-BK now provides the O(N) â€œvesselâ€ needed to host these new learning paradigms.
