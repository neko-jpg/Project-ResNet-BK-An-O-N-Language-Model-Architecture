# Project MUSE - ç¾çŠ¶ãƒ¬ãƒãƒ¼ãƒˆ

**æœ€çµ‚æ›´æ–°**: 2025-12-20

---

## 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ç›®æ¨™**: 10B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªLLMã‚’ ResNet-BK ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å­¦ç¿’

| é …ç›® | å€¤ |
|------|-----|
| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | 10B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | ResNet-BK (Phase 8) |
| ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª | æ—¥æœ¬èª |
| GPU | RTX 3080 8GB |
| è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ | `scripts/train_phase8_stable.py` |
| è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« | `configs/phase8_10b_japanese.yaml` |

---

## 2. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¼·ã¿ âœ…

### æ•°å­¦çš„åŸºç›¤
| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ç†è«–çš„æ ¹æ‹  |
|---------------|-----------|
| **Prime-BumpåˆæœŸåŒ–** | ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®é›¶ç‚¹åˆ†å¸ƒã«åŸºã¥ã |
| **Birman-Schwingeræ ¸** | é‡å­æ•£ä¹±ç†è«–ã®O(N)ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ |
| **åŒæ›²å¹¾ä½•åŸ‹ã‚è¾¼ã¿** | éšå±¤çš„è¡¨ç¾ã®åŠ¹ç‡çš„ãªå­¦ç¿’ |
| **Symplecticã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶** | ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹é€ ã‚’ä¿å­˜ |
| **TSP Path Optimizer** | ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹å‹•çš„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |

### æ¤œè¨¼æ¸ˆã¿ã®åŠ¹æœï¼ˆå°è¦æ¨¡å®Ÿé¨“ï¼‰
```
results/paper_experiments/ablation.json ã‚ˆã‚Š:

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:    PPL 44.09
+ prime_bump:    PPL 36.78 (-16.6%)
+ lap_stability: PPL 32.50 (-26.3%)
+ scattering:    PPL 29.51 (-33.1%)
+ semiseparable: PPL 27.63 (-37.3%)
å…¨éƒ¨å…¥ã‚Š:        PPL 24.15 (-45.2%)
```

---

## 3. ç¾åœ¨ã®èª²é¡Œ âš ï¸

### 3.1 å­¦ç¿’ã®é…ã•

| å•é¡Œ | åŸå›  | è§£æ±ºç­– |
|------|------|--------|
| WarmupãŒé•·ã™ãã‚‹ | `warmup_steps: 500` (optimizer steps) = 32,000 batch steps | âœ… `warmup_steps: 0` ã«å¤‰æ›´æ¸ˆã¿ |
| å‹¾é…è“„ç©ãŒå¤šã„ | `gradient_accumulation_steps: 64` | 8-16 ã«æ¸›ã‚‰ã™æ¤œè¨ |
| å­¦ç¿’ã®å‹•ããŒè¦‹ãˆã«ãã„ | 64 steps ã« 1å›ã—ã‹ optimizer.step() ã—ãªã„ | ãƒ­ã‚°é »åº¦ã‚’ä¸Šã’ã‚‹ |

### 3.2 ãƒ‡ãƒ¼ã‚¿é‡ã®ä¸è¶³

| ç¾çŠ¶ | ç›®æ¨™ | ã‚®ãƒ£ãƒƒãƒ— |
|------|------|----------|
| ~100M tokens | 10B+ tokens | 100å€ä¸è¶³ |

**æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹** (`data/japanese/`):
- Wikipedia JP
- CC-100 JP
- Dolly JP (Instruction)
- Alpaca JP (Instruction)

åˆè¨ˆ: ç´„ 1.1GB (pretrain_combined)

### 3.3 å‹¾é…çˆ†ç™ºï¼ˆåˆ¶å¾¡æ¸ˆã¿ï¼‰

| æŒ‡æ¨™ | å€¤ | çŠ¶æ…‹ |
|------|-----|------|
| Raw grad norm | 20,000 ~ 86,000 | é«˜ã„ãŒæƒ³å®šå†… |
| Clipped grad norm | 2.0 | âœ… ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‹•ä½œä¸­ |
| Gradient Aligner | 30-38% ãŒé€†æ–¹å‘ â†’ æ•´åˆ— | âœ… å‹•ä½œä¸­ |

---

## 4. è¨­å®šå¤‰æ›´å±¥æ­´

### 2025-12-20 ã®å¤‰æ›´

1. **Makefile**: `make resume-japanese`, `make resume`, `make start-japanese` ãŒ `train_phase8_stable.py` ã‚’ä½¿ã†ã‚ˆã†ã«çµ±ä¸€
2. **configs/phase8_10b_japanese.yaml**: `warmup_steps: 500 â†’ 0`

### æ¨å¥¨ã•ã‚Œã‚‹è¿½åŠ å¤‰æ›´

```yaml
# gradient_accumulation_steps ã‚’æ¸›ã‚‰ã—ã¦å­¦ç¿’ã®å‹•ãã‚’é€Ÿãè¦‹ã‚‹
gradient_accumulation_steps: 8  # 64 â†’ 8

# ã¾ãŸã¯å‹¾é…è“„ç©ãªã—ã§ç›´æ¥æ›´æ–°
gradient_accumulation_steps: 1
```

---

## 5. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

| å„ªå…ˆåº¦ | ã‚¿ã‚¹ã‚¯ | çŠ¶æ…‹ |
|--------|--------|------|
| ğŸ”´ é«˜ | å­¦ç¿’ã‚’å†é–‹ã—ã¦ warmup=0 ã®åŠ¹æœã‚’ç¢ºèª | å¾…æ©Ÿä¸­ |
| ğŸŸ¡ ä¸­ | gradient_accumulation_steps ã‚’æ¸›ã‚‰ã™ | æ¤œè¨ä¸­ |
| ğŸŸ¡ ä¸­ | æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ï¼ˆHuggingFaceç­‰ã‹ã‚‰è¿½åŠ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰ | æœªç€æ‰‹ |
| ğŸŸ¢ ä½ | learning_efficiency ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Œäº†ã•ã›ã‚‹ | ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆæ¸ˆã¿ |

---

## 6. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

### ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«

```
configs/
â””â”€â”€ phase8_10b_japanese.yaml    # 10Bæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®è¨­å®š

scripts/
â”œâ”€â”€ train_phase8_stable.py      # å®‰å®šç‰ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¼·åŒ–ï¼‰
â”œâ”€â”€ train_phase8.py             # æ—§ç‰ˆï¼ˆä½¿ç”¨éæ¨å¥¨ï¼‰
â”œâ”€â”€ benchmark_learning_efficiency.py  # å­¦ç¿’åŠ¹ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â””â”€â”€ prepare_japanese_data.py    # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿æº–å‚™

src/models/
â”œâ”€â”€ resnet_bk.py                # ResNet-BK ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ birman_schwinger_core.py    # BK-Coreï¼ˆé‡å­æ•£ä¹±ç†è«–ï¼‰
â”œâ”€â”€ hyperbolic_modules.py       # åŒæ›²å¹¾ä½•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”œâ”€â”€ prime_bump_potential.py     # Prime-BumpåˆæœŸåŒ–
â””â”€â”€ phase8/integrated_model.py  # Phase 8 çµ±åˆãƒ¢ãƒ‡ãƒ«

src/training/
â”œâ”€â”€ tsp_path_optimizer.py       # TSP Path Optimizer
â”œâ”€â”€ gradient_aligner.py         # å‹¾é…æ–¹å‘æ•´åˆå™¨
â””â”€â”€ symplectic_optimizer.py     # Symplecticã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶

results/
â”œâ”€â”€ paper_experiments/
â”‚   â”œâ”€â”€ ablation.json           # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
â”‚   â””â”€â”€ efficiency.json         # åŠ¹ç‡æ€§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â””â”€â”€ benchmarks/
    â””â”€â”€ 95_PERCENT_ACHIEVEMENT_ANALYSIS.md  # VRAMæœ€é©åŒ–åˆ†æ
```

---

## 7. ã‚³ãƒãƒ³ãƒ‰ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

```bash
# å­¦ç¿’é–‹å§‹/å†é–‹
make start-japanese      # æ–°è¦é–‹å§‹
make resume-japanese     # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
make prepare-japanese-data

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
python scripts/benchmark_learning_efficiency.py --max-tokens 2000000

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆVRAMç¢ºèªï¼‰
make dry-run
```

---

## 8. çµè«–

**ResNet-BK ã¯ç†è«–çš„ã«æ­£ã—ãã€å°è¦æ¨¡ã§æ¤œè¨¼æ¸ˆã¿**ã€‚10B ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®èª²é¡Œã¯ï¼š

1. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´** â€” warmupã€å‹¾é…è“„ç©ãªã©ã®è¨­å®šã‚’æœ€é©åŒ–ï¼ˆé€²è¡Œä¸­ï¼‰
2. **ãƒ‡ãƒ¼ã‚¿é‡** â€” 100M â†’ 10B+ tokens ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãŒå¿…è¦
3. **è¨ˆç®—æ™‚é–“** â€” RTX 3080 1å°ã§ã¯æ™‚é–“ãŒã‹ã‹ã‚‹

ã“ã‚Œã‚‰ã¯æŠ€è¡“çš„ã«è§£æ±ºå¯èƒ½ãªèª²é¡Œã§ã‚ã‚Šã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªä½“ã®æ¬ é™¥ã§ã¯ãªã„ã€‚
